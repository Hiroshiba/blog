<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hiho&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.hiroshiba.jp/"/>
  <updated>2019-03-02T16:39:41.549Z</updated>
  <id>http://blog.hiroshiba.jp/</id>
  
  <author>
    <name>Kazuyuki Hiroshiba</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ディープラーニングの力で人工知能になって結月ゆかりと会話してみた</title>
    <link href="http://blog.hiroshiba.jp/talk-to-yuduki-yukari-with-deep-learning-power/"/>
    <id>http://blog.hiroshiba.jp/talk-to-yuduki-yukari-with-deep-learning-power/</id>
    <published>2019-03-02T16:20:45.000Z</published>
    <updated>2019-03-02T16:39:41.549Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目次"><a href="#目次" class="headerlink" title="目次"></a>目次</h3><ul><li>（背景）結月ゆかりと会話したいが、結月ゆかりの人格は世界に存在しない。</li><li>（手法）自分が人工知能になり、余った自分の魂を結月ゆかりに宿らせて、自分と結月ゆかりが会話する手法を提案する。</li><li>（結果）結月ゆかりと会話することができた。</li><li>（展望）次は結月ゆかりの人工知能を作りたい。</li></ul><a id="more"></a><p><script type="application/javascript" src="https://embed.nicovideo.jp/watch/sm34712379/script?w=640&h=360"></script><noscript><a href="https://www.nicovideo.jp/watch/sm34712379" target="_blank" rel="external">ディープラーニングの力で人工知能になって結月ゆかりと会話してみた</a></noscript></p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>多くの人が結月ゆかりと会話したいと思っている。結月ゆかりと会話するには、結月ゆかりの人工知能を作ることで解決できる。しかし、結月ゆかりの真の人格は世界に存在しない。そのため、結月ゆかりのデータを集めることができず、人工知能を作ることは難しいという問題があった。</p><h3 id="手法"><a href="#手法" class="headerlink" title="手法"></a>手法</h3><p>発想を転換し、結月ゆかりの人工知能を作るのではなく、自分の人工知能を作ることを考えた。そして自分が結月ゆかりになり、その人工知能と会話する。これで、自分と結月ゆかりが会話できる。</p><p>最初に自分の人工知能の作り方を説明し、続いて結月ゆかりになる手法を説明する。</p><h4 id="自分の人工知能を作る"><a href="#自分の人工知能を作る" class="headerlink" title="自分の人工知能を作る"></a>自分の人工知能を作る</h4><p>人工知能と会話するためには、人工知能用のテキスト音声合成・会話応答・音声認識が必要になる。それぞれの手法を説明する。（手法の詳細は<a href="https://qiita.com/Hiroshiba/items/b4daca0176af5fd352a2" target="_blank" rel="external">過去の解説記事</a>にある。）</p><p>テキスト音声合成には<a href="https://r9y9.github.io/nnmnkwii/latest/nnmnkwii_gallery/notebooks/tts/02-Bidirectional-LSTM%20based%20RNNs%20for%20speech%20synthesis%20%28en%29.html" target="_blank" rel="external">ディープラーニングを使った既存手法</a>を用いた。文章５００文を読み上げた自分の音声データを学習データセットとした。</p><p>会話応答は、「問いかけの文章を自分なりの言葉にして返すもの」とし、ディープラーニングを使ったオートエンコーダを用いた。doc2vecを使ってテキストから特徴量を抽出し、その特徴量から元のテキストを生成する学習を行った。<a href="https://friends.nico/@hiho_karuta" target="_blank" rel="external">SNS（マストドン）で発言したテキスト約１１万文</a>を学習データセットとした。</p><p>音声認識はchromeに備わっているWeb Speech APIを用いた。デモを行う際、自分の発言の字幕をリアルタイムで表示するためにこれを採用した。</p><p>学習のためのコードや、これらをWebAPI化するコード、人工知能と会話できるWebフロントアプリ、全てをオープンソースで公開した。</p><ul><li><a href="https://github.com/Hiroshiba/hihobot" target="_blank" rel="external">会話応答（hihobot）</a></li><li><a href="https://github.com/Hiroshiba/hihobot-synthesis" target="_blank" rel="external">音声合成（hihobot-synthesis）</a></li><li><a href="https://github.com/Hiroshiba/hihobot-tts" target="_blank" rel="external">APIサーバー（hihobot-tts）</a></li><li><a href="https://github.com/Hiroshiba/hihobot-front" target="_blank" rel="external">フロントエンド（hihobot-front）</a></li></ul><h4 id="結月ゆかりになる"><a href="#結月ゆかりになる" class="headerlink" title="結月ゆかりになる"></a>結月ゆかりになる</h4><p>結月ゆかりになるには、自分の声をリアルタイムで結月ゆかりの声に変換する必要がある。これには<a href="../became-yuduki-yukari-with-deep-learning-power/">過去に開発した声質変換手法</a>を用いた。</p><h3 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h3><p>自分の人工知能と会話した結果を紹介する。<a href="https://twitter.com/i/status/1068656559406821376" target="_blank" rel="external">実際に会話した動画</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">自分　　　：おはよう</div><div class="line">人工知能　：おはようございます</div><div class="line">自分　　　：今日もいい天気ですね　気分はいかがですか</div><div class="line">人工知能　：今日は休日ですか</div><div class="line">自分　　　：土曜日なんで休日ですね</div><div class="line">人工知能　：２月１２日なんですかね</div><div class="line">自分　　　：それは僕の誕生日ですね</div><div class="line">人工知能　：今日は僕の誕生日ですね</div><div class="line">自分　　　：今日は僕の誕生日ではないですね</div><div class="line">人工知能　：僕は今日ですね</div><div class="line">自分　　　：あそうか確かにあなたの誕生日は今日かもしれない</div></pre></td></tr></table></figure><p>この結果から、人工知能が、質問に対し自分なりの言葉にして返事できていることがわかる。</p><p>また、自分が結月ゆかりになり、自分の人工知能と会話することで、自分と結月ゆかりが会話することができた。<a href="https://www.nicovideo.jp/watch/sm34712379" target="_blank" rel="external">デモ動画</a></p><p>実際に行った素直な感想としては、特に自分と会話している気持ちにはならず、ただただポンコツな何かに対して壁打ちのように会話している気分になった。会話後に、録画した映像を見直すと、ところどころゆかりさんが可愛くて、自分の人工知能を煽っているシーンなどが羨ましく感じることもあった。</p><h3 id="考察・展望"><a href="#考察・展望" class="headerlink" title="考察・展望"></a>考察・展望</h3><h4 id="会話応答の品質が良くない点"><a href="#会話応答の品質が良くない点" class="headerlink" title="会話応答の品質が良くない点"></a>会話応答の品質が良くない点</h4><p>今回は、質問文に対し、自分なりの言葉でその文章を返す学習を行った。その結果、質問文をオウム返しする傾向にあり、会話が成り立たないケースが多くあった。データｔ番目を入力、データｔ＋１番目を出力するように学習すれば、ある程度文脈を理解した会話ができるかもしれない。</p><h4 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h4><p>次は自分が思う理想の結月ゆかり人工知能を作りたい。結月ゆかり人工知能を作るには次の２つの課題がある。</p><ol><li>結月ゆかり用のテキストデータが大量に存在しない点</li><li>そもそも結月ゆかりの人格が世界に存在しない点</li></ol><p>これらの課題を解決するために、様々な人の発言データを用いて人格空間を作成し、その中から自分が思う理想の結月ゆかりの人格を探せるような手法を考えている。時間があれば取り掛かりたい。</p>]]></content>
    
    <summary type="html">
    
      結月ゆかりと会話したいが、結月ゆかりの人格は世界に存在しない。自分が人工知能になり、余った自分の魂を結月ゆかりに宿らせて、自分と結月ゆかりが会話する手法を提案する。結月ゆかりと会話することができた。次は結月ゆかりの人工知能を作りたい。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Brainwave Idea Challenge（脳波アイデアソン）見てきた</title>
    <link href="http://blog.hiroshiba.jp/brainware_idea_challenge_2018/"/>
    <id>http://blog.hiroshiba.jp/brainware_idea_challenge_2018/</id>
    <published>2018-11-18T17:09:49.000Z</published>
    <updated>2019-02-22T17:10:09.648Z</updated>
    
    <content type="html"><![CDATA[<p>BMI（Brain Machine Interface）を作る、<a href="https://www.pgv.co.jp/" target="_blank" rel="external">PGV</a>というベンチャー企業がある。11月、<a href="https://pgv.connpass.com/event/102250/" target="_blank" rel="external">PGVの開催するアイデアソン</a>が開かれた。今回のお題は、高性能ウェアラブル脳波センサーの利用のアイデア出しだ。BMIに興味があったので参加したかったが、アイデアソン枠が埋まっていたので、ブログ枠の観覧者として申し込んだ。</p><a id="more"></a><h2 id="デバイス紹介"><a href="#デバイス紹介" class="headerlink" title="デバイス紹介"></a>デバイス紹介</h2><p>最初にPGVが作った脳波センサーが紹介された。冷えピタシートみたいにおでこに”貼る”だけで、前頭葉周りの脳波を、医療用途でも使えるような精度で測定できる。そもそも、脳波は神経の電気信号から発生しているもので、非常に微弱な上、頭蓋骨という厚い層越しに計測するため、額で得られる信号はとても弱い。それでもこのデバイスを使えば、安静状態によく発生するθ波などなどの低周波のものから、興奮状態によく発生する高周波のものまでちゃんと取れるらしい。ワイヤレス通信が可能（普通はノイズが乗るのでたぶんすごい）。デモとして、開発責任者の方の脳波を測定し、その波形を表示し続けるタブレットが会場の真ん中に展示されていた。本人と喋りながら波形を見ていると、確かに感情によって何かしら波形が変わっていた。</p><h2 id="アイデアソンの様子"><a href="#アイデアソンの様子" class="headerlink" title="アイデアソンの様子"></a>アイデアソンの様子</h2><p>集まった３０人がそれぞれアイデアを練って、お互いのアイデアに投票していく。２０代〜３０代が多そうだ。脳波を、他の用途で解析して、何かを得るアイデアが多かった。エンジニア視点だと、じゃあどうやって学習データ集めるんだとか、そんなことは可能なのかを考えてしまいがちなので、とても参考になる。脳波を解析して好みの音楽を流してくれるとか、脳を計算資源にして提供可能にするとか、一目惚れを計測するとか、赤ちゃんにデバイス付けて育児サポートするとかのアイデアがあった。僕だったら、Vtuber向けの表情コントローラーを作ってみたいなと思う。</p><p>チームに分かれて調査と資料作りが始まった。アイデアを出した人がリーダー的ポジションで進行していくことになる。リーダーが思考フレームワークを持っていると、資料を作成するスピードがとても早い様子が見てわかった。</p><h2 id="発表"><a href="#発表" class="headerlink" title="発表"></a>発表</h2><p>１時間半後に発表が始まった。１チーム４分の発表、１分の質疑だ。それぞれのアイデアの概要と、聞いたときの感想を書いていく。</p><h3 id="Brain-Wave-DJ"><a href="#Brain-Wave-DJ" class="headerlink" title="Brain Wave DJ"></a>Brain Wave DJ</h3><p>脳波から感情を取り、そこから音楽を選択する。更に、脳波を計測し続けて、テンポなどのパラメータをリアルタイムに調整する。悲しいときに更に悲しくさせる、楽しい気分になりたいときに曲を変調して気持ちよくさせる、とか。</p><ul><li>質問：苦しいときや怒ってるときはどうさせるべきか。</li><li>回答：ヒーリング系とか。フィードバックで更にローテンポにするとか。</li><li>感想：音楽使って感情をコントロールするのは、集中力上げるのとかにも使えて面白そう。でも、そもそも人は音楽を変調してほしいのかはちょっと疑問。</li></ul><h3 id="脳波マップサービスの提供"><a href="#脳波マップサービスの提供" class="headerlink" title="脳波マップサービスの提供"></a>脳波マップサービスの提供</h3><p>ドライバーが感じる怖い道とかのデータを得たい。ドライバーから脳波を測定してストレスなどを取得し、そのデータをAPI化して提供する。ナビゲーションのときに渋滞情報などが使われるが、加えて恐怖情報なども提供できれば面白いのでは。ヒヤリハットマップも作れるかも。</p><ul><li>質問：ドライバーみんな冷えピタ貼ることになるんですか</li><li>回答：ビジネスライフで使ってる人に提供していけばよいのではと</li><li>感想：脳波じゃなくて心拍数で良さそう。</li></ul><h3 id="PITA（ひとめぼれサービス）"><a href="#PITA（ひとめぼれサービス）" class="headerlink" title="PITA（ひとめぼれサービス）"></a>PITA（ひとめぼれサービス）</h3><p>一目惚れした瞬間にデバイスを震えさせる。草食系男子に、「お前はこの人しかいない」、ということを気づかせる。まずは市場がありそうな婚活会社に提供する。利用者は婚活後に、「あのとき言ったことがだめだった」みたいなセルフ分析できる。</p><ul><li>感想：結構良いのでは。自分の気持ちが全部相手伝わっているとすると、だいぶ疲れそうだ。もし実現したら、何が伝わって何が伝わらないのかちゃんと知りたいな。</li></ul><h3 id="脳-MORE-BABY"><a href="#脳-MORE-BABY" class="headerlink" title="脳 MORE BABY"></a>脳 MORE BABY</h3><p>子供の夜泣きで眠れないことが多々ある。それで夫婦仲が悪くなったりするので少しでも解決したい。子供に脳波デバイスを取り付ける。子供の眠気のリズムを測定したり、起きそうなのを検知して親のスマホに通知する。</p><ul><li>質問：他に何か赤ちゃん向けのウェアラブルデバイスとかってあるのか、またこのシステムとの差分は。</li><li>回答：音声からいろいろ検知するのを開発した。脳波は脳からしか得られないものがあるので差分はあると思う。</li><li>感想：んーでも結局は子供に夜泣きで起こされるわけか。これで夫婦仲が悪くならなくなるのかはちょっと疑問だ。</li></ul><h3 id="Brainシアター"><a href="#Brainシアター" class="headerlink" title="Brainシアター"></a>Brainシアター</h3><p>映画を見てもらう人の脳波を測定する。そのデータを使って作品を改善したり、マーケティングを考えたりできる。どうやって付けてもらうかが課題。芸能人等のインフルエンサーに付けてもらって、その人との比較をできるようにする。脳波を記録し、その映画のシーンに対するログを取っておいて、そのシーンに対する感情を見直せるようにしておく。</p><ul><li>質問：そもそも映画だけじゃなくて、ゲームとかでも良いのでは。</li><li>回答：そう思う。まあ、映画業界は博打要素が強いので、こういった需要があるのではないかと思っている。</li><li>感想：<a href="http://www.caltech.edu/news/neural-networks-model-audience-reactions-movies-79098" target="_blank" rel="external">表情をカメラで撮る方法</a>で良さそう。</li></ul><h3 id="Think-connect-運転保険"><a href="#Think-connect-運転保険" class="headerlink" title="Think connect -運転保険-"></a>Think connect -運転保険-</h3><p>運転事故が多い。脳波を測定して、運転時のヤバさをスコア化する。そのスコアを用いた保険を作る。</p><ul><li>質問：アラート音によっては更に眠くなりそう。どういうアラートが良いだろうか。</li><li>回答：閃光手榴弾並のパトライトで良いのでは。</li><li>質問：リモートワークしてる人にも使えるのでは。</li><li>回答：この仕組みは、利用者にメリットがあるので、普及させられると思っている。</li><li>感想：データがあった。プレゼンが上手い。リーダーがもともとこういうことを考えていて、今回脳波デバイスに沿わせたという感じらしい。</li></ul><h3 id="Brain-Chain"><a href="#Brain-Chain" class="headerlink" title="Brain Chain"></a>Brain Chain</h3><p>大きな課題などの意思決定の最適解を見つけたい。脳を計算資源とし、大勢から余っている脳を提供してもらい、課題を解いてもらう。</p><ul><li>感想：面白いけど実現はだいぶ難しいなー</li></ul><h2 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h2><p>２位が「運転保険」、１位が「脳 MORE BABY」。</p><h2 id="全体的な感想"><a href="#全体的な感想" class="headerlink" title="全体的な感想"></a>全体的な感想</h2><p>脳波デバイスという制約に縛られない自由なアイデアが多く、聞いていて楽しかった。個人的に一番好きなのはひとめぼれ検知デバイスだった。たぶん実現は可能だと思うし、需要もあると思う。ぜひサービス化してほしい。逆に、脳波デバイスじゃないと実現できないようなアイデアは少なかった。安価な代替デバイスがあると旨味が減るだろうから、ちゃんと脳波を活かせるアイデアを思いつきたい。</p><p>全然関係ないけど、会場の<a href="https://lodge.yahoo.co.jp/" target="_blank" rel="external">LODGE</a>にたどり着くのがなかなか難しかった。最寄り駅から２０分くらいさまよっていた。</p>]]></content>
    
    <summary type="html">
    
      BMI（Brain Machine Interface）を作る、PGVというベンチャー企業がある。11月、PGVの開催するアイデアソンが開かれた。今回のお題は、高性能ウェアラブル脳波センサーの利用のアイデア出しだ。BMIに興味があったので参加したかったが、アイデアソン枠が埋まっていたので、ブログ枠の観覧者として申し込んだ。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CREPE(A Convolutional REpresentation for Pitch Estimation)使ってみた</title>
    <link href="http://blog.hiroshiba.jp/using-crepe/"/>
    <id>http://blog.hiroshiba.jp/using-crepe/</id>
    <published>2018-05-03T08:34:05.000Z</published>
    <updated>2019-02-22T17:10:09.649Z</updated>
    
    <content type="html"><![CDATA[<p>畳み込みニューラルネットを使ったピッチ推定手法、CREPEが提案された[^1]。<a href="https://pypi.org/project/crepe/" target="_blank" rel="external">PyPI</a>が用意されていて、発話音声にも簡単に適用できそうだったので試してみた。</p><a id="more"></a><p>使用方法はとても簡単で、以下のコマンドを叩けば良い。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install crepe</div></pre></td></tr></table></figure><p>GPUを使ったほうが圧倒的に早い。CREPEはKeras製なので、tensorflow-gpuを導入すればGPU使用可能になる。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install tensorflow-gpu</div></pre></td></tr></table></figure><p><a href="https://github.com/marl/crepe" target="_blank" rel="external">GitHubでの説明</a>によると、コマンドとして使うことが想定されている。コードはちゃんと機能ごとに切り分けられているので、適切にimportすればPython内で使うこともできる。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> librosa</div><div class="line"></div><div class="line"><span class="keyword">from</span> crepe.core <span class="keyword">import</span> build_and_load_model <span class="keyword">as</span> crepe_build</div><div class="line"><span class="keyword">from</span> crepe.core <span class="keyword">import</span> predict <span class="keyword">as</span> crepe_predict</div><div class="line"></div><div class="line">fs = <span class="number">16000</span></div><div class="line">x, _ = librosa.load(<span class="string">'hoge.wav'</span>, sr=fs)</div><div class="line"></div><div class="line">crepe_build()</div><div class="line">t, f0, confidence, _ = crepe_predict(x, sr=fs, viterbi=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">threshold = <span class="number">0.1</span></div><div class="line">f0[confidence &lt; threshold] = <span class="number">0</span></div></pre></td></tr></table></figure><p>CREPEは、ピッチの確率分布のようなものを推定したあと、その分布を元にピッチを定める。<code>viterbi=False</code>（初期値）の場合は確率が最大のピッチを、<code>viterbi=True</code>の場合はHMMを使ってよしなにピッチを推定するっぽい（論文読んでいない）。<code>confidence</code>の値は音声活動の信頼度(0~1)らしい。</p><p>WORLDのHarvestを用いて基本周波数を推定したものと、CREPEを比較してみた。</p><figure>  <figcaption>F0比較</figcaption>  <img src="/using-crepe/f0.png" alt="f0.png" title=""></figure><p><code>viterbi=False</code>だと値が吹っ飛ぶことが多いようだった。</p><p>続いて、推定したf0を元に、WORLDを用いて分析合成してみた。</p><figure>  <figcaption>元音声</figcaption>  <audio src="1_raw.wav" controls></audio></figure><figure>  <figcaption>WORLD</figcaption>  <audio src="2_world.wav" controls></audio></figure><figure>  <figcaption>CREPE w/o viterbi</figcaption>  <audio src="3_crepe.wav" controls></audio></figure><figure>  <figcaption>CREPE w/ viterbi</figcaption>  <audio src="4_crepe_viterbi.wav" controls></audio></figure><p>WORLDが一番良いように聞こえる。CREPEはフレーム時間が0.01秒固定で、WORLDは0.005秒なので、その差が出ているのかもしれない。</p><p>上の音声（約7秒）の推定時間は、Harvestが2.0秒、viterbiなしCREPEが1.0秒、viterbiありが1.3秒程度だった。GPU無しCREPEだと10秒程度かかった。</p><p>CREPEはWORLDほどバッファが必要ない（たぶん）ので、リアルタイム性が必要なサービスに使いやすそうだ。しかし、HMMを利用する場合は、前の音声のコンテキストが必要なはずだが、そこは実装されていないので、自分でどうにかしないといけない。</p><p>作図に用いた<a href="https://gist.github.com/Hiroshiba/ad19bef0200c5f312cb7cbd5678b185c#file-use_crepe-ipynb" target="_blank" rel="external">Jupyter notebookファイル</a>をGistにアップロードした。</p><p>[^1]: <a href="https://arxiv.org/abs/1802.06182" target="_blank" rel="external">CREPE: A Convolutional Representation for Pitch Estimation</a></p>]]></content>
    
    <summary type="html">
    
      畳み込みニューラルネットを使ったピッチ推定手法、CREPEが提案された。PyPIが用意されていて、発話音声にも簡単に適用できそうだったので試してみた。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CycleGANノンパラレル結月ゆかり声質変換やってみた</title>
    <link href="http://blog.hiroshiba.jp/became-yuduki-yukari-with-cycle-gan-power/"/>
    <id>http://blog.hiroshiba.jp/became-yuduki-yukari-with-cycle-gan-power/</id>
    <published>2018-04-21T23:21:21.000Z</published>
    <updated>2019-02-22T17:10:09.657Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目次"><a href="#目次" class="headerlink" title="目次"></a>目次</h3><ul><li>（背景）自分の声を結月ゆかりにしたい。前回はパラレルデータのアライメントが問題になったので、ノンパラレルデータの手法を試したい。</li><li>（手法）CycleGANを使ったノンパラレル声質変換を試みた。</li><li>（結果）アライメントしなくても聞き取れる音声が生成できた。しかし、言語性を保ちつつ声質変換できるパラメータは見つけられなかった。</li><li>（考察）CycleGANを用いて性能の良い声質変換を得るのは難しいと思った。Identity以外のお手頃な制約手法が見つかれば，また挑戦してみたい。</li></ul><p>この記事は、技術系同人誌<a href="http://signico.hi-king.me/" target="_blank" rel="external">SIGNICO vol.5</a>の掲載記事「CycleGANを用いたリアルタイム結月ゆかり声質変換」の結果音声を中心に載せています。詳しい手法や解説などは同人誌の記事をご参照ください。</p><a id="more"></a><h3 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h3><h4 id="フィルタ幅を変えた時の結果"><a href="#フィルタ幅を変えた時の結果" class="headerlink" title="フィルタ幅を変えた時の結果"></a>フィルタ幅を変えた時の結果</h4><figure>  <figcaption>「あらゆる現実を、全て、自分の方へ捻じ曲げたのだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="A01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-A01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「予防や、健康管理、リハビリテーションのための制度を、充実していく必要があろう。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="B01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-B01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「六百人のお客さんの人いきれに、むし暑くて、扇子を使わずにいられない。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="C01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-C01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「十分間の休憩を与えられ、乱れた髪を結い直し、肩の汗をぬぐって、支度部屋で呼吸を整える。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="D01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-D01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「家に来た年賀状は、三百枚ほどで、丁度、出した分と同じぐらいだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="E01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-E01.mp3" controls></audio></td>    </tr>  </table></figure><h4 id="損失の比率を変えたときの結果"><a href="#損失の比率を変えたときの結果" class="headerlink" title="損失の比率を変えたときの結果"></a>損失の比率を変えたときの結果</h4><figure>  <figcaption>「あらゆる現実を、全て、自分の方へ捻じ曲げたのだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-A01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「予防や、健康管理、リハビリテーションのための制度を、充実していく必要があろう。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-B01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「六百人のお客さんの人いきれに、むし暑くて、扇子を使わずにいられない。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-C01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「十分間の休憩を与えられ、乱れた髪を結い直し、肩の汗をぬぐって、支度部屋で呼吸を整える。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-D01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「家に来た年賀状は、三百枚ほどで、丁度、出した分と同じぐらいだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-E01.mp3" controls></audio></td>    </tr>  </table></figure><h4 id="Identity制約を追加した結果"><a href="#Identity制約を追加した結果" class="headerlink" title="Identity制約を追加した結果"></a>Identity制約を追加した結果</h4><figure>  <figcaption>「あらゆる現実を、全て、自分の方へ捻じ曲げたのだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-A01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「予防や、健康管理、リハビリテーションのための制度を、充実していく必要があろう。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-B01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「六百人のお客さんの人いきれに、むし暑くて、扇子を使わずにいられない。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-C01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「十分間の休憩を与えられ、乱れた髪を結い直し、肩の汗をぬぐって、支度部屋で呼吸を整える。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-D01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「家に来た年賀状は、三百枚ほどで、丁度、出した分と同じぐらいだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-E01.mp3" controls></audio></td>    </tr>  </table></figure>]]></content>
    
    <summary type="html">
    
      自分の声を結月ゆかりにしたい。前回はパラレルデータのアライメントが問題になったので、ノンパラレルデータの手法を試したい。CycleGANを使ったノンパラレル声質変換を試みた。アライメントしなくても聞き取れる音声が生成できた。しかし、言語性を保ちつつ声質変換できるパラメータは見つけられなかった。CycleGANを用いて性能の良い声質変換を得るのは難しいと思った。Identity以外のお手頃な制約手法が見つかれば，また挑戦してみたい。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ディープラーニングの力で結月ゆかりの声になってみた</title>
    <link href="http://blog.hiroshiba.jp/became-yuduki-yukari-with-deep-learning-power/"/>
    <id>http://blog.hiroshiba.jp/became-yuduki-yukari-with-deep-learning-power/</id>
    <published>2018-02-13T02:08:25.000Z</published>
    <updated>2019-02-22T17:10:09.744Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目次"><a href="#目次" class="headerlink" title="目次"></a>目次</h3><ul><li>（背景）自分の声を結月ゆかりにしたい。前回はあまりクオリティが良くなかったので、手法を変えて質を上げたい。</li><li>（手法）声質変換を、低音質変換と高音質化の二段階に分けてそれぞれ学習させた。画像分野で有名なモデルを使った。</li><li>（結果）性能が飛躍的に向上し、かなり聞き取れるものになった。</li><li>（考察）精度はまだ改善の余地があり、多対多声質変換にすることで精度が向上すると考えられる。今回の結果を論文化したい。</li></ul><a id="more"></a><h3 id="デモ動画"><a href="#デモ動画" class="headerlink" title="デモ動画"></a>デモ動画</h3><p><script type="application/javascript" src="https://embed.nicovideo.jp/watch/sm32724409/script?w=640&h=360"></script><noscript><a href="http://www.nicovideo.jp/watch/sm32724409" target="_blank" rel="external">ディープラーニングの力で結月ゆかりの声になってみた</a></noscript></p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>多くの人が可愛い女の子になりたいと思っている。ＣＧ技術やモーションキャプチャ技術の向上により、姿は女の子に仮想化できるようになってきた。しかし、声に関してはまだまだ課題が多い。声質変換は「遅延」「音質」「複数話者」などの難しい課題がある。今回は、自分の声を結月ゆかりにするための、低遅延で実現可能な高音質声質変換を目指した。</p><h3 id="手法"><a href="#手法" class="headerlink" title="手法"></a>手法</h3><p>大きく分けて３つの工夫をした。</p><ol><li>画像ディープラーニング分野で性能の良かったモデルを使用した</li><li>声質変換を「低音質声質変換」部分と「高音質化」部分に分けた</li><li>音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った</li></ol><h4 id="画像ディープラーニング分野で性能の良かったモデルを使用した"><a href="#画像ディープラーニング分野で性能の良かったモデルを使用した" class="headerlink" title="画像ディープラーニング分野で性能の良かったモデルを使用した"></a>画像ディープラーニング分野で性能の良かったモデルを使用した</h4><p>画像ディープラーニング分野は音声より数年早く進んでおり、解きたいタスクに対して使用すれば良いモデルが知られている。今回のように、対応関係のあるもの（音声分野で言うところのパラレルデータ）の変換を、少ないデータ数で学習させるには、<a href="https://phillipi.github.io/pix2pix/" target="_blank" rel="external">pix2pixモデル</a>が適している。今回はこのpix2pixモデルを使って声質変換タスクを解いた。</p><h4 id="声質変換を「低音質声質変換」部分と「高音質化」部分に分けた"><a href="#声質変換を「低音質声質変換」部分と「高音質化」部分に分けた" class="headerlink" title="声質変換を「低音質声質変換」部分と「高音質化」部分に分けた"></a>声質変換を「低音質声質変換」部分と「高音質化」部分に分けた</h4><p>声質変換には、複数話者によるペア音声を用意する必要があるが、結月ゆかりは大量の音声データを簡単に得られるのに対し、自分の声を大量に用意するのは容易ではない。そこで、少ないペア音声を使って低音質な声質変換するタスクと、大量の音声を使って高音質化するタスクを分けた。低音質声質変換では、WORLDを用いて自分の音響特徴量から結月ゆかりの音響特徴量を推定する学習を行った。高音質化では、低解像度なスペクトログラムから高解像度なスペクトログラムを推定する学習を行った。</p><h4 id="音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った"><a href="#音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った" class="headerlink" title="音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った"></a>音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った</h4><p>スペクトログラムは時間×周波数辺りの音声振幅であり、どちらの次元でも隣接情報に相関がある。そのため、スペクトログラムを１チャンネルの２次元画像とみなし、その変換には画像用のpix2pixモデルをそのまま適用した。一方、音響特徴量は時間辺りの特徴量であるため、画像用のpix2pixネットワークが適用できない。そこで、新たに1次元用のpix2pixネットワーク構造を提案し、これを音響特徴量の変換に適用した。</p><h3 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h3><p>以前の結果と、高音質化やpix2pixモデルでの特徴量変換の結果を比較した。この記事で用いたコードは<a href="https://github.com/Hiroshiba/become-yukarin" target="_blank" rel="external">become-yukarin Githubリポジトリ</a>で公開している。</p><h4 id="前回の結果"><a href="#前回の結果" class="headerlink" title="前回の結果"></a>前回の結果</h4><p>ベースとなる<a href="../voice-conversion-deep-leanring-and-other-delusions">前回</a>の結果は以下のようになっていた。（発話内容は「僕の声をディープラーニングの力を借りて結月ゆかりにするプロジェクト」）</p><figure>  <figcaption>入力音声</figcaption>  <audio src="0-input.wav" controls></audio></figure><figure>  <figcaption>ベース手法での変換結果</figcaption>  <audio src="0-output.wav" controls></audio></figure><figure>  <figcaption>ベース手法での変換結果スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/0-output-spectrogram.svg" alt="0-output-spectrogram.svg" title=""></figure><p>変換結果は結月ゆかりに近いが、発話内容が明細ではなく、音質も良くない。</p><h4 id="高音質化の結果"><a href="#高音質化の結果" class="headerlink" title="高音質化の結果"></a>高音質化の結果</h4><p>ベース手法の結果に高音質化を適用すると以下のようになった。</p><figure>  <figcaption>高音質化手法での変換結果</figcaption>  <audio src="1-output.wav" controls></audio></figure><figure>  <figcaption>高音質化手法での変換結果スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/1-output-spectrogram.svg" alt="1-output-spectrogram.svg" title=""></figure><p>音質は改善したが、発話内容がまだわかりにくい。</p><h4 id="音響特徴量変換pix2pixモデルの結果"><a href="#音響特徴量変換pix2pixモデルの結果" class="headerlink" title="音響特徴量変換pix2pixモデルの結果"></a>音響特徴量変換pix2pixモデルの結果</h4><p>音響特徴量変換をpix2pixモデルにすると以下のようになった。</p><figure>  <figcaption>pix2pixモデルでの変換結果</figcaption>  <audio src="2-output.wav" controls></audio></figure><figure>  <figcaption>pix2pixモデルでの変換結果スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/2-output-spectrogram.svg" alt="2-output-spectrogram.svg" title=""></figure><p>発話内容が明確にわかるようになった。</p><h4 id="おまけ"><a href="#おまけ" class="headerlink" title="おまけ"></a>おまけ</h4><p>入力データには全く関係のない音声を入力した結果は以下のようになった。</p><figure>  <figcaption>入力音声（<a href="http://www.nicovideo.jp/watch/sm28276238" target="_blank" rel="external">チュルリラ・チュルリラ・ダッダッダ！</a>の一部）</figcaption>  <audio src="3-input-spectrogram.wav" controls></audio></figure><figure>  <figcaption>変換結果</figcaption>  <audio src="3-output-spectrogram.wav" controls></audio></figure><figure>  <figcaption>入力音声スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/3-input-spectrogram.svg" alt="3-input-spectrogram.svg" title=""></figure><figure>  <figcaption>変換結果スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/3-output-spectrogram.svg" alt="3-output-spectrogram.svg" title=""></figure><p>入力音声にかかわらず、変換結果が学習データに近くなるように学習されていることがわかった。</p><h3 id="考察"><a href="#考察" class="headerlink" title="考察"></a>考察</h3><p>pix2pixモデルを適用することで、高音質化ができ、変換結果の視聴性が上がった。</p><p>高音質化が上手く行ったのは、pix2pixモデルがかなり広い範囲（時間方向に5ミリ秒×128≒0.6秒、周波数方向に12000Hz÷1024×128≒1500Hz）の入力を元に推定できること、音声スペクトルは周波数方向に周期性があってスペクトログラムと多層CNNの相性が良いこと、そもそもタスク（結月ゆかり音声合成の高音質化）が簡単だったことが理由に考えられる。</p><p>低音質声質変換が上手く行ったのは、特徴量の次元を下げたことで過学習を防ぎつつGANが有効に働いたこと、pix2pixモデルが時間方向で広い範囲の入力を元に推定できることが理由に考えられる。</p><p>まだ精度は改善の余地がある。実使用時の基本周波数推定のエラーが課題の１つとして挙げられる。スペクトログラムベースのend-to-endな声質変換モデルにしたいが、ディープラーニングを使って位相推定できる上手な手法が思いつかない。</p><p>今後は解析を進めつつ、多対多声質変換を目指したい。また、可能そうであれば論文にしたい。</p><p>（別の記事に試行錯誤や雑多なメモを日記として書きます）</p><h3 id="宣伝＆謝辞"><a href="#宣伝＆謝辞" class="headerlink" title="宣伝＆謝辞"></a>宣伝＆謝辞</h3><p>この記事の内容は、ニコニコ生放送でライブコーディングしていました。僕の<a href="http://com.nicovideo.jp/community/co3686550" target="_blank" rel="external">ニコニココミュ</a>で朝6:30くらいから生放送しているのでよかったらいらしてください。（特に音声信号処理に詳しい方！）</p><p>デモ動画の作成にあたって、上記放送に来て議論してくださった方々にはとても感謝しています。特にサムネイル絵を頂いた@Kyowsukeさんに深く感謝します。ありがとうございます。</p>]]></content>
    
    <summary type="html">
    
      自分の声を結月ゆかりにしたい。前回はあまりクオリティが良くなかったので、手法を変えて質を上げたい。声質変換を、低音質変換と高音質化の二段階に分けてそれぞれ学習させた。画像分野で有名なモデルを使った。性能が飛躍的に向上し、かなり聞き取れるものになった。精度はまだ改善の余地があり、多対多声質変換にすることで精度が向上すると考えられる。今回の結果を論文化したい。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Chainerを使った研究開発時のクラス設計</title>
    <link href="http://blog.hiroshiba.jp/class-design-at-research-and-development-using-chainer/"/>
    <id>http://blog.hiroshiba.jp/class-design-at-research-and-development-using-chainer/</id>
    <published>2017-12-22T19:14:42.000Z</published>
    <updated>2018-02-12T03:44:59.900Z</updated>
    
    <content type="html"><![CDATA[<p>この記事は<a href="https://qiita.com/advent-calendar/2017/chainer" target="_blank" rel="external">Chainer Advent Calendar 2017</a>の23日目の記事です。</p><p>僕は普段、Chainerを使って研究開発しています。このとき、クラスをどう分けるべきかよく悩みます。いろいろやってみてある程度固まってきたので、自分なりにまとめてみました。</p><a id="more"></a><p>ChainerなどのDeepLearningフレームワークを使う理由は大きく分けて３段階ほどあります。</p><ol><li>再現実験</li><li>試行錯誤を伴う実験</li><li>学習済みモデルを用いたシステムづくり</li></ol><p>世の中に転がっているChainerサンプルプログラムは大体(1)のもので、こちらは綺麗にまとまっているものが多いです。一方で、何か新規に実験していると、どうしても試行錯誤が発生してコードが煩雑になります(2)。そしてさらには、(1)や(2)で学習したモデルを使ってサービス応用しようとすることもあります(3)。</p><p>今回は研究開発用のコード、つまり、サービス応用を考えつつ実験コードを書く際に、どうクラスを切っていくべきか考えをまとめます。</p><hr><p>まず、細かいのも含めると、実験コードには次の構成要素があります。</p><ul><li>DataProcess : 入力・出力データの加工する</li><li>Dataset : データをChainer用にまとめる</li><li>Network : 汎用のニューラルネットワーク</li><li>Loss : 損失の取り回し</li><li>Model : ニューラルネットワーク全体</li><li>Updater : モデルの更新（＋データの取り回し）</li><li>Trainer : 便利モジュールとの連携</li></ul><p>規模や実験内容に応じて<code>DataProcess</code>は<code>Dataset</code>に、<code>Network</code>と<code>Loss</code>は<code>Model</code>にまとめることもあります。このうち、学習済みモデルを用いたサービスを作る際に必要なのは、<code>DataProcess</code>と<code>Model</code>だけです。</p><p>それぞれに関して、なんなのか、なぜそれが必要か、どういうときに必要かを書きます。</p><hr><h2 id="DataProcess"><a href="#DataProcess" class="headerlink" title="DataProcess"></a>DataProcess</h2><p>入力データや出力データを加工する関数、もしくは呼び出し可能なオブジェクトです。画像を読み出す、クロップする、線画化する、などなど。これらのデータ処理は、<strong><code>DatasetMixin</code>オブジェクトの<code>get_example</code>メソッドに書くこともできますが、こうしてしまうとあとで流用する際にそのオブジェクトの構造を意識する必要が出てきます。</strong>例えば１枚の画像を加工したいだけでも、<code>DatasetMixin</code>オブジェクトを作成し、<code>get_example(0)</code>しなければいけません。最初からデータを加工する関数を切り出しておけば、後で簡単に流用できます。</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>データをChainer用にまとめるクラスです。<code>DatasetMixin</code>を継承して作るのが一般的です。<code>DataProcess</code>にも書いたとおり、<strong>ここに記述した処理は後で流用しづらいので、なるべく簡単なことしか書かないほうが良い</strong>と思います。僕は<code>DataProcess</code>を１つだけ受け取ってデータ加工する<code>Dataset</code>クラスをよく使っています。</p><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(chainer.dataset.DatasetMixin)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs, data_process)</span>:</span></div><div class="line">    self._inputs = inputs</div><div class="line">    self._data_process = data_process</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> len(self._inputs)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_example</span><span class="params">(self, i)</span>:</span></div><div class="line">    <span class="keyword">return</span> self._data_process(self._inputs[i])</div></pre></td></tr></table></figure><h2 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h2><p>汎用のネットワークを書きます。簡単なモデルの場合はなくても良いと思います。僕はよくBatchNormalizationとConvolution2Dをまとめたのを流用しています。</p><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BNConvolution2D</span><span class="params">(chainer.link.Chain)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, ksize, stride=<span class="number">1</span>, pad=<span class="number">0</span>, **kwargs)</span>:</span></div><div class="line">    super().__init__()</div><div class="line">    <span class="keyword">with</span> super().init_scope():</div><div class="line">      self.conv = chainer.links.Convolution2D(in_channels, out_channels, ksize, stride, pad, nobias=<span class="keyword">True</span>, **kwargs)</div><div class="line">      self.bn = chainer.links.BatchNormalization(out_channels)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x)</span>:</span></div><div class="line">    <span class="keyword">return</span> chainer.functions.relu(self.bn(self.conv(x)))</div></pre></td></tr></table></figure><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>損失関数を実装します。簡単なモデルの場合はなくても良いと思います。Chainerの<code>Trainer</code>とloss周りの扱いはややこしく、<code>chainer.report</code>を使ったりする必要があります。<strong><code>Loss</code>クラスの書き方は<a href="https://github.com/chainer/chainer/blob/4ce120d09b6543ae60a6d18830b4345992f1322d/chainer/links/model/classifier.py" target="_blank" rel="external">chainer.links.Classifier</a>がとても参考になります。</strong>コンストラクタで<code>Model</code>オブジェクトを受け取って<code>__call__</code>でフォワードし、得られた出力を元にlossを作ってreturnする設計です。<strong><code>Loss</code>クラスが必要になるのはモデルが２種類以上あるとき</strong>です。DCGANなどのタスクでは<code>Loss</code>クラスを作って、生成器と判別器用のlossを返すと綺麗にコードが書けます。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>ニューラルネットワークをまとめたクラスです。<strong><code>Optimizer</code>１つにつき<code>Model</code>１つ</strong>と考えると理解しやすいです。<code>chainer.link.Chain</code>や<code>chainer.link.ChainList</code>を継承して書くのが一般的です。</p><h2 id="Updater"><a href="#Updater" class="headerlink" title="Updater"></a>Updater</h2><p>こいつがむちゃくちゃしんどいです。<code>Model</code>が１つしかなければ<code>chainer.training.StandardUpdater</code>を使うと大体うまく行きます。<code>Model</code>が複数ある場合、<code>StandardUpdater</code>を継承した<code>Updater</code>クラスを自分で定義し、データの流れとモデルの更新を自分で書く必要があります。<a href="https://github.com/chainer/chainer/blob/7d0d6e70aab9763727802e2a8524744687e9086d/examples/dcgan/updater.py#L10" target="_blank" rel="external">DCGANのサンプル実装</a>でちょっと雰囲気がつかめると思います。<code>Loss</code>クラスをうまく切り出せてさえいればある程度綺麗に書けます。</p><h2 id="Trainer"><a href="#Trainer" class="headerlink" title="Trainer"></a>Trainer</h2><p>Chainerが用意した学習用のクラスです。<code>Updater</code>や<code>Model</code>を与えるとよしなに色々やってくれます。これに関してはいろんな記事があるので説明は割愛します。</p><hr><p>これらの方式で実験コードを書くと、ある程度煩雑になってきても大規模な改修は発生しづらくなります。また、<code>DataProcess</code>と<code>Model</code>をライブラリ化すれば、サービス応用も比較的簡単に行なえます。</p><p>Chainerは柔軟でいろんなクラス設計が可能です。試行錯誤を伴う実験をしていてもコードが散らばらないような設計があれば、ぜひ教えてください。開発方針を自分の中で持っておいて、どんどん研究開発していきたいものです。</p>]]></content>
    
    <summary type="html">
    
      この記事はChainer Advent Calendar 2017の23日目の記事です。僕は普段、Chainerを使って研究開発しています。このとき、クラスをどう分けるべきかよく悩みます。いろいろやってみてある程度固まってきたので、自分なりにまとめてみました。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>DeepLearningでも声質変換したい！</title>
    <link href="http://blog.hiroshiba.jp/voice-conversion-deep-leanring-and-other-delusions/"/>
    <id>http://blog.hiroshiba.jp/voice-conversion-deep-leanring-and-other-delusions/</id>
    <published>2017-12-09T18:39:08.000Z</published>
    <updated>2018-02-13T02:15:26.103Z</updated>
    
    <content type="html"><![CDATA[<p>これは<a href="https://qiita.com/advent-calendar/2017/dwango" target="_blank" rel="external">ドワンゴ Advent Calendar 2017</a>の9日目の記事です。</p><p>漫画やアニメを見ていると、可愛い女の子になって可愛い女の子と他愛もない会話をして過ごす日常に憧れます。そんな感じで、可愛い女の子になりたい人は多いと思います[^1]。しかし残念なことに、現在の技術で真の可愛い女の子になるのはとても難しいです。じゃあせめて仮想でいいから可愛い女の子になりたいですよね（バーチャルyoutuberキズナアイみたいな）。しかし、仮に姿を可愛い女の子にしても、声が可愛くなければ願いは叶いません。ということで、声を可愛くする声質変換を目指してみました。今回は僕の声をDeepLearningの力を借りて結月ゆかりにしました。</p><a id="more"></a><h2 id="お勉強"><a href="#お勉強" class="headerlink" title="お勉強"></a>お勉強</h2><p>まずは音声の勉強をします。これが一番時間かかりました。最近の音声合成手法は3種類あります。</p><ul><li>音響特徴量+vocoder</li><li>wavenet</li><li>STFT+位相推定</li></ul><p>今回使ったのは音響特徴量+vocoderですが、紹介がてらこれらを簡単に説明していきます。</p><h3 id="音響特徴量-vocoder"><a href="#音響特徴量-vocoder" class="headerlink" title="音響特徴量+vocoder"></a>音響特徴量+vocoder</h3><p>声と音響特徴量（基本周波数、スペクトラム、非周期信号）を相互変換して音声合成する手法です。基本周波数は声の高さ、スペクトラムは声質、非周期信号は子音とかの情報をうまくコードすることが期待されています。声と音響特徴量を相互変換する仕組みがvocoderです。話者Aの音声から、話者Bの対応する基本周波数・スペクトラムを推定することで声質変換ができます。非周期信号は推定無しで、そのまま転写すればいいとのこと。数年前まで、音響特徴量の推定にはGMMが用いられてきましたが、近年はDeepで換装した報告が多くあります。</p><h3 id="wavenet"><a href="#wavenet" class="headerlink" title="wavenet"></a>wavenet</h3><p>自己回帰モデルを使って生の音声を直接推定する手法です。とても綺麗な音声を合成できることが知られていますが、学習に時間がかかります。自己回帰モデルをなので当然生成に時間がかかります。が、後者の問題に関しては、先月高速に音声合成できるらしい手法が提案されました。お金とデータがいっぱいあるならこの手法が一番良さそうです。</p><h3 id="STFT-位相推定"><a href="#STFT-位相推定" class="headerlink" title="STFT+位相推定"></a>STFT+位相推定</h3><p>生の音声よりは推定しやすそうなSTFTを経由して音声合成する手法です。STFTを推定した後、さらに位相推定して音声を復元します。位相推定には古き良き手法がよく用いられますが、この手法は推定に時間がかかるので、リアルタイム音声合成には向きません。STFTは画像なので、画像生成分野の手法をそのままこれに適用するのが流行るかと思ってましたが、全然流行りませんでした。</p><h3 id="データセットの準備"><a href="#データセットの準備" class="headerlink" title="データセットの準備"></a>データセットの準備</h3><p><a href="http://www.ah-soft.com/voiceroid/yukari/" target="_blank" rel="external">voiceloid2の結月ゆかり</a>の音声と、自分の音声を用いました。まず結月ゆかりに５０３文読んでもらい、それを僕が読みます。僕は下手くそだったのと、録り直ししたのとで十数時間かかりました。もう二度とやりたくありません。波形をスペクトログラムにしたのがこちらで、見て分かる通り音声がズレています。</p><figure>  <figcaption>話者１のスペクトログラム</figcaption>  <img src="/voice-conversion-deep-leanring-and-other-delusions/raw-melspec-1-000.svg" alt="raw-melspec-1-000.svg" title=""></figure><figure>  <figcaption>話者２のスペクトログラム</figcaption>  <img src="/voice-conversion-deep-leanring-and-other-delusions/raw-melspec-2-000.svg" alt="raw-melspec-2-000.svg" title=""></figure><p>統計的声質変換の学習には、このズレが無い綺麗なデータが必要です。そのため音声アライメントする必要があります。今回は無音区間を省いてMFCCを求め、DTWを使ってアライメントしました（詳細：<a href="../sandbox-alignment-voice-actress-data/">声優統計コーパスをアライメントしてみる</a>）。あとついでに、特徴量がN(0,1)になるように正規化しました。</p><h2 id="モデル"><a href="#モデル" class="headerlink" title="モデル"></a>モデル</h2><p>時系列データを生成でき、かつ変換手法に適していそうなresidual構造のあるものを使います。ちょうどよかったので音声合成で良い性能を叩き出した<a href="https://arxiv.org/abs/1703.10135" target="_blank" rel="external">Tacotronモデル</a>の一部を使いました。</p><h2 id="学習条件"><a href="#学習条件" class="headerlink" title="学習条件"></a>学習条件</h2><p>損失関数は目標の音声と合成した音声の平均絶対誤差にしました。</p><p>音響特徴量は無声区間とそうじゃない区間で意味が変わってきます。例えば無声区間では基本周波数に意味がありません。僕はこの部分の損失が0になるようにマスクして学習させました。</p><p>他にもここに書いてないような細かい処理を色々しています。他の条件はまあ<a href="https://github.com/Hiroshiba/become-yukarin" target="_blank" rel="external">ソースコード</a>を参照ということで。。</p><h2 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h2><p>学習させてみました。全然ダメでした。</p><figure>  <figcaption>損失のグラフ</figcaption>  <img src="/voice-conversion-deep-leanring-and-other-delusions/loss-bad.png" alt="loss-bad.png" title=""></figure><p>めっちゃ過学習します。モデルを弱くしてもやっぱり過学習します。時間がなかったので、入出力に大量にノイズ乗せて学習することにしました。学習したモデルを使って学習データを変換した結果はこうなりました。</p><figure>  <figcaption>声質変換結果</figcaption>  <audio src="output-hiho-pause-atr-A01.mp3" controls></audio></figure><p>変換された音声はまあ、四捨五入すると結月ゆかりでした。ちなみに入力音声と目標音声はこんな感じです。</p><figure>  <figcaption>入力音声</figcaption>  <audio src="input-hiho-pause-atr-A01.mp3" controls></audio></figure><figure>  <figcaption>目標音声</figcaption>  <audio src="target-yukari-pause-atr-A01.mp3" controls></audio></figure><p>学習済みモデルを使っていろいろ試してみました。</p><h3 id="テスト音声"><a href="#テスト音声" class="headerlink" title="テスト音声"></a>テスト音声</h3><p>先ほどの例は学習に用いた音声を変換しました。学習に用いていないテスト音声を変換してみます。</p><figure>  <figcaption>入力音声「僕の声をDeepLearningの力を借りて、結月ゆかりにするプロジェクト」</figcaption>  <audio src="input-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><figure>  <figcaption>変換結果「僕の声をDeepLearningの力を借りて、結月ゆかりにするプロジェクト」</figcaption>  <audio src="output-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><p>学習に用いた音声と同程度のクオリティでした。長い音声だと最後の方の声がふにゃふにゃになる傾向がありました。</p><h3 id="歌ってみた"><a href="#歌ってみた" class="headerlink" title="歌ってみた"></a>歌ってみた</h3><p>歌は母音の期間が長いので声質変換の結果がよく聞こえるらしいです。ということで歌ってみました。</p><figure>  <figcaption>入力音声「かえるのうたが」</figcaption>  <audio src="input-test-kaeru.mp3" controls></audio></figure><figure>  <figcaption>変換結果「かえるのうたが」</figcaption>  <audio src="output-test-kaeru.mp3" controls></audio></figure><p>音程がだいぶ揺れていました。歌は苦手なようです。</p><h3 id="ささやき声"><a href="#ささやき声" class="headerlink" title="ささやき声"></a>ささやき声</h3><p>可愛い女の子に耳元でささやかれたい人は多いと思います。ささやき声は母音が消えやすく、結構特殊な音声らしいです。ということで、ささやき声も変換してみました。</p><figure>  <figcaption>入力音声「お姉ちゃんのねぼすけ」（2014年春アニメ「ご注文はうさぎですか？」最終羽のチノがココアに言うセリフ）</figcaption>  <audio src="input-test-oneechan-no-nebosuke.mp3" controls></audio></figure><figure>  <figcaption>変換結果「お姉ちゃんのねぼすけ」</figcaption>  <audio src="output-test-oneechan-no-nebosuke.mp3" controls></audio></figure><p>だいぶぶっ壊れていました。結構難しいようです。</p><h3 id="他の人の音声"><a href="#他の人の音声" class="headerlink" title="他の人の音声"></a>他の人の音声</h3><p>これは「僕の声を」結月ゆかりにするプロジェクトです。この世に結月ゆかりは３人も必要ありません。他の人の声も変換できちゃうとダメです。僕の声だけを使って学習させたモデルを使って、他の人の音声も変換してみました。</p><figure>  <figcaption>入力音声（僕）「ありがとうございます」</figcaption>  <audio src="input-test-thankyou.mp3" controls></audio></figure><figure>  <figcaption>↑の変換結果</figcaption>  <audio src="output-test-thankyou.mp3" controls></audio></figure><figure>  <figcaption>入力音声（@lamazeP）「ありがとうございます」</figcaption>  <audio src="input-test-chino-thankyou.mp3" controls></audio></figure><figure>  <figcaption>↑の変換結果</figcaption>  <audio src="output-test-chino-thankyou.mp3" controls></audio></figure><figure>  <figcaption>入力音声（@AKIKAZEMOMIJInico）「ありがとうございます」</figcaption>  <audio src="input-test-momiji-thankyou.mp3" controls></audio></figure><figure>  <figcaption>↑の変換結果</figcaption>  <audio src="output-test-momiji-thankyou.mp3" controls></audio></figure><p>他の人の音声でもなんとなく変換できていましたが、僕の声が一番綺麗に変換できていました。</p><h2 id="その他試行錯誤"><a href="#その他試行錯誤" class="headerlink" title="その他試行錯誤"></a>その他試行錯誤</h2><h3 id="DRAGAN"><a href="#DRAGAN" class="headerlink" title="DRAGAN"></a>DRAGAN</h3><p><a href="https://arxiv.org/abs/1705.07215" target="_blank" rel="external">DRAGAN</a>は、画像生成分野でわりといい結果を出してる手法です。この手法を適用した判別器に、<a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="external">pix2pix</a>のように条件付けした音声を入力して学習してみました。まだパラメータ調整がうまく行っていないのかきれいな結果は得られていません。代わりに、実験者の正気度を下げるような結果が生成されました。</p><figure>  <figcaption>入力音声「僕の声をDeepLearningの力を借りて、結月ゆかりにするプロジェクト」</figcaption>  <audio src="input-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><figure>  <figcaption>変換結果１</figcaption>  <audio src="dragan10-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><figure>  <figcaption>変換結果２</figcaption>  <audio src="dragan30-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><p>これはこれで何かに使えそうですね。</p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>この程度のクオリティじゃ全然満足できないので、ちょくちょく改良していきたいと思います。あと、音声について色々学べたので、今後も色々やっていきたいです。成功したら、VRで結月ゆかり[^1]になって、フル仮想してみたいです。</p><p>[^1]: <a href="https://qiita.com/toRisouP/items/14fe62f89808013f9f6e" target="_blank" rel="external">VRで「結月ゆかり」になって生放送する @toRisouP</a></p>]]></content>
    
    <summary type="html">
    
      漫画やアニメを見ていると、可愛い女の子になって可愛い女の子と他愛もない会話をして過ごす日常に憧れます。今回は僕の声をDeepLearningの力を借りて結月ゆかりにしました。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>マストドンアイドル チュートリアル</title>
    <link href="http://blog.hiroshiba.jp/mastodon-idol-tutorial/"/>
    <id>http://blog.hiroshiba.jp/mastodon-idol-tutorial/</id>
    <published>2017-11-30T14:52:19.000Z</published>
    <updated>2017-12-03T20:31:32.294Z</updated>
    
    <content type="html"><![CDATA[<h3 id="なに？"><a href="#なに？" class="headerlink" title="なに？"></a>なに？</h3><p>マストドンアイドルに厳格な定義はありません。その意味は、漠然と、<a href="https://github.com/tootsuite/mastodon" target="_blank" rel="external">マストドン</a>で人気のある人を指しています。狭義には、<a href="https://friends.nico/" target="_blank" rel="external">friends.nico</a>で生放送する人のことを指します。</p><h3 id="だれが？"><a href="#だれが？" class="headerlink" title="だれが？"></a>だれが？</h3><p>創作活動をしている、あるいはしたいと思っているが、モチベーションが湧かない人向けです。</p><h3 id="なぜ？"><a href="#なぜ？" class="headerlink" title="なぜ？"></a>なぜ？</h3><p>チャット感覚での交流が行われるマストドンは、創作活動のフィードバックを得られやすいためです。</p><a id="more"></a><p>何かしらの才能を持っているけど、それを日常でいまいち発揮できないクリエイターがたくさんいます。そういう人たちはたまに、ニコ動投稿者や、Youtuberを目指します。しかし、これらはファンの獲得が難しく、最初に人気を獲得するまでに挫折してしまうことが多いです。</p><p>それらのアイドルを目指す前に、SNSで一定のファンを得ておくと、初めたばかりでもフィードバックがもらえ、やる気に繋がります。そのSNSとして、チャット感覚でユーザー間交流ができるマストドンが適しています。</p><h3 id="どこで？"><a href="#どこで？" class="headerlink" title="どこで？"></a>どこで？</h3><p>ユーザー数が多い、クリエイターが多い、同趣味の人がいる可能性が高いマストドンインスタンスだと良いです。</p><p>創作活動の内容によって最適なインスタンスは変わります。専用のインスタンスでやっても良いですが、 <em>チャット文化が根付いていてユーザー数も多い<a href="https://friends.nico/" target="_blank" rel="external">friends.nico</a></em> で始めることをおすすめします。</p><h3 id="どうやって？"><a href="#どうやって？" class="headerlink" title="どうやって？"></a>どうやって？</h3><p>まずマストドンを初めてチャットベースで知り合いを増やします。続いて創作活動の経験があればその体験を、なければ抱負を語ります。興味を示してくれた知り合いが、あなたのファンです。</p><p>マストドンでのアイドル活動の流れを、例で紹介します。</p><h4 id="自信のある創作活動がある例（歌がうまい）"><a href="#自信のある創作活動がある例（歌がうまい）" class="headerlink" title="自信のある創作活動がある例（歌がうまい）"></a>自信のある創作活動がある例（歌がうまい）</h4><ol><li>マストドンを始める</li><li>ユーザーとチャットベースで交流し、知り合いを増やす</li><li>会話の流れで出た曲を歌った録音をアップロードするなど、歌がうまいことを伝える</li><li>知り合いが興味を持つ（＝ファン）</li><li>生放送など、よりコストの掛かる創作活動をする</li></ol><p>会話の流れで出た何かを使うことで、知り合いが興味を持ってくれやすくなります。会話はチャットベースで進みます。そのため脈絡のない発言は、ほとんどの場合で興味も持たれません。</p><p>脈絡があれば興味は持たれます。誰かのアイコンを描いたり、ショートストーリーを書いたり、誰かの放送の発言をMADにしたり、マストドン用の簡単なシステムを作ったり、手法は様々です。</p><h4 id="自信のある創作活動がないが、強い興味がある例（ゲーム作りたい）"><a href="#自信のある創作活動がないが、強い興味がある例（ゲーム作りたい）" class="headerlink" title="自信のある創作活動がないが、強い興味がある例（ゲーム作りたい）"></a>自信のある創作活動がないが、強い興味がある例（ゲーム作りたい）</h4><ol><li>マストドンを始める</li><li>ユーザーとチャットベースで交流し、知り合いを増やす</li><li>会話の流れで、ゲームを作りたいことを伝える</li><li>知り合いが興味を持つ（＝ファン）</li><li>ゲーム作成の本を買うなど、よりコストの掛かる創作活動をする</li></ol><p>これも同じく、会話の流れで発言することで興味を持ってくれやすくなります。突然言っても、スルーされたり反応が少なかったりします。</p><h3 id="いつ？"><a href="#いつ？" class="headerlink" title="いつ？"></a>いつ？</h3><h4 id="まだアカウントを持っていない場合"><a href="#まだアカウントを持っていない場合" class="headerlink" title="まだアカウントを持っていない場合"></a>まだアカウントを持っていない場合</h4><p>今すぐでも良いですが、新規参入者が多いタイミングで入るとより良いです。インスタンスには独自の文化が根づいています。新参が自分ひとりだと、昔からいる人たちの輪に入りづらいためです。friends.nicoであれば、ニコニコのバージョンアップの際が最も良いでしょう。他にも、大手ITサイトがマストドン関連の記事を出した際など、イベントがあったタイミングがベストです。</p><h4 id="既にアカウントがある場合"><a href="#既にアカウントがある場合" class="headerlink" title="既にアカウントがある場合"></a>既にアカウントがある場合</h4><p>ある程度仲の良い知り合いが増えた段階だと良いです。10人程度から挨拶を貰えるようになった段階が理想です。</p><h3 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h3><p>マストドンにいろんなクリエイターが集まって欲しいなー、という願いがあってこんな記事を書きました。</p><p>僕自身、friends.nicoでマストドンアイドル活動（創作活動＋生放送）やってます。内容は主にプログラミングやDeeeeeeepLearningです。良ければ<a href="http://com.nicovideo.jp/community/co3686550" target="_blank" rel="external">茶化しに来て</a>ください。</p><p>この記事は、<a href="https://adventar.org/calendars/2220" target="_blank" rel="external">friends.nicoアドベントカレンダー</a>初日の投稿記事です。12月1日の0時に間に合わせる予定でした。しかし、<a href="https://twitter.com/hiho_karuta/status/936153544800976896" target="_blank" rel="external">11月30日現在はタイに出張</a>しており、記事を書き始めた22時には日本時間で12月になってしまっていました。すみませんでした。でも、間に合わせるつもりでした。許してください。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;なに？&quot;&gt;&lt;a href=&quot;#なに？&quot; class=&quot;headerlink&quot; title=&quot;なに？&quot;&gt;&lt;/a&gt;なに？&lt;/h3&gt;&lt;p&gt;マストドンアイドルに厳格な定義はありません。
その意味は、漠然と、&lt;a href=&quot;https://github.com/tootsuite/mastodon&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;マストドン&lt;/a&gt;で人気のある人を指しています。
狭義には、&lt;a href=&quot;https://friends.nico/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;friends.nico&lt;/a&gt;で生放送する人のことを指します。&lt;/p&gt;
&lt;h3 id=&quot;だれが？&quot;&gt;&lt;a href=&quot;#だれが？&quot; class=&quot;headerlink&quot; title=&quot;だれが？&quot;&gt;&lt;/a&gt;だれが？&lt;/h3&gt;&lt;p&gt;創作活動をしている、あるいはしたいと思っているが、モチベーションが湧かない人向けです。&lt;/p&gt;
&lt;h3 id=&quot;なぜ？&quot;&gt;&lt;a href=&quot;#なぜ？&quot; class=&quot;headerlink&quot; title=&quot;なぜ？&quot;&gt;&lt;/a&gt;なぜ？&lt;/h3&gt;&lt;p&gt;チャット感覚での交流が行われるマストドンは、創作活動のフィードバックを得られやすいためです。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>声優統計コーパスをアライメントしてみる</title>
    <link href="http://blog.hiroshiba.jp/sandbox-alignment-voice-actress-data/"/>
    <id>http://blog.hiroshiba.jp/sandbox-alignment-voice-actress-data/</id>
    <published>2017-11-03T10:58:12.000Z</published>
    <updated>2018-02-12T03:44:59.903Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目次"><a href="#目次" class="headerlink" title="目次"></a>目次</h3><ul><li>（背景）声質変換用のデータを作るために音声アライメントを試してみたい</li><li>（手法）<a href="http://voice-statistics.github.io/" target="_blank" rel="external">声優統計コーパス</a>のデータを使用し、MFCCでアライメントした</li><li>（結果）アライメント後の音声はところどころ伸びていた。無音とする閾値を下げると伸びは抑制された。</li><li>（考察）もっと完璧に揃うと思っていた。</li></ul><figure>  <figcaption>話者１のメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-1-000.svg" alt="muteup-aligned-melspec-1-000.svg" title=""></figure><figure>  <figcaption>話者２のメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-2-000.svg" alt="muteup-aligned-melspec-2-000.svg" title=""></figure><a id="more"></a><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>DeeeeepLearingを使って声質変換してみたい。どうやら最初に、違う話者による同じ発話内容の音声を揃える必要があるらしい。声質変換用のデータを作るために音声アライメントを試してみた。</p><h3 id="手法"><a href="#手法" class="headerlink" title="手法"></a>手法</h3><p>データセットは<a href="http://voice-statistics.github.io/" target="_blank" rel="external">声優統計コーパス</a>の音声データを使用した。話者は<code>fujitou_normal</code>（話者１）と<code>tsuchiya_normal</code>（話者２）を選んだ。</p><p>アライメント用の特徴量はいろんな文献に従いMFCCとした。MFCC抽出には<a href="http://ml.cs.yamanashi.ac.jp/world/" target="_blank" rel="external">World</a>と、そのpythonラッパー<a href="https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder" target="_blank" rel="external">pyworld</a>を用いた。</p><p>音声のアライメントはMFCCにDTWを適用した結果を用いた。DTWは<a href="https://r9y9.github.io/nnmnkwii/latest/nnmnkwii_gallery/notebooks/vc/01-GMM%20voice%20conversion%20%28en%29.html" target="_blank" rel="external">音声変換の実装例</a>アライメントされた音声の生成は、DTW結果のindexに従ってSTFTを並び替え、STFTを逆変換して求めた。</p><p>比較用にメルスペクトログラムを求め、作図に用いた。これの生成は<a href="https://github.com/keithito/tacotron/blob/master/util/audio.py" target="_blank" rel="external">tacotronの実装例</a>を参考にした。</p><p>コードは<a href="https://gist.github.com/Hiroshiba/25fee12b3e51b2209b249fdfbb6ade88" target="_blank" rel="external">gist</a>に公開した。無音とする閾値は<code>librosa.effects.split</code>の<code>top_db</code>で変更できる。</p><h3 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h3><h4 id="アライメントされた音声"><a href="#アライメントされた音声" class="headerlink" title="アライメントされた音声"></a>アライメントされた音声</h4><p>話者１と話者２の音声をアライメント結果は以下のようになった。</p><figure>  <figcaption>話者１の元ボイス</figcaption>  <audio src="raw-voice-1-000.wav" controls></audio></figure><figure>  <figcaption>話者２の元ボイス</figcaption>  <audio src="raw-voice-2-000.wav" controls></audio></figure><figure>  <figcaption>アライメントされた話者１のボイス</figcaption>  <audio src="aligned-voice-1-000.wav" controls></audio></figure><figure>  <figcaption>アライメントされた話者２のボイス</figcaption>  <audio src="raw-voice-2-000.wav" controls></audio></figure><p>前半はきれいに揃っているが、後半の最後は伸びた感じになっていた。</p><h4 id="アライメントされたメルスペクトログラム"><a href="#アライメントされたメルスペクトログラム" class="headerlink" title="アライメントされたメルスペクトログラム"></a>アライメントされたメルスペクトログラム</h4><p>先程の音声のメルスペクトログラムは以下のようになった。</p><figure>  <figcaption>話者１の元ボイスのメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/raw-melspec-1-000.svg" alt="raw-melspec-1-000.svg" title=""></figure><figure>  <figcaption>話者２の元ボイスのメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/raw-melspec-2-000.svg" alt="raw-melspec-2-000.svg" title=""></figure><figure>  <figcaption>アライメントされた話者１の元ボイスのメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/aligned-melspec-1-000.svg" alt="aligned-melspec-1-000.svg" title=""></figure><figure>  <figcaption>アライメントされた話者２の元ボイスのメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/aligned-melspec-2-000.svg" alt="aligned-melspec-2-000.svg" title=""></figure><p>無音区間の部分アライメントに悪影響しているようだった。</p><h4 id="無音区間の閾値を下げてアライメントした結果"><a href="#無音区間の閾値を下げてアライメントした結果" class="headerlink" title="無音区間の閾値を下げてアライメントした結果"></a>無音区間の閾値を下げてアライメントした結果</h4><p>無音区間とする音圧の閾値を下げてアライメントした。<code>librosa.effects.split</code>の<code>top_db</code>をデフォルト値から<code>20</code>にした。</p><figure>  <figcaption>話者１のボイス</figcaption>  <audio src="muteup-aligned-voice-1-000.wav" controls></audio></figure><figure>  <figcaption>話者２のボイス</figcaption>  <audio src="muteup-aligned-voice-2-000.wav" controls></audio></figure><figure>  <figcaption>話者１のメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-1-000.svg" alt="muteup-aligned-melspec-1-000.svg" title=""></figure><figure>  <figcaption>話者２のメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-2-000.svg" alt="muteup-aligned-melspec-2-000.svg" title=""></figure><p>ところどころブザーのような音になった。よくなっているように思える。</p><h3 id="考察"><a href="#考察" class="headerlink" title="考察"></a>考察</h3><p>DTWの仕組みから考えて無音区間は点的なのだろう。積極的に排除した方がいい。（声質変換の論文の実験データ項目を見ても、MFCCを使った、程度のことしか書いてなかった。）</p><p>DTWで音声が完璧に揃えられると思っていたけど、いまいちだった。とりあえずこのまま機械学習の入出力にしたいと思う。</p><p>MFCCでDTWして別のデータを並び替えるために、DTWAlignerに別のデータを与える設計にしたが、これは良くない。<a href="https://librosa.github.io/librosa/generated/librosa.effects.trim.html#" target="_blank" rel="external">librosa.effects.trim</a>のように、インデックスを返す実装の方がいい。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;目次&quot;&gt;&lt;a href=&quot;#目次&quot; class=&quot;headerlink&quot; title=&quot;目次&quot;&gt;&lt;/a&gt;目次&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;（背景）声質変換用のデータを作るために音声アライメントを試してみたい&lt;/li&gt;
&lt;li&gt;（手法）&lt;a href=&quot;http://voice-statistics.github.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;声優統計コーパス&lt;/a&gt;のデータを使用し、MFCCでアライメントした&lt;/li&gt;
&lt;li&gt;（結果）アライメント後の音声はところどころ伸びていた。無音とする閾値を下げると伸びは抑制された。&lt;/li&gt;
&lt;li&gt;（考察）もっと完璧に揃うと思っていた。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
  &lt;figcaption&gt;話者１のメルスペクトログラム&lt;/figcaption&gt;
  &lt;img src=&quot;/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-1-000.svg&quot; alt=&quot;muteup-aligned-melspec-1-000.svg&quot; title=&quot;&quot;&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;figcaption&gt;話者２のメルスペクトログラム&lt;/figcaption&gt;
  &lt;img src=&quot;/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-2-000.svg&quot; alt=&quot;muteup-aligned-melspec-2-000.svg&quot; title=&quot;&quot;&gt;
&lt;/figure&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ブログ作りました</title>
    <link href="http://blog.hiroshiba.jp/first-commit/"/>
    <id>http://blog.hiroshiba.jp/first-commit/</id>
    <published>2017-09-30T20:12:35.000Z</published>
    <updated>2017-11-02T12:21:42.028Z</updated>
    
    <content type="html"><![CDATA[<p>技術系の日記をつける予定です。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;技術系の日記をつける予定です。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
