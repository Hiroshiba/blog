<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hiho&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://pubsubhubbub.appspot.com" rel="hub"/>
  <link href="http://blog.hiroshiba.jp/"/>
  <updated>2020-12-27T18:57:01.174Z</updated>
  <id>http://blog.hiroshiba.jp/</id>
  
  <author>
    <name>Kazuyuki Hiroshiba</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ディープラーニングの力で誰でもゆかりさんの声になれる声変換技術を作ってみた</title>
    <link href="http://blog.hiroshiba.jp/everybody-yukarin-with-deep-learning-power/"/>
    <id>http://blog.hiroshiba.jp/everybody-yukarin-with-deep-learning-power/</id>
    <published>2020-12-27T18:57:01.000Z</published>
    <updated>2020-12-27T18:57:01.174Z</updated>
    
    <content type="html"><![CDATA[<p>　2年ほど前に、自分の声を<a href="https://www.ah-soft.com/voiceroid/yukari/" target="_blank" rel="noopener">結月ゆかり</a>にする声質変換技術を作り、動画を投稿しました。この技術には利用者の音声データが大量に必要であるという欠点があり、ゆかりさんになりたいというみなさんの願いを叶えるのが難しい状態でした。そこで、この技術を利用者の音声データが不要になるように改良し、誰でも簡単に使えるようにしました。ここではその技術について解説します。</p><a id="more"></a><h3 id="手法"><a href="#手法" class="headerlink" title="手法"></a>手法</h3><p>　音声を直接変換しようとすると、利用者の音声データが必要になってしまいます。そこで、音声を直接変換するのをやめて、①音声を構成する要素である音素と音高に分解し、②音素と音高を目標の声（ゆかりさん）に再合成することを考えました。</p><figure>  <img src="2.png" style="max-height: 18em"></figure><p>　①は、音素の抽出に音声認識と<a href="http://open-jtalk.sp.nitech.ac.jp/" target="_blank" rel="noopener">OpenJTalk</a>と<a href="https://julius.osdn.jp/" target="_blank" rel="noopener">Julius</a>を、音高の抽出に<a href="http://www.kki.yamanashi.ac.jp/~mmorise/world/english/index.html" target="_blank" rel="noopener">WORLD</a>を用いれば簡単に実現できます。そのため、<strong>②さえ実現できれば、利用者の声のデータを用意することなく、誰でもゆかりさんの声に変換することができるようになります。</strong></p><p>　音素と音高からゆかりさんの声を再合成する部分は、入出力データを大量に用意してから、ディープラーニング技術を用いて実現します。</p><figure>  <img src="3.png" style="max-height: 16em"></figure><p>　まず、VOICEROID2結月ゆかりを用いて、ゆかりさんの音声データを大量に用意します。ゆかりさんの音声から先程の方法を使って音素と音高を抽出します。音声データは約15,000ほど用意しました。今回はゆかりさんの音声データのみを用いたので1万超えの大量のデータが必要でしたが、<a href="https://dmv.nico/ja/articles/seiren_voice/" target="_blank" rel="noopener">Seiren Voice</a>のように色んな人の声を用いれば、1人あたりのデータ量はもっと少なくできると思います。</p><p>　音素と音高を入力データに、音声データを出力データとして、ディープラーニングを行います。今回はディープラーニングモデルとして<a href="https://arxiv.org/abs/1802.08435" target="_blank" rel="noopener">WaveRNN</a>を用いました。</p><h3 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h3><h4 id="「ということで、結月ゆかり実況でした。ばいばい！」"><a href="#「ということで、結月ゆかり実況でした。ばいばい！」" class="headerlink" title="「ということで、結月ゆかり実況でした。ばいばい！」"></a>「ということで、結月ゆかり実況でした。ばいばい！」</h4><p>入力話者: 著者</p><figure>  <figcaption>入力音声</figcaption>  <audio src="2-hiho.wav" controls></audio></figure><figure>  <figcaption>変換結果</figcaption>  <audio src="2-yukari.wav" controls></audio></figure><h4 id="「初見さんいらっしゃい、ゆっくりしていってね」"><a href="#「初見さんいらっしゃい、ゆっくりしていってね」" class="headerlink" title="「初見さんいらっしゃい、ゆっくりしていってね」"></a>「初見さんいらっしゃい、ゆっくりしていってね」</h4><p>入力話者: <a href="https://www.a-quest.com/index.html" target="_blank" rel="noopener">AquesTalk 女性1</a></p><figure>  <figcaption>入力音声</figcaption>  <audio src="3-yukkuri.wav" controls></audio></figure><figure>  <figcaption>変換結果</figcaption>  <audio src="3-yukari.wav" controls></audio></figure><h4 id="「トースト２枚と、オレンジジュースをお願いします。」"><a href="#「トースト２枚と、オレンジジュースをお願いします。」" class="headerlink" title="「トースト２枚と、オレンジジュースをお願いします。」"></a>「トースト２枚と、オレンジジュースをお願いします。」</h4><p>入力話者: <a href="https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_corpus" target="_blank" rel="noopener">JVS001</a></p><figure>  <figcaption>入力音声</figcaption>  <audio src="4-jvs.wav" controls></audio></figure><figure>  <figcaption>変換結果</figcaption>  <audio src="4-yukari.wav" controls></audio></figure><h4 id="「ゼロをダイヤルして、オペレーターを呼んでください。」"><a href="#「ゼロをダイヤルして、オペレーターを呼んでください。」" class="headerlink" title="「ゼロをダイヤルして、オペレーターを呼んでください。」"></a>「ゼロをダイヤルして、オペレーターを呼んでください。」</h4><p>入力話者: <a href="https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_corpus" target="_blank" rel="noopener">JVS010</a></p><figure>  <figcaption>入力音声</figcaption>  <audio src="5-jvs.wav" controls></audio></figure><figure>  <figcaption>変換結果</figcaption>  <audio src="5-yukari.wav" controls></audio></figure><h3 id="終わりに"><a href="#終わりに" class="headerlink" title="終わりに"></a>終わりに</h3><p>　この技術はエンタメ分野などでいろんな面白いことに使えるかもしれません。なにかお手伝い・ご協力できることがあれば、ツイッター（<a href="https://twitter.com/hiho_karuta" target="_blank" rel="noopener">@hiho_karuta</a>）等でぜひお気軽にお声掛けください。</p>]]></content>
    
    <summary type="html">
    
      2年ほど前に、自分の声を結月ゆかりにする声質変換技術を作り、動画を投稿しました。この技術には利用者の音声データが大量に必要であるという欠点があり、ゆかりさんになりたいというみなさんの願いを叶えるのが難しい状態でした。そこで、この技術を利用者の音声データが不要になるように改良し、誰でも簡単に使えるようにしました。ここではその技術について解説します。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>GCPでできるだけ安くディープラーニング</title>
    <link href="http://blog.hiroshiba.jp/gcp-low-cost-deeplearning/"/>
    <id>http://blog.hiroshiba.jp/gcp-low-cost-deeplearning/</id>
    <published>2020-07-16T15:31:25.000Z</published>
    <updated>2020-07-16T15:38:30.726Z</updated>
    
    <content type="html"><![CDATA[<p>　私は仕事でも趣味でもディープラーニングをしています。趣味ではいつも<a href="https://colab.research.google.com/notebooks/basic_features_overview.ipynb" target="_blank" rel="noopener">Google Colaboratory</a>を使ってお金をかけずにディープラーニングしていたのですが、Colabは1日12時間ほどしかGPUを使えず、しかも頻繁に学習タスクを回していると弱いGPUしか利用できなくなるので、進捗があまりよくありませんでした。そこで、お金を使って進捗を出すことを考えました。</p><p>　<a href="https://cloud.google.com/gcp/" target="_blank" rel="noopener">Google Cloud Platform（GCP）</a>なら、ちょっと弱めのGPU（Tesla T4）を1時間あたり約12円で借りられます。これならまあ趣味の予算で可能だと感じたので実際にやってみたのですが、GCPは思った以上に複雑で、わかりづらい点が多くありました。そこでこのブログでは、<strong>GCPに登録するところから、１コマンドでディープラーニングできる環境を構築するまでの方法</strong>を紹介します。</p><a id="more"></a><h3 id="手順"><a href="#手順" class="headerlink" title="手順"></a>手順</h3><p><a href="https://cloud.google.com/gcp/" target="_blank" rel="noopener">Google Cloud Platform（GCP）</a>でディープラーニングするためには、GCPに登録したり、ストレージを用意したりする必要があります。このブログでは、次の項目に沿って手順を紹介します。</p><ul><li>準備する（GCPに登録、gcloudをインストール、GCPの設定）</li><li>データセット・学習結果用のストレージを用意する</li><li>学習用のVMイメージを作成する</li><li>自動で学習・停止するスクリプトを書く</li><li>VMインスタンスを起動してディープラーニングする</li><li>学習結果を回収する</li></ul><p>ちなみに、VMとは仮想マシンのことで、GCPでは「<a href="https://cloud.google.com/compute" target="_blank" rel="noopener">Google Compute Engine</a>」と呼ばれています。</p><h4 id="準備する（GCPに登録、gcloudをインストール、GCPの設定）"><a href="#準備する（GCPに登録、gcloudをインストール、GCPの設定）" class="headerlink" title="準備する（GCPに登録、gcloudをインストール、GCPの設定）"></a>準備する（GCPに登録、gcloudをインストール、GCPの設定）</h4><p>　GCPを利用するにはアカウントの登録が必要です。<a href="https://cloud.google.com/" target="_blank" rel="noopener">GCPのページ</a>から登録することができます。また、GCPでGPUを使うには、アカウント登録の他に、支払い方法の登録が必要です。支払い方法はGCPコンソールの<a href="https://console.cloud.google.com/billing" target="_blank" rel="noopener">お支払い</a>で登録することができます。支払い方法の登録に関する詳細は、<a href="https://cloud.google.com/billing/docs/how-to/payment-methods#add_a_payment_method" target="_blank" rel="noopener">Cloud Billingのガイド</a>で確認できます。ちなみに、GCPには<a href="https://cloud.google.com/free" target="_blank" rel="noopener">300ドルの無料クレジット</a>があります。</p><p>　手元のパソコンからコマンドでGCPにアクセスするには、<code>gcloud</code>ツールがとても便利です。このブログの説明では<code>gcloud</code>を使う方法を主に紹介しています。<a href="https://cloud.google.com/sdk/docs/quickstarts" target="_blank" rel="noopener">gcloudのクイックスタート</a>に、OS別のインストール方法が紹介されています。</p><p>　<code>gcloud</code>の設定中に、デフォルトの「リージョン・ゾーン」の入力を求められます。リージョンとはデータセンターの場所で、ゾーンはその中で更に区分けされた場所のことです。場所の決め方は、<a href="https://cloud.google.com/compute/gpus-pricing" target="_blank" rel="noopener">GPUの料金ページ</a>を見て、日本から近くて安い場所にすると良いと思います。また、同じリージョン内でも、ゾーンによって使用可能なGPUの種類が異なる場合があります。リージョン・ゾーンごとの使用可能なGPUの種類は、<a href="https://cloud.google.com/compute/docs/gpus#gpus-list" target="_blank" rel="noopener">GPUの詳細ページ</a>に掲載されています。私は安いGPU（Tesla T4）が使えると嬉しいので、台湾（asia-east1）にしました。</p><p>　GCPでGPUを用いるには、GPUの「割り当て」を申請する必要があります。割り当ては、GCPコンソールの<a href="https://console.cloud.google.com/iam-admin/quotas" target="_blank" rel="noopener">割り当てページ</a>から申請できます。GPUを用いるためには、2つの割り当て項目を申請する必要があります。1つ目は、GPUの合計数です。割り当てページの「フィルタ」に「gpu」と入力して出てくる「GPUs(all regions)」を選択し、「割り当ての編集」ボタンを押します。右側に「連絡先の詳細」入力欄が出てくるので、「名前」「メールアドレス」「電話番号」をそれぞれ入力して次へ進みます。次は「割り当ての変更」入力欄が出てくるので、「新しい上限」に使いたいGPUの数を書いて、「リクエストの説明」に使用目的を書き、「リクエストを送信」ボタンを押します。申請後、しばらくすると申請が通ったことを伝えるメールが来ます。このときのページ画面はこんな感じです。<img src="screen-quota-gpu.png" border="1" style="max-height: 22em"></p><p>　割り当て項目の2つ目は、GPUの種類ごとの数です。先程と同様に割り当てページの「フィルタ」に「gpu」と入力し、好きなGPU（例えばT4の場合は「Commited NVIDIA T4 GPUs」）を選択します。種類ごとのGPUの割り当て申請はリージョンごとに区分されているので、いろんなリージョンの項目が表示されます。その中から、<code>gcloud</code>で設定したリージョンと同じ項目を選択して、「割り当ての編集」ボタンを押します。あとは先程と同様に入力して、割り当てを申請します。このときのページ画面はこんな感じです。<img src="screen-quota-t4.png" border="1" style="max-height: 22em"></p><h4 id="データセット・学習結果用のストレージを用意する"><a href="#データセット・学習結果用のストレージを用意する" class="headerlink" title="データセット・学習結果用のストレージを用意する"></a>データセット・学習結果用のストレージを用意する</h4><p>　データを保存するストレージは、「<a href="https://cloud.google.com/compute/docs/disks" target="_blank" rel="noopener">永続ディスク</a>」と「<a href="https://cloud.google.com/storage/docs" target="_blank" rel="noopener">Cloud Storage</a>」の2種類を目的に分けて使います。永続ディスクはデータ転送速度が早い一方、複数のVMインスタンスから同時に書き込みできないという仕様があります（※正確には、複数のVMインスタンスに書き込み権限付きでマウントすることができません）。逆にCloud Storageは複数のVMインスタンスから同時に書き込めますが、データ転送速度が遅いです。これらの理由から、学習データセットは永続ディスクに、学習結果はCloud Storageに保存することにします。</p><p>　永続ディスクは、GCPコンソールの「<a href="https://console.cloud.google.com/compute/disks" target="_blank" rel="noopener">ディスク</a>」から作成することができます。ディスクページにある「ディスクを作成」ボタンを押すと、ディスク作成のための入力欄が表示されます。入力欄の「名前」に好きな名前を書き、「リージョン」や「ゾーン」には<code>gcloud</code>で設定したものを選びます。「タイプ」や「サイズ」は<a href="https://cloud.google.com/compute/disks-image-pricing#persistentdisk" target="_blank" rel="noopener">料金ページ</a>を参考に予算と相談して、SSDもしくはHDD（標準ディスク）から選びます。それ以外はデフォルトのままにして、作成ボタンを押します。このときのページ画面はこんな感じです。<img src="screen-disk.png" border="1" style="max-height: 22em"></p><p>　永続ディスクが作成できたら、データセットを保存しておきます。永続ディスクにデータを転送するには、VMインスタンスを立ち上げて永続ディスクをマウントし、<code>gcloud</code>コマンドを用いてコピーするのが手っ取り早いです。データセットをコピーするために、まずは作業用の安いVMインスタンスを起動します。GCPコンソールの「<a href="https://console.cloud.google.com/compute/instances" target="_blank" rel="noopener">VMインスタンス</a>」ページを開き、「インスタンスを作成」ボタンを押すと、VMインスタンス作成のための入力欄が表示されます。「名前」に好きな名前を書いて、「リージョン」と「ゾーン」には<code>gcloud</code>で設定したものを選択します。また、「管理、セキュリティ、ディスク、ネットワーク、単一テナンシー」を展開し、「ディスク」タブで「既存のディスクを接続」から、先ほど作成した永続ディスクを選びます。それ以外はデフォルトのままにして、作成ボタンを押します。このときのページ画面はこんな感じです。<img src="screen-instance-dataset.png" border="1" style="max-height: 22em"></p><p>　VMインスタンスが起動したら、<code>gcloud compute ssh</code>でVMインスタンスにssh接続したあと、永続ディスクをマウントします。</p><pre><code class="hljs bash"><span class="hljs-comment"># ssh接続</span>gcloud compute ssh [VMインスタンスの名前]</code></pre><pre><code class="hljs bash"><span class="hljs-comment"># 永続ディスクをマウント</span>mkdir -p [永続ディスクの名前]sudo mount -o discard,defaults /dev/sdb [永続ディスクの名前]</code></pre><p>　マウントが完了したら、手元のパソコンから<code>gcloud compute scp</code>でデータセットを転送することができます。転送後、VMインスタンス内で<code>ls [永続ディスクの名前]</code>することで、ファイルが転送できたことを確認できます。</p><pre><code class="hljs bash">gcloud compute scp --recurse [データセットのパス] [VMインスタンスの名前]:[永続ディスクの名前]</code></pre><p>　続いて、Cloud Storageで、学習結果を保存するためのストレージ（バケット）を作成します。バケットは、GCPコンソールの「<a href="https://console.cloud.google.com/storage/browser" target="_blank" rel="noopener">ストレージ ブラウザ</a>」から作成できます。「バケットを作成」を押すと、バケット作成のための入力欄が表示されます。入力欄の「バケットに名前を付ける」に好きな名前を書き、「ロケーションタイプ」を「Region」にして、「ロケーション」に<code>gcloud</code>で設定したリージョンを入力します。あとは「アクセス制御」を「均一」に設定し、それ以外はデフォルトのままにして、作成ボタンを押します。このときのページ画面はこんな感じです。<img src="screen-storage.png" border="1" style="max-height: 22em"></p><p>　以上でストレージの準備は完了です。</p><h4 id="学習用のVMイメージを作成する"><a href="#学習用のVMイメージを作成する" class="headerlink" title="学習用のVMイメージを作成する"></a>学習用のVMイメージを作成する</h4><p>　GCPにはデフォルトで、PyTorchやTensorFlowがインストールされたVMイメージ（<a href="https://cloud.google.com/ai-platform/deep-learning-vm/docs" target="_blank" rel="noopener">Deep Learning VM</a>）があります。しかしこれらのVMイメージには、NVIDIAドライバなどがインストールされていません。既成のVMイメージにNVIDIAドライバなどをインストールして自分用のVMイメージを作成しておくと、VMイメージの使い回しができて便利です。</p><p>　まず最初に、元となるVMイメージを使ってVMインスタンスを起動します。GCPには<a href="https://cloud.google.com/ai-platform/deep-learning-vm/docs/images" target="_blank" rel="noopener">色々なVMイメージ</a>があり、PyTorchなどのフレームワークがインストール済みのものや、CUDAだけ入っているものなどが用意されています。私はPyTorchを使いたいので、「pytorch-latest-gpu」を選びました。より細かいバージョンを指定したい場合は<code>gcloud compute images list</code>を実行することで確認できます。</p><p>　選んだVMイメージを指定して、<code>gcloud compute instances create</code>でVMインスタンスを起動します。<code>--image-family</code>に選んだVMイメージを指定し、<code>--accelerator</code>には先ほど割り当てを申請したGPUを指定します。ここで指定するGPUのラベル名は、<a href="https://cloud.google.com/compute/docs/gpus#introduction" target="_blank" rel="noopener">GPUの対応表</a>で確認することができます。<code>--metadata=&quot;install-nvidia-driver=True&quot;</code>を指定することで、NVIDIAドライバが自動的にインストールされます。</p><pre><code class="hljs bash">gcloud compute instances create [VMインスタンスの名前] \  --image-project=<span class="hljs-string">"deeplearning-platform-release"</span> \  --image-family=<span class="hljs-string">"pytorch-latest-gpu"</span> \  --metadata=<span class="hljs-string">"install-nvidia-driver=True"</span> \  --accelerator=<span class="hljs-string">"type=nvidia-tesla-t4,count=1"</span> \  --maintenance-policy=<span class="hljs-string">"TERMINATE"</span></code></pre><p>　ちなみに<code>--maintenance-policy=&quot;TERMINATE&quot;</code>は、GPUを使用してVMインスタンスを起動する際に必ず指定する必要があります。引数の詳細は<a href="https://cloud.google.com/ai-platform/deep-learning-vm/docs/cli#creating_an_instance_with_one_or_more_gpus" target="_blank" rel="noopener">こちら</a>に紹介されています。</p><p>　NVIDIAドライバがインストールできたので、このVMインスタンスを再利用することで素早くディープラーニングすることが可能です。が、いつもインストールする他のツールもここでインストールしておくと、VMインスタンスの起動時間を更に減らすことができます。デバッグをしやすくするためのログ出力ツールと、Cloud StorageバケットをマウントするためのFUSEツールも、ここでインストールしておきます。</p><p>　まず、<code>gcloud compute ssh</code>でVMインスタンスにssh接続します。</p><pre><code class="hljs bash"><span class="hljs-comment"># ssh接続</span>gcloud compute ssh [VMインスタンスの名前]</code></pre><p>ssh接続したあと、次のコマンドを実行してツールをインストールします。</p><pre><code class="hljs bash"><span class="hljs-comment"># ログ出力ツールのインストール</span>curl -sS https://dl.google.com/cloudagents/install-logging-agent.sh | sudo bash -</code></pre><pre><code class="hljs bash"><span class="hljs-comment"># FUSEのインストール</span><span class="hljs-built_in">export</span> GCSFUSE_REPO=gcsfuse-`lsb_release -c -s`<span class="hljs-built_in">echo</span> <span class="hljs-string">"deb http://packages.cloud.google.com/apt <span class="hljs-variable">$GCSFUSE_REPO</span> main"</span> |\sudo tee /etc/apt/sources.list.d/gcsfuse.listcurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -sudo apt-get updatesudo apt-get install -y gcsfuse</code></pre><p>他にも、ディープラーニングフレームワークのアップデートなど、時間がかかる環境構築作業もここで行うと良いと思います。全てのインストールが完了したら、VMインスタンスを終了しておきます。</p><pre><code class="hljs bash">sudo shutdown now</code></pre><p>　最後に、色々インストールしたVMインスタンスを元にVMイメージを作成して、再利用できるようにします。VMイメージの作成は、GCPコンソールの「<a href="https://console.cloud.google.com/compute/images" target="_blank" rel="noopener">イメージ</a>」で行えます。「イメージを作成」ボタンを押すと入力欄を表示されるので、「名前」に好きな名前を書き、「ソース」を「ディスク」に選択したあと、「ソースディスク」に先程のVMインスタンスの名前を選択します。あとは「ロケーション」を「リージョン」にして、<code>gcloud</code>で設定したリージョンを選択します。それ以外はデフォルトのままにして、作成ボタンを押します。このときのページ画面はこんな感じです。<img src="screen-image.png" border="1" style="max-height: 22em"></p><p>　以上で学習用のVMイメージの作成は完了です。</p><h4 id="自動で学習・停止するスクリプトを書く"><a href="#自動で学習・停止するスクリプトを書く" class="headerlink" title="自動で学習・停止するスクリプトを書く"></a>自動で学習・停止するスクリプトを書く</h4><p>　全ての準備が完了したので、VMインスタンスを起動してディープラーニングを実行します。起動時間をできるだけ短くするために、起動直後に自動的に学習を開始し、学習が完了したら自動的に停止するようにします。</p><p>　起動直後に学習を開始するためには、VMインスタンスの「起動スクリプト」を使うのが手っ取り早いです。起動直後に実行するスクリプトを書いておき、<code>gcloud</code>でVMインスタンスを起動するときの引数に指定します。以降で、起動スクリプトの内容（自動停止の設定、ディスクのマウント、訓練開始コードなど）を紹介します。</p><p>　まず1行目に、エラー発生時にこのスクリプトの実行を終了させる設定を書きます。</p><pre><code class="hljs bash"><span class="hljs-built_in">set</span> -eu</code></pre><p>起動スクリプトはrootユーザーで実行されます。rootユーザーではPythonへのパスが通っていないので、パスを追加します。</p><pre><code class="hljs bash"><span class="hljs-built_in">export</span> PATH=<span class="hljs-string">"/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:<span class="hljs-variable">$PATH</span>"</span></code></pre><p>続いて、VMインスタンスが自動的に終了するようにします。</p><pre><code class="hljs bash"><span class="hljs-built_in">trap</span> finally EXIT<span class="hljs-keyword">function</span> finally &#123;    shutdown&#125;</code></pre><p>Cloud Storageバケットをマウントして、学習結果を保存できるようにします。この例の場合、<code>/mnt/result</code>ディレクトリに学習結果を保存すると、自動的にバケットに保存されます。</p><pre><code class="hljs bash">mkdir -p /mnt/resultgcsfuse [バケットの名前] /mnt/result</code></pre><p>永続ディスクをマウントして、データセットを読み込めるようにします。この例の場合、<code>/mnt/dataset</code>ディレクトリからデータセットを読み込むことができます。</p><pre><code class="hljs bash">mkdir -p /mnt/dataset/mount -o discard,defaults /dev/sdb /mnt/dataset/</code></pre><p>最後に、訓練用のコードをダウンロードして実行するようにします。この例では、GithubからPyTorchのMNIST分類サンプルコードをダウンロードして実行しています。他にも、Cloud Storageバケットや永続ディスクに訓練コードを保存しておいて、起動スクリプトから実行する方法もあると思います。</p><pre><code class="hljs bash"><span class="hljs-comment"># ダウンロード</span>curl https://raw.githubusercontent.com/pytorch/examples/master/mnist/main.py &gt; train.py<span class="hljs-comment"># 実行</span>python train.py --batch-size 64 --save-model<span class="hljs-comment"># 学習結果を保存</span>cp * /mnt/result/</code></pre><p>これらのコードをまとめて<code>[スクリプト名].sh</code>に保存します。</p><h4 id="VMインスタンスを起動してディープラーニングする"><a href="#VMインスタンスを起動してディープラーニングする" class="headerlink" title="VMインスタンスを起動してディープラーニングする"></a>VMインスタンスを起動してディープラーニングする</h4><p>　ストレージ・VMイメージ・スクリプトの準備ができたので、これらをVMインスタンスの起動コマンドの引数に指定して、ディープラーニングを実行します。<code>--disk</code>でデータセット用の永続ディスクの名前を、<code>--image</code>で作成したVMイメージの名前を、<code>--metadata-from-file=&quot;startup-script=&quot;</code>でスクリプト名を指定します。他にも、<code>--machine-type</code>に<a href="https://cloud.google.com/compute/docs/machine-types" target="_blank" rel="noopener">マシンタイプ</a>を指定することで、CPUやメモリ量を変更できます。</p><pre><code class="hljs bash">gcloud compute instances create [VMインスタンスの名前] \  --disk=<span class="hljs-string">"name=[永続ディスクの名前],auto-delete=no,mode=ro"</span> \  --image=<span class="hljs-string">"[VMイメージの名前]"</span> \  --metadata-from-file=<span class="hljs-string">"startup-script=[スクリプト名].sh"</span> \  --machine-type=<span class="hljs-string">"n1-standard-2"</span> \  --accelerator=<span class="hljs-string">"type=nvidia-tesla-t4,count=1"</span> \  --maintenance-policy=<span class="hljs-string">"TERMINATE"</span> \  --scopes=<span class="hljs-string">"default,storage-full"</span> \  --preemptible \  --no-restart-on-failure \  --no-shielded-integrity-monitoring \</code></pre><p>　紹介していないけど必要な引数も追加しています。<code>--scopes=&quot;default,storage-full&quot;</code>でCloud Storageバケットへのフルアクセス権限を付与しています。<code>--preemptible</code>でプリエンプティブルインスタンス（有効期間が短い代わりに料金が手頃なVMインスタンス）を利用するようにします。<code>--no-restart-on-failure</code>は自動再起動の抑止を、<code>--no-shielded-integrity-monitoring</code>は整合性モニタリングの無効を指定しています。</p><p>　VMインスタンス起動後、正常に学習できていると、学習結果が<a href="https://console.cloud.google.com/storage/browser" target="_blank" rel="noopener">Cloud Storageのバケット</a>にこんな感じで保存されているはずです。<img src="screen-result.png" border="1" style="max-height: 14em"></p><p>　学習結果のダウンロードは、GCPコンソールのバケットから1つずつファイルをダウンロードする方法と、<code>gsutil</code>を用いてディレクトリごとダウンロードする方法があります。<code>gsutil</code>は<a href="https://cloud.google.com/storage/docs/gsutil_install" target="_blank" rel="noopener">GCPのリファレンス</a>に、インストール方法が書かれています。<code>gsutil</code>を使えば、次のようなコマンドで<code>MNIST</code>ディレクトリをダウンロードすることができます。</p><pre><code class="hljs bash">gsutil -m cp -r <span class="hljs-string">"gs://[バケットの名前]/MNIST"</span> ~/Downloads/</code></pre><p>　以上で、GCPを使ってディープラーニングする手順の紹介は終わりです。</p><h3 id="実践"><a href="#実践" class="headerlink" title="実践"></a>実践</h3><p>実際にGCPでディープラーニングしてみて、気になったことや、知っていると役立つかもしれないテクニックを少し紹介します。</p><h4 id="コスト"><a href="#コスト" class="headerlink" title="コスト"></a>コスト</h4><p>　実際のタスクに適用してみたときのコストを計算してみます。StarGAN v2を<a href="https://github.com/Hiroshiba/yukarin_style" target="_blank" rel="noopener">追実装</a>し、StarGAN v2を音声ドメインへ適用する実験をGCPで行ってみました。GCPのリージョンは台湾を選びました。マシンタイプは<code>n1-standard-2</code>（vCPU２つ、メモリ7.5GB）で、プリエンプティブルだと1時間あたり0.022ドルでした。GPUは<code>NVIDIA Tesla T4</code>1つ（GPUメモリ16GB）で、プリエンプティブルだと1時間あたり0.11ドルでした。StarGAN v2の学習は、プリエンプティブルインスタンスの最大時間の24時間でほぼ完了したので、実験1回で3.168ドル≒339円でした。ストレージなどの費用はこれらに比べると微々たるものでした。それ以外で費用は発生しませんでした。</p><p>　過去の経験から、生成系のディープラーニングの追実験は、20回ほど学習を試せば、目的の品質が得られそうかわかる気がします。1日かかる学習を20回行っても339円*20回=6780円です。ゲーム1本くらいだと思えば、まあ趣味の範囲内かな、という感触でした。</p><h4 id="ログの表示"><a href="#ログの表示" class="headerlink" title="ログの表示"></a>ログの表示</h4><p>　ログはすべてGCPコンソールの「<a href="https://console.cloud.google.com/logs" target="_blank" rel="noopener">ログビューア</a>」で見ることができます。VMインスタンス一覧から「ログを表示」を選択して、そのインスタンスのログだけを見ることもできます。24時間経たずにVMインスタンスが終了している場合は、ログの一番最後を見ると終了した理由がわかります。例えばプリエンプトされて早期終了した場合は、最後に「<code>preempted event</code>」が表示されていました。</p><p>　ログはプログレスの表示なども含めて全て記録されていることもあり、ちょっと見づらいです。クエリビルダーに「startup-script」を追記すると、起動スクリプトからのログだけを表示できます。<img src="screen-log.png" border="1" style="max-height: 14em"></p><h4 id="プリエンプト対応"><a href="#プリエンプト対応" class="headerlink" title="プリエンプト対応"></a>プリエンプト対応</h4><p>　プリエンプティブルインスタンスを使う場合、最大時間の24時間までに早期終了する（プリエンプトされる）ことがあります。学習プロセスが異常終了すると学習結果がおかしくなる可能性があるので、プリエンプトを検知したら学習を停止させるようにしました。VMインスタンスがプリエンプトされる対象になったことを検知する方法はいくつかありますが、VMインスタンス内で<a href="https://cloud.google.com/compute/docs/instances/create-start-preemptible-instance#detecting_if_an_instance_was_preempted" target="_blank" rel="noopener">プリエンプトされたかどうかを確認するAPI</a>をロングポーリングする方法が便利でした。例えば以下のように書くことで、プリエンプトを検知した際に、ディープラーニングのプロセスに停止シグナルを送信することができます。</p><pre><code class="hljs bash">python train.py &amp;  <span class="hljs-comment"># ディープラーニングをバックグラウンドプロセスとして起動</span>pid_train=<span class="hljs-string">"$!"</span>  <span class="hljs-comment"># プロセスIDを取得</span>api=<span class="hljs-string">"http://metadata.google.internal/computeMetadata/v1/instance/preempted?wait_for_change=true"</span>curl -sS <span class="hljs-variable">$api</span> -H <span class="hljs-string">"Metadata-Flavor: Google"</span> &amp;  <span class="hljs-comment"># プリエンプトを検知するプロセス</span>pid_preempted=<span class="hljs-string">"$!"</span>  <span class="hljs-comment"># プロセスIDを取得</span><span class="hljs-built_in">wait</span> -n  <span class="hljs-comment"># バックグラウンドプレセスのいずれかが完了するまで待機</span><span class="hljs-built_in">kill</span> -SIGINT <span class="hljs-variable">$pid_train</span>  <span class="hljs-comment"># ディープラーニングのプロセスに停止シグナルを送信</span></code></pre><h4 id="学習パラメータの渡し方"><a href="#学習パラメータの渡し方" class="headerlink" title="学習パラメータの渡し方"></a>学習パラメータの渡し方</h4><p>　ディープラーニングする際、複数のパラメータを与えて複数の学習を同時に行いたいことがあります。GCPで別々の学習パラメータを与える方法を色々調べたところ、起動スクリプトを複製して書き換えるのが手っ取り早そうでした。例えば先ほどのMNIST分類で、バッチサイズを変えて複数のVMインスタンスを起動したいときは、次のように<code>sed</code>コマンドを使うことによって簡単に実現することができます。</p><pre><code class="hljs bash"><span class="hljs-comment"># 起動スクリプトを修正</span>python train.py --batch-size BATCHSIZE --save-model</code></pre><pre><code class="hljs bash"><span class="hljs-comment"># VMインスタンス起動コマンドを修正</span><span class="hljs-keyword">for</span> batchsize <span class="hljs-keyword">in</span> 16 32 64; <span class="hljs-keyword">do</span>  <span class="hljs-comment"># バッチサイズ16, 32, 64</span>  cat [スクリプト名].sh | sed <span class="hljs-string">"s/BATCHSIZE/<span class="hljs-variable">$batchsize</span>/g"</span> &gt; tmp_script.sh  <span class="hljs-comment"># 起動スクリプトを複製</span>    gcloud compute instances create [VMインスタンスの名前] \    --metadata-from-file=<span class="hljs-string">"startup-script=tmp_script.sh"</span> \    （省略）<span class="hljs-keyword">done</span></code></pre><h4 id="終わりに"><a href="#終わりに" class="headerlink" title="終わりに"></a>終わりに</h4><p>Google Colabの制限に疲れていたとき、GCPのプリエンプティブルインスタンスであれば割とお手頃価格でクラウドディープラーニングができることを知り、実際に試してみました。設定は大変でしたが、それなりに使える環境がそれなりの価格で整備できました。もっとお金があったら次はGoogle Kubernetes Engineとか使って、もっとかっこいい仕組みを作ってみたいです。ちなみにStarGAN v2の音声適用ですが、全然ダメでした。ディープラーニングむずかしい。</p><p>　記事に関して何かありましたら、ツイッターの<a href="https://twitter.com/hiho_karuta" target="_blank" rel="noopener">@hiho_karuta</a>までご連絡いただけると嬉しいです。</p><p>　最後に。GCP試行錯誤やブログ執筆は、ずっとニコ生で放送しながら行っていました。助言をくださった方々や、進捗を見守ってくださった方々、ありがとうございました。</p>]]></content>
    
    <summary type="html">
    
      私は仕事でも趣味でもディープラーニングをしています。趣味ではいつもGoogle Colaboratoryを使ってお金をかけずにディープラーニングしていたのですが、Colabは1日12時間ほどしかGPUを使えず、しかも頻繁に学習タスクを回していると弱いGPUしか利用できなくなるので、進捗があまりよくありませんでした。そこで、お金を使って進捗を出すことを考えました。Google Cloud Platform（GCP）なら、ちょっと弱めのGPU（Tesla T4）を1時間あたり約12円で借りられます。これならまあ趣味の予算で可能だと感じたので実際にやってみたのですが、GCPは思った以上に複雑で、わかりづらい点が多くありました。そこでこのブログでは、GCPに登録するところから、１コマンドでディープラーニングできる環境を構築するまでの方法を紹介します。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ディープラーニングで歌声音声合成エンジンを自作する</title>
    <link href="http://blog.hiroshiba.jp/create-singing-engine-with-deep-learning/"/>
    <id>http://blog.hiroshiba.jp/create-singing-engine-with-deep-learning/</id>
    <published>2019-11-30T20:30:00.000Z</published>
    <updated>2020-07-13T16:48:13.828Z</updated>
    
    <content type="html"><![CDATA[<p>この記事は、<a href="https://qiita.com/advent-calendar/2019/dwango" target="_blank" rel="noopener">ドワンゴ Advent Calendar 2019</a>の１日目の記事です。</p><h3 id="モチベーション"><a href="#モチベーション" class="headerlink" title="モチベーション"></a>モチベーション</h3><p>　最近、理想の人工知能（歌ったり踊ったり喋ったりできるキャラクター）を作りたいと思うようになりました。人工知能が歌を歌うためには、歌声音声合成エンジンが必要です。ということで、ディープラーニングを使って、歌声音声合成エンジンの作成に挑戦してみました。この記事では、実際に音声合成した歌声や、その仕組み、別条件での実験結果、ディープラーニング周りの手法を紹介します。</p><a id="more"></a><h3 id="デモ動画"><a href="#デモ動画" class="headerlink" title="デモ動画"></a>デモ動画</h3><p>実際に作成した音声合成エンジンを使った歌唱のデモです。<br>（歌声音声合成を作ったら絶対最初にカバーしたいと思っていた歌、<a href="https://www.nicovideo.jp/watch/sm1274898" target="_blank" rel="noopener">ハジメテノオト</a>のカバーです。）</p><figure>  <figcaption>歌唱デモ音声</figcaption>  <center><video src="demo-w-f0.mp4" controls style="max-height: 15em"></video></center></figure><p>　まだ挑戦し始めてから２週間ほどしか経っていないのでかなり荒削りですが、それでもちゃんと歌詞や音程が取れていたり、溜めの部分で大きく息を吸う音が入っていたりと、そこそこの成果をあげることができました。</p><h3 id="仕組みの紹介"><a href="#仕組みの紹介" class="headerlink" title="仕組みの紹介"></a>仕組みの紹介</h3><p>　歌唱データを用いてディープラーニングして歌声音声合成エンジンを作り、そのエンジンを使って歌を生成しています。生成する歌は、ディープラーニングに用いた歌唱データに含まれないような新しい歌でも大丈夫です。</p><figure>  <figcaption>歌声音声合成エンジンの作成・使用の流れ</figcaption>  <img src="1.png" style="max-height: 19em"></figure><p>　ディープラーニングには、DeepMindやGoogle Brainの方が提唱した<a href="https://arxiv.org/abs/1802.08435" target="_blank" rel="noopener">WaveRNN</a>というネットワーク構造を用いています。サンプリングレートは24000Hz、ビット深度は10bitです（詳細は後述）。</p><p>　今回作った音声合成エンジンでは、次の３つの時系列ラベルを入力すると、歌の音声波形が生成できるようにしました。</p><ul><li>音素（母音や子音）</li><li>音楽の音階（ドレミファソラシド）</li><li>声の高さ（周波数）</li></ul><p>つまり、歌詞と楽譜と声の高さとタイミングを指定すれば、歌が生成される感じです。</p><p>　ディープラーニングには<a href="https://zunko.jp/kiridev/login.php" target="_blank" rel="noopener">東北きりたん歌唱データベース</a>の音声データを活用しました。こちらのデータベースには、東北きりたんというキャラクターがアカペラで歌った歌唱データや音素ラベルが50曲も収録されています。</p><figure>  <figcaption>東北きりたん</figcaption>  <img src="a1zunko100.png" style="max-height: 15em"></figure><h3 id="別条件での実験結果"><a href="#別条件での実験結果" class="headerlink" title="別条件での実験結果"></a>別条件での実験結果</h3><p>　せっかく歌声音声合成エンジンができたので、いろいろな条件で結果がどうなるか実験してみました。</p><h4 id="声の高さラベルを省いてディープラーニングしたとき"><a href="#声の高さラベルを省いてディープラーニングしたとき" class="headerlink" title="声の高さラベルを省いてディープラーニングしたとき"></a>声の高さラベルを省いてディープラーニングしたとき</h4><p>　歌を歌うとき、基本的には音楽の音階に沿って声の高さを変えて歌います。このとき、声の高さにわざと高低をつけて、歌に表現力を持たせることができます（ビブラートなど）。先ほどのデモでは声の高さラベルも入力していますが、これを省いてディープラーニングすれば、声の高さも良い感じに生成してくれて、元の歌声に近い表現になるかもしれません。ということで、声の高さラベルを省いてディープラーニングを行い、その音声合成結果を聞いてみました。</p><figure>  <figcaption>声の高さラベルを省いたときの歌唱デモ</figcaption>  <audio src="demo-wo.wav" controls></audio></figure><p>　結果を聞くと、へにょへにょした歌声になってしまうことがわかりました。こうなった原因はおそらく、今回用いたディープラーニング手法は、時間方向に一貫している音声波形の生成が苦手だったからだと考えています。例えばビブラートは、一定のリズムかつ一定の高低差で声の高さを揺らして表現しますが、今回の手法はこの「一定にする」ことが難しかったんだと思います。</p><h4 id="声の大きさラベルを足してディープラーニングしたとき"><a href="#声の大きさラベルを足してディープラーニングしたとき" class="headerlink" title="声の大きさラベルを足してディープラーニングしたとき"></a>声の大きさラベルを足してディープラーニングしたとき</h4><p>　先ほどのデモを聞いてみると、声の大きさが安定していない印象を受けます。これはおそらく、前述と同様に今回の手法は「一定にする」ことが難しく、声の大きさをうまく調整できないためだと思われます。ということは、声の大きさラベルを足せば、もっと安定した歌声になるはずです。声の大きさラベルを足してディープラーニングし、その結果を聞いてみました。</p><figure>  <figcaption>声の大きさラベルを足したときの歌唱デモ</figcaption>  <audio src="demo-w-volume.wav" controls></audio></figure><p>　結果を聞くと、ノイズのような歌声になってしまうことがわかりました。これは・・・なんでこうなるのか、全く見当がつきません。ディープラーニングはこんな感じで、問題が発生したとき、その問題が真に解決不可能なのか、それともただプログラムがバグっているだけなのかを切り分けづらく、もやもやすることがあります。今回のこの問題は、原因不明です。</p><p>　（2019/12/08追記）声の大きさラベルの値を全体的に小さくしてみたところ、品質がかなり上がりました。ディープラーニングなにもわからない。</p><figure>  <figcaption>声の大きさラベルの値を小さくしたときの歌唱デモ</figcaption>  <audio src="demo-w-volume-2.wav" controls></audio></figure><h3 id="ディープラーニング周りの紹介"><a href="#ディープラーニング周りの紹介" class="headerlink" title="ディープラーニング周りの紹介"></a>ディープラーニング周りの紹介</h3><p>　音声合成エンジンの作成にはディープラーニング、つまり機械学習を用いています。ここでは、今回用いた用いディープラーニング手法や、学習データの前処理を紹介します。ここはちょっと技術寄りの説明になります。</p><h4 id="end-to-end音声合成"><a href="#end-to-end音声合成" class="headerlink" title="end-to-end音声合成"></a>end-to-end音声合成</h4><p>　機械学習を用いて音声合成する手法は、音声波形を直接生成するのではなく、音響特徴量を生成したあと、ボコーダーと呼ばれる変換器を使って音声波形にする手法が主流でした。この方法は、声に関連する特徴量だけを生成すれば良いため、少ないデータ数で機械学習することができます。しかし、一度特徴量にすると、呼吸音などの情報が欠け落ちてしまいます<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。呼吸音は尊いので、なんとしてでも救いたいです。そのために、ラベルから音声波形を直接生成する手法（end-to-end音声合成）に挑戦してみました。</p><p>　音声波形は特徴量に比べて情報量が大きいため、学習するのがより難しくなります。東北きりたん歌唱データベースには約１時間という十分な量の歌唱データがあるため、なんとかなるだろうと思ってこちらの手法にしました。今回は、前々から追実装して手軽に使える環境があったので、WaveRNN（の改造版）を用いました。</p><h4 id="学習に関する詳細"><a href="#学習に関する詳細" class="headerlink" title="学習に関する詳細"></a>学習に関する詳細</h4><p>　学習に用いる音声データは、サンプリングレートを24000Hz、ビット深度を10bitにしました。音素ラベルや音階ラベルは<a href="https://github.com/Hiroshiba/kiritan_singing_label_reader" target="_blank" rel="noopener">自作のリーダー</a>を用いて読み込み、音階ラベルが欠けている箇所を省きました。音素ラベル・音階ラベルはそれぞれone-hotベクトルにし、それぞれのラベルの持続時間を加えたものを入力としました。歌声の周波数は<a href="https://github.com/mmorise/World" target="_blank" rel="noopener">WORLD</a>を用いて推定し、その対数を入力としました。ネットワークは、WaveRNNのdual-softmaxを失くし、ラベルデータを双方向GRUを2層通過させたデータを局所ベクトルとして用いました。バッチサイズを32にし、0.07秒の音声波形を1データとしました。このとき、ラベルデータを前後1秒ずつほど長くサンプルして双方向GRUに通し、音声波形に合わせてトリミングして学習すると、性能が大きく改善しました。20万イテレーション学習したモデルを用いて音声合成しました。学習完了までに、TITAN Xで3日ほどかかりました。</p><h3 id="終わりに"><a href="#終わりに" class="headerlink" title="終わりに"></a>終わりに</h3><p>　今回は実験報告をゴールにしていたので適当にタスクを設計しました。作った歌声音声合成エンジンを誰でも使えるようにしようと考えてみたところ、何を入力にし、何を推定し、何を修正可能にするのか過不足なく設定するのって難しそうだなぁと思いました。例えば先ほど、音量を調整可能にするために音量ラベルを入力する実験を紹介しましたが、音量程度であれば推定後のポストプロセスで正規化しちゃうことも可能だな～と、さっき気づきました。発見が多くて楽しいです。</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">2020/04/28追記：ボコーダーは呼吸音も合成できると<a href="https://twitter.com/ryuuma1826/status/1254998647416053761" target="_blank" rel="noopener">ご指摘いただきました</a>、ありがとうございます。</span><a href="#fnref:1" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      最近、理想の人工知能（歌ったり踊ったり喋ったりできるキャラクター）を作りたいと思うようになりました。人工知能が歌を歌うためには、歌声音声合成エンジンが必要です。ということで、ディープラーニングを使って、歌声音声合成エンジンの作成に挑戦してみました。この記事では、実際に音声合成した歌声や、その仕組み、別条件での実験結果、ディープラーニング周りの手法を紹介します。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>リアルタイム声質変換ライブラリ「Realtime Yukarin」を公開しました</title>
    <link href="http://blog.hiroshiba.jp/realtime-yukarin-introduction/"/>
    <id>http://blog.hiroshiba.jp/realtime-yukarin-introduction/</id>
    <published>2019-09-27T12:08:59.000Z</published>
    <updated>2020-07-13T16:26:43.662Z</updated>
    
    <content type="html"><![CDATA[<h3 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h3><p>リアルタイム声質変換アプリケーション、<a href="https://github.com/Hiroshiba/realtime-yukarin" target="_blank" rel="noopener">Realtime Yukarin</a>を開発し、OSS（オープンソースソフトウェア）として公開しました。ここで言う声質変換とは、「誰でも好きな声になれる」技術のことを指します。好きな声になれる声質変換は夢があって流行りそうなのですが、まだ全然普及していないと思います。それは現時点で、声質変換を実際にリアルタイムで使えるフリーな仕組みが無いためだと考えました。そこで、自由に使えるリアルタイム声質変換アプリケーションを作り、ソースコードと合わせて公開しました。</p><a id="more"></a><h3 id="声質変換とは"><a href="#声質変換とは" class="headerlink" title="声質変換とは"></a>声質変換とは</h3><p>声を変える方法で有名なのは、声の高さや音色を変える手法、いわゆるボイスチェンジャーです。既存のボイスチェンジャーは、元の声を起点として、変換パラメータを自分で調整する必要があります。一方ここでの声質変換は、元の声と好きな声を用いて機械学習し、変換パラメータを自動で調整します。</p><figure>  <figcaption>既存のボイスチェンジャーと声質変換の違い</figcaption>  <img src="7.svg" style="max-height: 14em"></figure><p>また、声質変換は、ボイスチェンジャーで変換できないパラメータも変換することができます。音声は、声の高さ・声の音色・イントネーション・音の強弱などに分解できますが、ボイスチェンジャーが変換するのは声の高さと音色だけです。声質変換は、イントネーションや音の強弱も変換することができます。</p><h3 id="Realtime-Yukarin"><a href="#Realtime-Yukarin" class="headerlink" title="Realtime Yukarin"></a>Realtime Yukarin</h3><p>Realtime Yukarinとは、リアルタイムで声質変換ができる、オープンソースのアプリケーションです。このアプリケーションは、「誰でも好きな声になれる声質変換システム：Yukarinライブラリ」の１つです。「Yukarin」及び「Become Yukarin」と連携することで、リアルタイム声質変換を実現します。</p><h4 id="どうやって使うか"><a href="#どうやって使うか" class="headerlink" title="どうやって使うか"></a>どうやって使うか</h4><p>声質変換を担当する<a href="https://github.com/Hiroshiba/yukarin" target="_blank" rel="noopener">Yukarin</a>モデルと、変換結果を高品質化する<a href="https://github.com/Hiroshiba/become-yukarin" target="_blank" rel="noopener">Become Yukarin</a>モデルの、２つの機械学習済みモデルを用意して、コマンドを実行すれば使えます。それぞれのリンク先のドキュメントに従って、誰でも機械学習済みモデルを作成することができます。以下の解説動画にて、より詳細な手順を紹介しています。</p><ul><li><a href="https://www.nicovideo.jp/watch/sm35735482" target="_blank" rel="noopener">ディープラーニングで好きな声になれるツール作ってみた</a></li></ul><h4 id="何ができるか"><a href="#何ができるか" class="headerlink" title="何ができるか"></a>何ができるか</h4><p>声質変換の機械学習タスクで作成したモデルと、GPU搭載パソコンを用いて、コマンド１つでリアルタイムな声質変換ができます。また、OSSなので、これをベースにコードを改良したり、商用非商用問わずアプリケーションに組み込んだりすることができます。ソースコードはGitHubで公開しています。</p><ul><li><a href="https://github.com/Hiroshiba/realtime-yukarin" target="_blank" rel="noopener">Realtime Yukarinのソースコード</a></li></ul><h3 id="特徴"><a href="#特徴" class="headerlink" title="特徴"></a>特徴</h3><h4 id="変換処理を３段階に分け、それぞれを別プロセスで動かす"><a href="#変換処理を３段階に分け、それぞれを別プロセスで動かす" class="headerlink" title="変換処理を３段階に分け、それぞれを別プロセスで動かす"></a>変換処理を３段階に分け、それぞれを別プロセスで動かす</h4><p>Realtime Yukarinでは、音声入力デバイスから取り込んだ音声を細切れ（セグメント化と呼ぶことにします）にしたあと、セグメントごとに変換し、繋ぎ合わせてから音声に戻すことで、音声を変換しています。</p><figure>  <img src="1.svg" style="max-height: 7.5em"></figure><p>変換処理を１プロセスで行う場合、セグメントの長さよりも変換に時間がかかると、変換結果の音声が途切れてしまいます。</p><figure>  <img src="2.svg" style="max-height: 10.5em"></figure><p>Realtime Yukarinでは、変換処理をエンコード、コンバート、デコードの３段階に分け、それぞれ別プロセスで処理するように工夫しており、スムーズに変換処理を行います。</p><figure>  <img src="3.svg" style="max-height: 14.5em"></figure><h4 id="セグメントをオーバーラップさせて変換精度を一定にする"><a href="#セグメントをオーバーラップさせて変換精度を一定にする" class="headerlink" title="セグメントをオーバーラップさせて変換精度を一定にする"></a>セグメントをオーバーラップさせて変換精度を一定にする</h4><p>Realtime Yukarinは、声質変換の際に前後の入力音声も用いることで、変換精度を向上させています。しかし、セグメント端は音声が欠けているため、その区間だけ変換精度が下がってしまいます。</p><figure>  <img src="4.svg" style="max-height: 9.5em"></figure><p>これを避けるために、Realtime Yukarinでは、セグメントをオーバーラップできる機能を用意し、変換精度を一定に保ちます。</p><figure>  <img src="5.svg" style="max-height: 9.5em"></figure><h4 id="ノイズを除去する"><a href="#ノイズを除去する" class="headerlink" title="ノイズを除去する"></a>ノイズを除去する</h4><p>全ての音を声に変換する声質変換では、小さなノイズを拾って声を生成しようとし、大きなノイズが出力されてしまうことがあります。Realtime Yukarinでは、音量ベースの簡単なノイズ除去機能を用意し、耳障りなノイズを減らす工夫をしています。</p><h3 id="デモ"><a href="#デモ" class="headerlink" title="デモ"></a>デモ</h3><p>Intel Core i7-7700 CPU @ 3.60GHzと、GeForce GTX 1060 6GBを搭載したWindowsパソコンを使って、遅延約2秒の声質変換が可能です。</p><p>使い方の解説動画内にデモがあります。ぜひご覧ください。</p><ul><li><a href="https://www.nicovideo.jp/watch/sm35735482" target="_blank" rel="noopener">解説動画</a></li></ul><p>また、今年の５月に、Realtime Yukarinを用いて、声質変換した声でお客さんと会話するイベントを行いました。Realtime Yukarinは、実際の現場でもちゃんと使えると思います。</p><ul><li><a href="https://blog.hiroshiba.jp/backstage-of-talking-event-with-yuzuki-yukari/">声質変換を用いたイベント例</a></li></ul><h3 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h3><p>最初に声質変換を使ってみてから１年間かけ、自分自身で検証を重ねてクオリティと実用性を上げました。そしてやっと、誰でもリアルタイム声質変換を使える仕組み、Realtime Yukarinを公開することができました。これで、ちょっとでも声質変換が普及したら嬉しいです。ぜひお試しください。</p>]]></content>
    
    <summary type="html">
    
      リアルタイム声質変換アプリケーション、Realtime Yukarinを開発し、OSS（オープンソースソフトウェア）として公開しました。ここで言う声質変換とは、「誰でも好きな声になれる」技術のことを指します。好きな声になれる声質変換は夢があって流行りそうなのですが、まだ全然普及していないと思います。それは現時点で、声質変換を実際にリアルタイムで使えるフリーな仕組みが無いためだと考えました。そこで、自由に使えるリアルタイム声質変換アプリケーションを作り、ソースコードと合わせて公開しました。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>「結月ゆかり」とお喋りできるイベントの舞台裏</title>
    <link href="http://blog.hiroshiba.jp/backstage-of-talking-event-with-yuzuki-yukari/"/>
    <id>http://blog.hiroshiba.jp/backstage-of-talking-event-with-yuzuki-yukari/</id>
    <published>2019-05-26T05:59:08.000Z</published>
    <updated>2019-05-26T06:00:48.821Z</updated>
    
    <content type="html"><![CDATA[<p>２０１９年５月頭に開催された、<a href="http://yukari.blueskywings.net/" target="_blank" rel="noopener">結月ゆかり・紲星あかり中心VOICEROIDオンリー同人イベント「この声届け、月までも五」（声月）</a>の一画で、企画展示をする機会を頂きました。この展示ブースにて、既存のＶＲ技術と、以前開発した声質変換技術を組み合わせ、イベント会場に来場されたお客さんが「結月ゆかり」とお喋りできる催しを行いました。その経験をもとに得られた知見や課題、感想をまとめたいと思います。</p><a id="more"></a><figure width="50%">  <figcaption>企画タイトルは「ゆかりが声月にやってきた」</figcaption>  <img src="fig1b-twitter.jpg" style="max-height: 15em"></figure><h3 id="企画内容"><a href="#企画内容" class="headerlink" title="企画内容"></a>企画内容</h3><p>この企画の趣旨を一言で言うと、「イベント会場に来ると結月ゆかりと会話できる」というものです。</p><figure>  <figcaption>ＶＲ世界に来たお客さん（左）と会話する「結月ゆかり」（右）</figcaption>  <img src="fig2b-vc.jpg" style="max-height: 15em"></figure><p>「結月ゆかり」は音声合成ソフトウェアVOICEROIDのキャラクターで、今回のVOICEROIDオンリーイベント「声月」にはファンの方が大勢集まります。僕自身も「結月ゆかり」が大好きで、話せたらきっと楽しいと思い、企画しました。展示では、「結月ゆかり」とコミュニケーションが取れる体験の演出に注力しました。</p><p>来場したお客さんは、</p><ul><li>会場のプロジェクター映像とマイクを使って「結月ゆかり」と会話したり</li><li>会場に設置されたＶＲ機材を使って「結月ゆかり」と会ってお喋りしたり</li><li>「結月ゆかり」がゲームしている姿をプロジェクター映像越しに眺めたり</li></ul><p>できるようにしました。</p><figure>  <figcaption>会場のプロジェクターに映る「結月ゆかり」</figcaption>  <img src="fig2a-satoru.jpg" style="max-height: 15em"></figure><p>このような企画を実現するために、多くの技術的なハードルがありました。以降では、企画の舞台裏を振り返りたいと思います。</p><h3 id="仕組み"><a href="#仕組み" class="headerlink" title="仕組み"></a>仕組み</h3><h4 id="「結月ゆかり」は別室で人（アクター）が演じた"><a href="#「結月ゆかり」は別室で人（アクター）が演じた" class="headerlink" title="「結月ゆかり」は別室で人（アクター）が演じた"></a>「結月ゆかり」は別室で人（アクター）が演じた</h4><p>人と違和感なく会話できるのは、いまのところ人だけなので、人（アクター）が結月ゆかりを演じることにしました。そのためには、アクターの声と姿を「結月ゆかり」のものにする必要があります。アクターは、ＶＲ技術を使って「結月ゆかり」の体を動かしつつ、声質変換技術を使ってアクター自身の声を「結月ゆかり」の声に変換しました<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。ここでは、<a href="../became-yuduki-yukari-with-deep-learning-power/">過去に開発したディープラーニング声質変換</a>を用いました。</p><figure>  <figcaption>別室と会場の関係図</figcaption>  <img src="fig3-ponchi.png" style="max-height: 7.5em"></figure><p>動き回れるくらいの空間が必要なのと、声の変換のためにノイズが少ない環境が必要だったため、会場から離れた別室を用意してもらいました。</p><h4 id="姿・声の変換のためにGPUパソコンを２つ用意した"><a href="#姿・声の変換のためにGPUパソコンを２つ用意した" class="headerlink" title="姿・声の変換のためにGPUパソコンを２つ用意した"></a>姿・声の変換のためにGPUパソコンを２つ用意した</h4><p>今回の企画では、姿を変えるためにバーチャルキャストを、声を変えるためにディープラーニング声質変換を使いました。<a href="https://virtualcast.jp/" target="_blank" rel="noopener">バーチャルキャスト</a>は、バーチャルキャラクターになってリアルタイムでコミュニケーションできるVRライブ・コミュニケーションサービスです。バーチャルキャストとディープラーニング声質変換はどちらもGPUを使ってしまうという独特な問題があります。同じGPUを使っていると、バーチャルキャストの描画頻度が低下してアクターが酔いやすくなったり、声質変換が遅延して会話しづらくなったりします。この問題を解決するために、GPUパソコンを２台用意しました。片方のパソコンでバーチャルキャストしつつ、もう片方で声を変換しました。</p><figure>  <figcaption>パソコンを２つ用意してソフトウェアを分離する</figcaption>  <img src="fig4-ponchi.png" style="max-height: 11.5em"></figure><h4 id="リアルタイムで字幕を載せた"><a href="#リアルタイムで字幕を載せた" class="headerlink" title="リアルタイムで字幕を載せた"></a>リアルタイムで字幕を載せた</h4><p>会場にきたお客さんと「結月ゆかり」がお喋りするときに、会場が賑やかだったり、うまく声が変換できなかったりで、「結月ゆかり」の発言が伝わりにくいかもしれません。そこで、音声認識サービスを用いて、リアルタイムで字幕をつけることにしました。この字幕表示も一工夫しました。あまり意識されませんが、テレビなどのセリフ字幕は、発言とほぼ同じタイミングで表示されます。いろいろ試した結果、音声認識を用いた字幕表示も、この方式に沿ったほうが見やすくなることがわかりました。しかし、音声認識を使うと、字幕テキストを得られるのが発言よりも必ず遅れます。そこで、会場への配信映像を合成するときに、映像と音声を遅らせることで、字幕表示の違和感を減らしました<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>。</p><figure>  <figcaption>音声を遅らせて字幕のタイミングと合わせる</figcaption>  <img src="fig5-ponchi.png" style="max-height: 12em"></figure><p>これらの仕組みを実践していく中で、課題がいくつか見えてきました。</p><h3 id="課題"><a href="#課題" class="headerlink" title="課題"></a>課題</h3><h4 id="会場の人と会話しにくかった"><a href="#会場の人と会話しにくかった" class="headerlink" title="会場の人と会話しにくかった"></a>会場の人と会話しにくかった</h4><p>予想以上に会場が賑やかだったたため、ＶＲ空間に来たお客さんは「結月ゆかり」の声を聞き取りづらいようでした。お客さんと会話していて気づきましたが、字幕なしであっても意外と意思疎通ができ、変換結果の声でも会話内容はそれなりに伝わることがわかりました。なので、ＶＲ用にヘッドホン等を用意して、ちゃんと声が聞こえるようにすると良さそうでした（今回はスピーカーのみを用意していました）。</p><p>もう１つ、遅延による会話への影響が思っていたより大きいようでした。今回用いたディープラーニング声質変換は、２秒ほど遅延してしまいます。このことをお客さんに伝えていなかったため、意思疎通できているのか不安に感じる方が多くいました。遅延することを伝えつつ、それに対するストーリー（宇宙から交信してるとか！）も作ればよかったかもしれません。</p><h4 id="相手の表情を見れなかった"><a href="#相手の表情を見れなかった" class="headerlink" title="相手の表情を見れなかった"></a>相手の表情を見れなかった</h4><p>ＶＲ機材を装着しなくとも、プロジェクターの前に立てば「結月ゆかり」とお喋りできるようにしていました。このとき、会場にカメラを設置して、お客さんの表情をアクターが見られるようにするつもりでしたが、何らかの原因で正常な映像をアクター側に送ることができませんでした。会場パソコン側のネットワーク帯域か、計算リソースが不足していたのだと思います。会場パソコンはバーチャルキャストが起動しているので、映像配信用のデバイスをもう１つ用意すると良かったかもしれません。</p><h4 id="アクター役をやりながらオペレーションすることはできない"><a href="#アクター役をやりながらオペレーションすることはできない" class="headerlink" title="アクター役をやりながらオペレーションすることはできない"></a>アクター役をやりながらオペレーションすることはできない</h4><p>実は今回、僕が「結月ゆかり」のアクター役をしました。一方で、僕は企画のオペレータでもありました。僕がいる別室側でトラブルが起こった場合は、声でスタッフとやりとりすることができますが、会場側にトラブルが起こったときにオペレーションできません。特に、アクターが会場スタッフからの連絡を受けとるのは難しいと思います。今回はスタッフが優秀だったため大きなトラブルは起こりませんでしたが、さすがにオペレーターとアクターは分けるべきだということを学びました。</p><figure>  <figcaption>別室にはＶＲ機材と声質変換・ＶＲ用のパソコン２台を設置した</figcaption>  <img src="fig7-besshitsu.jpg" style="max-height: 15em"></figure><h3 id="結び（ポエム）"><a href="#結び（ポエム）" class="headerlink" title="結び（ポエム）"></a>結び（ポエム）</h3><p>声質変換をイベントに活用するのは、僕がやりたいことの１つでした。声月関係者の方々には貴重な機会をいただき、とても感謝しています。今回使ったリアルタイム声質変換のコードは全て公開するつもりです。いろんな方に使ってもらって、いろんなアイデアを見てみたいです。</p><p>僕はこれからも、チャンスがあればイベントやサービス開発に挑戦してみたいです。もし何か面白いことがあったら、ぜひお声掛けください！</p><figure>  <figcaption>休憩中用の画像（thanks 告白P）</figcaption>  <img src="fig6-kokuhaku.png" style="max-height: 15em"></figure><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">発話内容を音声認識でテキストにした後、VOICEROIDソフトでテキスト音声合成するという方法もあります。それに比べて声質変換を使う方法は、「遅延が短い」「笑い声などの非テキスト音声も変換可能」といった特性があります。今回の企画では、ゆかりさんとお喋りする臨場感を体験できることを目指したので、声質変換を用いました。より詳しい仕組みは<a href="../became-yuduki-yukari-with-deep-learning-power/">こちらの記事</a>で紹介しています。</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">本当は、ＶＲ空間で「結月ゆかり」と会ってお喋りしてる人にも違和感のない字幕を表示したかったのですが、バーチャルキャストのモーションを遅延させる方法が思いつかず、断念しました。</span><a href="#fnref:2" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      ２０１９年５月頭に開催された、結月ゆかり・紲星あかり中心VOICEROIDオンリー同人イベント「この声届け、月までも五」（声月）の一画で、企画展示をする機会を頂きました。この展示ブースにて、既存のＶＲ技術と、以前開発した声質変換技術を組み合わせ、イベント会場に来場されたお客さんが「結月ゆかり」とお喋りできる催しを行いました。その経験をもとに得られた知見や課題、感想をまとめたいと思います。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ディープラーニングの力で人工知能になって結月ゆかりと会話してみた</title>
    <link href="http://blog.hiroshiba.jp/talk-to-yuduki-yukari-with-deep-learning-power/"/>
    <id>http://blog.hiroshiba.jp/talk-to-yuduki-yukari-with-deep-learning-power/</id>
    <published>2019-03-02T16:20:45.000Z</published>
    <updated>2019-09-27T11:37:21.545Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目次"><a href="#目次" class="headerlink" title="目次"></a>目次</h3><ul><li>（背景）結月ゆかりと会話したいが、結月ゆかりの人格は世界に存在しない。</li><li>（手法）自分が人工知能になり、余った自分の魂を結月ゆかりに宿らせて、自分と結月ゆかりが会話する手法を提案する。</li><li>（結果）結月ゆかりと会話することができた。</li><li>（展望）次は結月ゆかりの人工知能を作りたい。</li></ul><a id="more"></a><script type="application/javascript" src="https://embed.nicovideo.jp/watch/sm34712379/script?w=640&h=360"></script><noscript><a href="https://www.nicovideo.jp/watch/sm34712379" target="_blank" rel="noopener">ディープラーニングの力で人工知能になって結月ゆかりと会話してみた</a></noscript><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>多くの人が結月ゆかりと会話したいと思っている。結月ゆかりと会話するには、結月ゆかりの人工知能を作ることで解決できる。しかし、結月ゆかりの真の人格は世界に存在しない。そのため、結月ゆかりのデータを集めることができず、人工知能を作ることは難しいという問題があった。</p><h3 id="手法"><a href="#手法" class="headerlink" title="手法"></a>手法</h3><p>発想を転換し、結月ゆかりの人工知能を作るのではなく、自分の人工知能を作ることを考えた。そして自分が結月ゆかりになり、その人工知能と会話する。これで、自分と結月ゆかりが会話できる。</p><p>最初に自分の人工知能の作り方を説明し、続いて結月ゆかりになる手法を説明する。</p><h4 id="自分の人工知能を作る"><a href="#自分の人工知能を作る" class="headerlink" title="自分の人工知能を作る"></a>自分の人工知能を作る</h4><p>人工知能と会話するためには、人工知能用のテキスト音声合成・会話応答・音声認識が必要になる。それぞれの手法を説明する。（手法の詳細は<a href="https://qiita.com/Hiroshiba/items/b4daca0176af5fd352a2" target="_blank" rel="noopener">過去の解説記事</a>にある。）</p><p>テキスト音声合成には<a href="https://r9y9.github.io/nnmnkwii/latest/nnmnkwii_gallery/notebooks/tts/02-Bidirectional-LSTM%20based%20RNNs%20for%20speech%20synthesis%20%28en%29.html" target="_blank" rel="noopener">ディープラーニングを使った既存手法</a>を用いた。文章５００文を読み上げた自分の音声データを学習データセットとした。</p><p>会話応答は、「問いかけの文章を自分なりの言葉にして返すもの」とし、ディープラーニングを使ったオートエンコーダを用いた。doc2vecを使ってテキストから特徴量を抽出し、その特徴量から元のテキストを生成する学習を行った。<a href="https://friends.nico/@hiho_karuta" target="_blank" rel="noopener">SNS（マストドン）で発言したテキスト約１１万文</a>を学習データセットとした。</p><p>音声認識はchromeに備わっているWeb Speech APIを用いた。デモを行う際、自分の発言の字幕をリアルタイムで表示するためにこれを採用した。</p><p>学習のためのコードや、これらをWebAPI化するコード、人工知能と会話できるWebフロントアプリ、全てをオープンソースで公開した。</p><ul><li><a href="https://github.com/Hiroshiba/hihobot" target="_blank" rel="noopener">会話応答（hihobot）</a></li><li><a href="https://github.com/Hiroshiba/hihobot-synthesis" target="_blank" rel="noopener">音声合成（hihobot-synthesis）</a></li><li><a href="https://github.com/Hiroshiba/hihobot-tts" target="_blank" rel="noopener">APIサーバー（hihobot-tts）</a></li><li><a href="https://github.com/Hiroshiba/hihobot-front" target="_blank" rel="noopener">フロントエンド（hihobot-front）</a></li></ul><h4 id="結月ゆかりになる"><a href="#結月ゆかりになる" class="headerlink" title="結月ゆかりになる"></a>結月ゆかりになる</h4><p>結月ゆかりになるには、自分の声をリアルタイムで結月ゆかりの声に変換する必要がある。これには<a href="../became-yuduki-yukari-with-deep-learning-power/">過去に開発した声質変換手法</a>を用いた。</p><h3 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h3><p>自分の人工知能と会話した結果を紹介する。<a href="https://twitter.com/i/status/1068656559406821376" target="_blank" rel="noopener">実際に会話した動画</a></p><pre><code class="hljs plain">自分　　　：おはよう人工知能　：おはようございます自分　　　：今日もいい天気ですね　気分はいかがですか人工知能　：今日は休日ですか自分　　　：土曜日なんで休日ですね人工知能　：２月１２日なんですかね自分　　　：それは僕の誕生日ですね人工知能　：今日は僕の誕生日ですね自分　　　：今日は僕の誕生日ではないですね人工知能　：僕は今日ですね自分　　　：あそうか確かにあなたの誕生日は今日かもしれない</code></pre><p>この結果から、人工知能が、質問に対し自分なりの言葉にして返事できていることがわかる。</p><p>また、自分が結月ゆかりになり、自分の人工知能と会話することで、自分と結月ゆかりが会話することができた。<a href="https://www.nicovideo.jp/watch/sm34712379" target="_blank" rel="noopener">デモ動画</a></p><p>実際に行った素直な感想としては、特に自分と会話している気持ちにはならず、ただただポンコツな何かに対して壁打ちのように会話している気分になった。会話後に、録画した映像を見直すと、ところどころゆかりさんが可愛くて、自分の人工知能を煽っているシーンなどが羨ましく感じることもあった。</p><h3 id="考察・展望"><a href="#考察・展望" class="headerlink" title="考察・展望"></a>考察・展望</h3><h4 id="会話応答の品質が良くない点"><a href="#会話応答の品質が良くない点" class="headerlink" title="会話応答の品質が良くない点"></a>会話応答の品質が良くない点</h4><p>今回は、質問文に対し、自分なりの言葉でその文章を返す学習を行った。その結果、質問文をオウム返しする傾向にあり、会話が成り立たないケースが多くあった。データｔ番目を入力、データｔ＋１番目を出力するように学習すれば、ある程度文脈を理解した会話ができるかもしれない。</p><h4 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h4><p>次は自分が思う理想の結月ゆかり人工知能を作りたい。結月ゆかり人工知能を作るには次の２つの課題がある。</p><ol><li>結月ゆかり用のテキストデータが大量に存在しない点</li><li>そもそも結月ゆかりの人格が世界に存在しない点</li></ol><p>これらの課題を解決するために、様々な人の発言データを用いて人格空間を作成し、その中から自分が思う理想の結月ゆかりの人格を探せるような手法を考えている。時間があれば取り掛かりたい。</p>]]></content>
    
    <summary type="html">
    
      結月ゆかりと会話したいが、結月ゆかりの人格は世界に存在しない。自分が人工知能になり、余った自分の魂を結月ゆかりに宿らせて、自分と結月ゆかりが会話する手法を提案する。結月ゆかりと会話することができた。次は結月ゆかりの人工知能を作りたい。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Brainwave Idea Challenge（脳波アイデアソン）見てきた</title>
    <link href="http://blog.hiroshiba.jp/brainware_idea_challenge_2018/"/>
    <id>http://blog.hiroshiba.jp/brainware_idea_challenge_2018/</id>
    <published>2018-11-18T17:09:49.000Z</published>
    <updated>2019-02-22T17:10:09.648Z</updated>
    
    <content type="html"><![CDATA[<p>BMI（Brain Machine Interface）を作る、<a href="https://www.pgv.co.jp/" target="_blank" rel="noopener">PGV</a>というベンチャー企業がある。11月、<a href="https://pgv.connpass.com/event/102250/" target="_blank" rel="noopener">PGVの開催するアイデアソン</a>が開かれた。今回のお題は、高性能ウェアラブル脳波センサーの利用のアイデア出しだ。BMIに興味があったので参加したかったが、アイデアソン枠が埋まっていたので、ブログ枠の観覧者として申し込んだ。</p><a id="more"></a><h2 id="デバイス紹介"><a href="#デバイス紹介" class="headerlink" title="デバイス紹介"></a>デバイス紹介</h2><p>最初にPGVが作った脳波センサーが紹介された。冷えピタシートみたいにおでこに”貼る”だけで、前頭葉周りの脳波を、医療用途でも使えるような精度で測定できる。そもそも、脳波は神経の電気信号から発生しているもので、非常に微弱な上、頭蓋骨という厚い層越しに計測するため、額で得られる信号はとても弱い。それでもこのデバイスを使えば、安静状態によく発生するθ波などなどの低周波のものから、興奮状態によく発生する高周波のものまでちゃんと取れるらしい。ワイヤレス通信が可能（普通はノイズが乗るのでたぶんすごい）。デモとして、開発責任者の方の脳波を測定し、その波形を表示し続けるタブレットが会場の真ん中に展示されていた。本人と喋りながら波形を見ていると、確かに感情によって何かしら波形が変わっていた。</p><h2 id="アイデアソンの様子"><a href="#アイデアソンの様子" class="headerlink" title="アイデアソンの様子"></a>アイデアソンの様子</h2><p>集まった３０人がそれぞれアイデアを練って、お互いのアイデアに投票していく。２０代〜３０代が多そうだ。脳波を、他の用途で解析して、何かを得るアイデアが多かった。エンジニア視点だと、じゃあどうやって学習データ集めるんだとか、そんなことは可能なのかを考えてしまいがちなので、とても参考になる。脳波を解析して好みの音楽を流してくれるとか、脳を計算資源にして提供可能にするとか、一目惚れを計測するとか、赤ちゃんにデバイス付けて育児サポートするとかのアイデアがあった。僕だったら、Vtuber向けの表情コントローラーを作ってみたいなと思う。</p><p>チームに分かれて調査と資料作りが始まった。アイデアを出した人がリーダー的ポジションで進行していくことになる。リーダーが思考フレームワークを持っていると、資料を作成するスピードがとても早い様子が見てわかった。</p><h2 id="発表"><a href="#発表" class="headerlink" title="発表"></a>発表</h2><p>１時間半後に発表が始まった。１チーム４分の発表、１分の質疑だ。それぞれのアイデアの概要と、聞いたときの感想を書いていく。</p><h3 id="Brain-Wave-DJ"><a href="#Brain-Wave-DJ" class="headerlink" title="Brain Wave DJ"></a>Brain Wave DJ</h3><p>脳波から感情を取り、そこから音楽を選択する。更に、脳波を計測し続けて、テンポなどのパラメータをリアルタイムに調整する。悲しいときに更に悲しくさせる、楽しい気分になりたいときに曲を変調して気持ちよくさせる、とか。</p><ul><li>質問：苦しいときや怒ってるときはどうさせるべきか。</li><li>回答：ヒーリング系とか。フィードバックで更にローテンポにするとか。</li><li>感想：音楽使って感情をコントロールするのは、集中力上げるのとかにも使えて面白そう。でも、そもそも人は音楽を変調してほしいのかはちょっと疑問。</li></ul><h3 id="脳波マップサービスの提供"><a href="#脳波マップサービスの提供" class="headerlink" title="脳波マップサービスの提供"></a>脳波マップサービスの提供</h3><p>ドライバーが感じる怖い道とかのデータを得たい。ドライバーから脳波を測定してストレスなどを取得し、そのデータをAPI化して提供する。ナビゲーションのときに渋滞情報などが使われるが、加えて恐怖情報なども提供できれば面白いのでは。ヒヤリハットマップも作れるかも。</p><ul><li>質問：ドライバーみんな冷えピタ貼ることになるんですか</li><li>回答：ビジネスライフで使ってる人に提供していけばよいのではと</li><li>感想：脳波じゃなくて心拍数で良さそう。</li></ul><h3 id="PITA（ひとめぼれサービス）"><a href="#PITA（ひとめぼれサービス）" class="headerlink" title="PITA（ひとめぼれサービス）"></a>PITA（ひとめぼれサービス）</h3><p>一目惚れした瞬間にデバイスを震えさせる。草食系男子に、「お前はこの人しかいない」、ということを気づかせる。まずは市場がありそうな婚活会社に提供する。利用者は婚活後に、「あのとき言ったことがだめだった」みたいなセルフ分析できる。</p><ul><li>感想：結構良いのでは。自分の気持ちが全部相手伝わっているとすると、だいぶ疲れそうだ。もし実現したら、何が伝わって何が伝わらないのかちゃんと知りたいな。</li></ul><h3 id="脳-MORE-BABY"><a href="#脳-MORE-BABY" class="headerlink" title="脳 MORE BABY"></a>脳 MORE BABY</h3><p>子供の夜泣きで眠れないことが多々ある。それで夫婦仲が悪くなったりするので少しでも解決したい。子供に脳波デバイスを取り付ける。子供の眠気のリズムを測定したり、起きそうなのを検知して親のスマホに通知する。</p><ul><li>質問：他に何か赤ちゃん向けのウェアラブルデバイスとかってあるのか、またこのシステムとの差分は。</li><li>回答：音声からいろいろ検知するのを開発した。脳波は脳からしか得られないものがあるので差分はあると思う。</li><li>感想：んーでも結局は子供に夜泣きで起こされるわけか。これで夫婦仲が悪くならなくなるのかはちょっと疑問だ。</li></ul><h3 id="Brainシアター"><a href="#Brainシアター" class="headerlink" title="Brainシアター"></a>Brainシアター</h3><p>映画を見てもらう人の脳波を測定する。そのデータを使って作品を改善したり、マーケティングを考えたりできる。どうやって付けてもらうかが課題。芸能人等のインフルエンサーに付けてもらって、その人との比較をできるようにする。脳波を記録し、その映画のシーンに対するログを取っておいて、そのシーンに対する感情を見直せるようにしておく。</p><ul><li>質問：そもそも映画だけじゃなくて、ゲームとかでも良いのでは。</li><li>回答：そう思う。まあ、映画業界は博打要素が強いので、こういった需要があるのではないかと思っている。</li><li>感想：<a href="http://www.caltech.edu/news/neural-networks-model-audience-reactions-movies-79098" target="_blank" rel="noopener">表情をカメラで撮る方法</a>で良さそう。</li></ul><h3 id="Think-connect-運転保険"><a href="#Think-connect-運転保険" class="headerlink" title="Think connect -運転保険-"></a>Think connect -運転保険-</h3><p>運転事故が多い。脳波を測定して、運転時のヤバさをスコア化する。そのスコアを用いた保険を作る。</p><ul><li>質問：アラート音によっては更に眠くなりそう。どういうアラートが良いだろうか。</li><li>回答：閃光手榴弾並のパトライトで良いのでは。</li><li>質問：リモートワークしてる人にも使えるのでは。</li><li>回答：この仕組みは、利用者にメリットがあるので、普及させられると思っている。</li><li>感想：データがあった。プレゼンが上手い。リーダーがもともとこういうことを考えていて、今回脳波デバイスに沿わせたという感じらしい。</li></ul><h3 id="Brain-Chain"><a href="#Brain-Chain" class="headerlink" title="Brain Chain"></a>Brain Chain</h3><p>大きな課題などの意思決定の最適解を見つけたい。脳を計算資源とし、大勢から余っている脳を提供してもらい、課題を解いてもらう。</p><ul><li>感想：面白いけど実現はだいぶ難しいなー</li></ul><h2 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h2><p>２位が「運転保険」、１位が「脳 MORE BABY」。</p><h2 id="全体的な感想"><a href="#全体的な感想" class="headerlink" title="全体的な感想"></a>全体的な感想</h2><p>脳波デバイスという制約に縛られない自由なアイデアが多く、聞いていて楽しかった。個人的に一番好きなのはひとめぼれ検知デバイスだった。たぶん実現は可能だと思うし、需要もあると思う。ぜひサービス化してほしい。逆に、脳波デバイスじゃないと実現できないようなアイデアは少なかった。安価な代替デバイスがあると旨味が減るだろうから、ちゃんと脳波を活かせるアイデアを思いつきたい。</p><p>全然関係ないけど、会場の<a href="https://lodge.yahoo.co.jp/" target="_blank" rel="noopener">LODGE</a>にたどり着くのがなかなか難しかった。最寄り駅から２０分くらいさまよっていた。</p>]]></content>
    
    <summary type="html">
    
      BMI（Brain Machine Interface）を作る、PGVというベンチャー企業がある。11月、PGVの開催するアイデアソンが開かれた。今回のお題は、高性能ウェアラブル脳波センサーの利用のアイデア出しだ。BMIに興味があったので参加したかったが、アイデアソン枠が埋まっていたので、ブログ枠の観覧者として申し込んだ。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CREPE(A Convolutional REpresentation for Pitch Estimation)使ってみた</title>
    <link href="http://blog.hiroshiba.jp/using-crepe/"/>
    <id>http://blog.hiroshiba.jp/using-crepe/</id>
    <published>2018-05-03T08:34:05.000Z</published>
    <updated>2019-05-25T16:31:10.008Z</updated>
    
    <content type="html"><![CDATA[<p>畳み込みニューラルネットを使ったピッチ推定手法、CREPEが提案された<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。<a href="https://pypi.org/project/crepe/" target="_blank" rel="noopener">PyPI</a>が用意されていて、発話音声にも簡単に適用できそうだったので試してみた。</p><a id="more"></a><p>使用方法はとても簡単で、以下のコマンドを叩けば良い。</p><pre><code class="hljs bash">pip install crepe</code></pre><p>GPUを使ったほうが圧倒的に早い。CREPEはKeras製なので、tensorflow-gpuを導入すればGPU使用可能になる。</p><pre><code class="hljs bash">pip install tensorflow-gpu</code></pre><p><a href="https://github.com/marl/crepe" target="_blank" rel="noopener">GitHubでの説明</a>によると、コマンドとして使うことが想定されている。コードはちゃんと機能ごとに切り分けられているので、適切にimportすればPython内で使うこともできる。</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> librosa<span class="hljs-keyword">from</span> crepe.core <span class="hljs-keyword">import</span> build_and_load_model <span class="hljs-keyword">as</span> crepe_build<span class="hljs-keyword">from</span> crepe.core <span class="hljs-keyword">import</span> predict <span class="hljs-keyword">as</span> crepe_predictfs = <span class="hljs-number">16000</span>x, _ = librosa.load(<span class="hljs-string">'hoge.wav'</span>, sr=fs)crepe_build()t, f0, confidence, _ = crepe_predict(x, sr=fs, viterbi=<span class="hljs-literal">True</span>)threshold = <span class="hljs-number">0.1</span>f0[confidence &lt; threshold] = <span class="hljs-number">0</span></code></pre><p>CREPEは、ピッチの確率分布のようなものを推定したあと、その分布を元にピッチを定める。<code>viterbi=False</code>（初期値）の場合は確率が最大のピッチを、<code>viterbi=True</code>の場合はHMMを使ってよしなにピッチを推定するっぽい（論文読んでいない）。<code>confidence</code>の値は音声活動の信頼度(0~1)らしい。</p><p>WORLDのHarvestを用いて基本周波数を推定したものと、CREPEを比較してみた。</p><figure>  <figcaption>F0比較</figcaption>  <img src="/using-crepe/f0.png" class=""></figure><p><code>viterbi=False</code>だと値が吹っ飛ぶことが多いようだった。</p><p>続いて、推定したf0を元に、WORLDを用いて分析合成してみた。</p><figure>  <figcaption>元音声</figcaption>  <audio src="1_raw.wav" controls></audio></figure><figure>  <figcaption>WORLD</figcaption>  <audio src="2_world.wav" controls></audio></figure><figure>  <figcaption>CREPE w/o viterbi</figcaption>  <audio src="3_crepe.wav" controls></audio></figure><figure>  <figcaption>CREPE w/ viterbi</figcaption>  <audio src="4_crepe_viterbi.wav" controls></audio></figure><p>WORLDが一番良いように聞こえる。CREPEはフレーム時間が0.01秒固定で、WORLDは0.005秒なので、その差が出ているのかもしれない。</p><p>上の音声（約7秒）の推定時間は、Harvestが2.0秒、viterbiなしCREPEが1.0秒、viterbiありが1.3秒程度だった。GPU無しCREPEだと10秒程度かかった。</p><p>CREPEはWORLDほどバッファが必要ない（たぶん）ので、リアルタイム性が必要なサービスに使いやすそうだ。しかし、HMMを利用する場合は、前の音声のコンテキストが必要なはずだが、そこは実装されていないので、自分でどうにかしないといけない。</p><p>作図に用いた<a href="https://gist.github.com/Hiroshiba/ad19bef0200c5f312cb7cbd5678b185c#file-use_crepe-ipynb" target="_blank" rel="noopener">Jupyter notebookファイル</a>をGistにアップロードした。</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;"><a href="https://arxiv.org/abs/1802.06182" target="_blank" rel="noopener">CREPE: A Convolutional Representation for Pitch Estimation</a></span><a href="#fnref:1" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      畳み込みニューラルネットを使ったピッチ推定手法、CREPEが提案された。PyPIが用意されていて、発話音声にも簡単に適用できそうだったので試してみた。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CycleGANノンパラレル結月ゆかり声質変換やってみた</title>
    <link href="http://blog.hiroshiba.jp/became-yuduki-yukari-with-cycle-gan-power/"/>
    <id>http://blog.hiroshiba.jp/became-yuduki-yukari-with-cycle-gan-power/</id>
    <published>2018-04-21T23:21:21.000Z</published>
    <updated>2019-02-22T17:10:09.657Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目次"><a href="#目次" class="headerlink" title="目次"></a>目次</h3><ul><li>（背景）自分の声を結月ゆかりにしたい。前回はパラレルデータのアライメントが問題になったので、ノンパラレルデータの手法を試したい。</li><li>（手法）CycleGANを使ったノンパラレル声質変換を試みた。</li><li>（結果）アライメントしなくても聞き取れる音声が生成できた。しかし、言語性を保ちつつ声質変換できるパラメータは見つけられなかった。</li><li>（考察）CycleGANを用いて性能の良い声質変換を得るのは難しいと思った。Identity以外のお手頃な制約手法が見つかれば，また挑戦してみたい。</li></ul><p>この記事は、技術系同人誌<a href="http://signico.hi-king.me/" target="_blank" rel="noopener">SIGNICO vol.5</a>の掲載記事「CycleGANを用いたリアルタイム結月ゆかり声質変換」の結果音声を中心に載せています。詳しい手法や解説などは同人誌の記事をご参照ください。</p><a id="more"></a><h3 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h3><h4 id="フィルタ幅を変えた時の結果"><a href="#フィルタ幅を変えた時の結果" class="headerlink" title="フィルタ幅を変えた時の結果"></a>フィルタ幅を変えた時の結果</h4><figure>  <figcaption>「あらゆる現実を、全て、自分の方へ捻じ曲げたのだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="A01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-A01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「予防や、健康管理、リハビリテーションのための制度を、充実していく必要があろう。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="B01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-B01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「六百人のお客さんの人いきれに、むし暑くて、扇子を使わずにいられない。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="C01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-C01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「十分間の休憩を与えられ、乱れた髪を結い直し、肩の汗をぬぐって、支度部屋で呼吸を整える。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="D01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-D01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「家に来た年賀状は、三百枚ほどで、丁度、出した分と同じぐらいだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="E01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅5ms</th>      <td><audio src="result-1-1-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅15ms</th>      <td><audio src="result-1-2-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>フィルタ幅1.28s</th>      <td><audio src="result-1-3-E01.mp3" controls></audio></td>    </tr>  </table></figure><h4 id="損失の比率を変えたときの結果"><a href="#損失の比率を変えたときの結果" class="headerlink" title="損失の比率を変えたときの結果"></a>損失の比率を変えたときの結果</h4><figure>  <figcaption>「あらゆる現実を、全て、自分の方へ捻じ曲げたのだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-A01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「予防や、健康管理、リハビリテーションのための制度を、充実していく必要があろう。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-B01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「六百人のお客さんの人いきれに、むし暑くて、扇子を使わずにいられない。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-C01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「十分間の休憩を与えられ、乱れた髪を結い直し、肩の汗をぬぐって、支度部屋で呼吸を整える。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-D01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「家に来た年賀状は、三百枚ほどで、丁度、出した分と同じぐらいだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-2-1-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:10</th>      <td><audio src="result-2-2-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:100</th>      <td><audio src="result-2-3-E01.mp3" controls></audio></td>    </tr>  </table></figure><h4 id="Identity制約を追加した結果"><a href="#Identity制約を追加した結果" class="headerlink" title="Identity制約を追加した結果"></a>Identity制約を追加した結果</h4><figure>  <figcaption>「あらゆる現実を、全て、自分の方へ捻じ曲げたのだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-A01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-A01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「予防や、健康管理、リハビリテーションのための制度を、充実していく必要があろう。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-B01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-B01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「六百人のお客さんの人いきれに、むし暑くて、扇子を使わずにいられない。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-C01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-C01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「十分間の休憩を与えられ、乱れた髪を結い直し、肩の汗をぬぐって、支度部屋で呼吸を整える。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-D01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-D01.mp3" controls></audio></td>    </tr>  </table></figure><figure>  <figcaption>「家に来た年賀状は、三百枚ほどで、丁度、出した分と同じぐらいだ。」</figcaption>  <table>    <tr>      <th>入力</th>      <td><audio src="hiho-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>目標</th>      <td><audio src="E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:1</th>      <td><audio src="result-3-1-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.1</th>      <td><audio src="result-3-2-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.02</th>      <td><audio src="result-3-3-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.01</th>      <td><audio src="result-3-4-E01.mp3" controls></audio></td>    </tr>    <tr>      <th>1:0.002</th>      <td><audio src="result-3-5-E01.mp3" controls></audio></td>    </tr>  </table></figure>]]></content>
    
    <summary type="html">
    
      自分の声を結月ゆかりにしたい。前回はパラレルデータのアライメントが問題になったので、ノンパラレルデータの手法を試したい。CycleGANを使ったノンパラレル声質変換を試みた。アライメントしなくても聞き取れる音声が生成できた。しかし、言語性を保ちつつ声質変換できるパラメータは見つけられなかった。CycleGANを用いて性能の良い声質変換を得るのは難しいと思った。Identity以外のお手頃な制約手法が見つかれば，また挑戦してみたい。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ディープラーニングの力で結月ゆかりの声になってみた</title>
    <link href="http://blog.hiroshiba.jp/became-yuduki-yukari-with-deep-learning-power/"/>
    <id>http://blog.hiroshiba.jp/became-yuduki-yukari-with-deep-learning-power/</id>
    <published>2018-02-13T02:08:25.000Z</published>
    <updated>2019-02-22T17:10:09.744Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目次"><a href="#目次" class="headerlink" title="目次"></a>目次</h3><ul><li>（背景）自分の声を結月ゆかりにしたい。前回はあまりクオリティが良くなかったので、手法を変えて質を上げたい。</li><li>（手法）声質変換を、低音質変換と高音質化の二段階に分けてそれぞれ学習させた。画像分野で有名なモデルを使った。</li><li>（結果）性能が飛躍的に向上し、かなり聞き取れるものになった。</li><li>（考察）精度はまだ改善の余地があり、多対多声質変換にすることで精度が向上すると考えられる。今回の結果を論文化したい。</li></ul><a id="more"></a><h3 id="デモ動画"><a href="#デモ動画" class="headerlink" title="デモ動画"></a>デモ動画</h3><script type="application/javascript" src="https://embed.nicovideo.jp/watch/sm32724409/script?w=640&h=360"></script><noscript><a href="http://www.nicovideo.jp/watch/sm32724409" target="_blank" rel="noopener">ディープラーニングの力で結月ゆかりの声になってみた</a></noscript><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>多くの人が可愛い女の子になりたいと思っている。ＣＧ技術やモーションキャプチャ技術の向上により、姿は女の子に仮想化できるようになってきた。しかし、声に関してはまだまだ課題が多い。声質変換は「遅延」「音質」「複数話者」などの難しい課題がある。今回は、自分の声を結月ゆかりにするための、低遅延で実現可能な高音質声質変換を目指した。</p><h3 id="手法"><a href="#手法" class="headerlink" title="手法"></a>手法</h3><p>大きく分けて３つの工夫をした。</p><ol><li>画像ディープラーニング分野で性能の良かったモデルを使用した</li><li>声質変換を「低音質声質変換」部分と「高音質化」部分に分けた</li><li>音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った</li></ol><h4 id="画像ディープラーニング分野で性能の良かったモデルを使用した"><a href="#画像ディープラーニング分野で性能の良かったモデルを使用した" class="headerlink" title="画像ディープラーニング分野で性能の良かったモデルを使用した"></a>画像ディープラーニング分野で性能の良かったモデルを使用した</h4><p>画像ディープラーニング分野は音声より数年早く進んでおり、解きたいタスクに対して使用すれば良いモデルが知られている。今回のように、対応関係のあるもの（音声分野で言うところのパラレルデータ）の変換を、少ないデータ数で学習させるには、<a href="https://phillipi.github.io/pix2pix/" target="_blank" rel="noopener">pix2pixモデル</a>が適している。今回はこのpix2pixモデルを使って声質変換タスクを解いた。</p><h4 id="声質変換を「低音質声質変換」部分と「高音質化」部分に分けた"><a href="#声質変換を「低音質声質変換」部分と「高音質化」部分に分けた" class="headerlink" title="声質変換を「低音質声質変換」部分と「高音質化」部分に分けた"></a>声質変換を「低音質声質変換」部分と「高音質化」部分に分けた</h4><p>声質変換には、複数話者によるペア音声を用意する必要があるが、結月ゆかりは大量の音声データを簡単に得られるのに対し、自分の声を大量に用意するのは容易ではない。そこで、少ないペア音声を使って低音質な声質変換するタスクと、大量の音声を使って高音質化するタスクを分けた。低音質声質変換では、WORLDを用いて自分の音響特徴量から結月ゆかりの音響特徴量を推定する学習を行った。高音質化では、低解像度なスペクトログラムから高解像度なスペクトログラムを推定する学習を行った。</p><h4 id="音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った"><a href="#音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った" class="headerlink" title="音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った"></a>音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った</h4><p>スペクトログラムは時間×周波数辺りの音声振幅であり、どちらの次元でも隣接情報に相関がある。そのため、スペクトログラムを１チャンネルの２次元画像とみなし、その変換には画像用のpix2pixモデルをそのまま適用した。一方、音響特徴量は時間辺りの特徴量であるため、画像用のpix2pixネットワークが適用できない。そこで、新たに1次元用のpix2pixネットワーク構造を提案し、これを音響特徴量の変換に適用した。</p><h3 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h3><p>以前の結果と、高音質化やpix2pixモデルでの特徴量変換の結果を比較した。この記事で用いたコードは<a href="https://github.com/Hiroshiba/become-yukarin" target="_blank" rel="noopener">become-yukarin Githubリポジトリ</a>で公開している。</p><h4 id="前回の結果"><a href="#前回の結果" class="headerlink" title="前回の結果"></a>前回の結果</h4><p>ベースとなる<a href="../voice-conversion-deep-leanring-and-other-delusions">前回</a>の結果は以下のようになっていた。（発話内容は「僕の声をディープラーニングの力を借りて結月ゆかりにするプロジェクト」）</p><figure>  <figcaption>入力音声</figcaption>  <audio src="0-input.wav" controls></audio></figure><figure>  <figcaption>ベース手法での変換結果</figcaption>  <audio src="0-output.wav" controls></audio></figure><figure>  <figcaption>ベース手法での変換結果スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/0-output-spectrogram.svg" class=""></figure><p>変換結果は結月ゆかりに近いが、発話内容が明細ではなく、音質も良くない。</p><h4 id="高音質化の結果"><a href="#高音質化の結果" class="headerlink" title="高音質化の結果"></a>高音質化の結果</h4><p>ベース手法の結果に高音質化を適用すると以下のようになった。</p><figure>  <figcaption>高音質化手法での変換結果</figcaption>  <audio src="1-output.wav" controls></audio></figure><figure>  <figcaption>高音質化手法での変換結果スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/1-output-spectrogram.svg" class=""></figure><p>音質は改善したが、発話内容がまだわかりにくい。</p><h4 id="音響特徴量変換pix2pixモデルの結果"><a href="#音響特徴量変換pix2pixモデルの結果" class="headerlink" title="音響特徴量変換pix2pixモデルの結果"></a>音響特徴量変換pix2pixモデルの結果</h4><p>音響特徴量変換をpix2pixモデルにすると以下のようになった。</p><figure>  <figcaption>pix2pixモデルでの変換結果</figcaption>  <audio src="2-output.wav" controls></audio></figure><figure>  <figcaption>pix2pixモデルでの変換結果スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/2-output-spectrogram.svg" class=""></figure><p>発話内容が明確にわかるようになった。</p><h4 id="おまけ"><a href="#おまけ" class="headerlink" title="おまけ"></a>おまけ</h4><p>入力データには全く関係のない音声を入力した結果は以下のようになった。</p><figure>  <figcaption>入力音声（[チュルリラ・チュルリラ・ダッダッダ！](http://www.nicovideo.jp/watch/sm28276238)の一部）</figcaption>  <audio src="3-input-spectrogram.wav" controls></audio></figure><figure>  <figcaption>変換結果</figcaption>  <audio src="3-output-spectrogram.wav" controls></audio></figure><figure>  <figcaption>入力音声スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/3-input-spectrogram.svg" class=""></figure><figure>  <figcaption>変換結果スペクトログラム</figcaption>  <img src="/became-yuduki-yukari-with-deep-learning-power/3-output-spectrogram.svg" class=""></figure><p>入力音声にかかわらず、変換結果が学習データに近くなるように学習されていることがわかった。</p><h3 id="考察"><a href="#考察" class="headerlink" title="考察"></a>考察</h3><p>pix2pixモデルを適用することで、高音質化ができ、変換結果の視聴性が上がった。</p><p>高音質化が上手く行ったのは、pix2pixモデルがかなり広い範囲（時間方向に5ミリ秒×128≒0.6秒、周波数方向に12000Hz÷1024×128≒1500Hz）の入力を元に推定できること、音声スペクトルは周波数方向に周期性があってスペクトログラムと多層CNNの相性が良いこと、そもそもタスク（結月ゆかり音声合成の高音質化）が簡単だったことが理由に考えられる。</p><p>低音質声質変換が上手く行ったのは、特徴量の次元を下げたことで過学習を防ぎつつGANが有効に働いたこと、pix2pixモデルが時間方向で広い範囲の入力を元に推定できることが理由に考えられる。</p><p>まだ精度は改善の余地がある。実使用時の基本周波数推定のエラーが課題の１つとして挙げられる。スペクトログラムベースのend-to-endな声質変換モデルにしたいが、ディープラーニングを使って位相推定できる上手な手法が思いつかない。</p><p>今後は解析を進めつつ、多対多声質変換を目指したい。また、可能そうであれば論文にしたい。</p><p>（別の記事に試行錯誤や雑多なメモを日記として書きます）</p><h3 id="宣伝＆謝辞"><a href="#宣伝＆謝辞" class="headerlink" title="宣伝＆謝辞"></a>宣伝＆謝辞</h3><p>この記事の内容は、ニコニコ生放送でライブコーディングしていました。僕の<a href="http://com.nicovideo.jp/community/co3686550" target="_blank" rel="noopener">ニコニココミュ</a>で朝6:30くらいから生放送しているのでよかったらいらしてください。（特に音声信号処理に詳しい方！）</p><p>デモ動画の作成にあたって、上記放送に来て議論してくださった方々にはとても感謝しています。特にサムネイル絵を頂いた@Kyowsukeさんに深く感謝します。ありがとうございます。</p>]]></content>
    
    <summary type="html">
    
      自分の声を結月ゆかりにしたい。前回はあまりクオリティが良くなかったので、手法を変えて質を上げたい。声質変換を、低音質変換と高音質化の二段階に分けてそれぞれ学習させた。画像分野で有名なモデルを使った。性能が飛躍的に向上し、かなり聞き取れるものになった。精度はまだ改善の余地があり、多対多声質変換にすることで精度が向上すると考えられる。今回の結果を論文化したい。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Chainerを使った研究開発時のクラス設計</title>
    <link href="http://blog.hiroshiba.jp/class-design-at-research-and-development-using-chainer/"/>
    <id>http://blog.hiroshiba.jp/class-design-at-research-and-development-using-chainer/</id>
    <published>2017-12-22T19:14:42.000Z</published>
    <updated>2018-02-12T03:44:59.900Z</updated>
    
    <content type="html"><![CDATA[<p>この記事は<a href="https://qiita.com/advent-calendar/2017/chainer" target="_blank" rel="noopener">Chainer Advent Calendar 2017</a>の23日目の記事です。</p><p>僕は普段、Chainerを使って研究開発しています。このとき、クラスをどう分けるべきかよく悩みます。いろいろやってみてある程度固まってきたので、自分なりにまとめてみました。</p><a id="more"></a><p>ChainerなどのDeepLearningフレームワークを使う理由は大きく分けて３段階ほどあります。</p><ol><li>再現実験</li><li>試行錯誤を伴う実験</li><li>学習済みモデルを用いたシステムづくり</li></ol><p>世の中に転がっているChainerサンプルプログラムは大体(1)のもので、こちらは綺麗にまとまっているものが多いです。一方で、何か新規に実験していると、どうしても試行錯誤が発生してコードが煩雑になります(2)。そしてさらには、(1)や(2)で学習したモデルを使ってサービス応用しようとすることもあります(3)。</p><p>今回は研究開発用のコード、つまり、サービス応用を考えつつ実験コードを書く際に、どうクラスを切っていくべきか考えをまとめます。</p><hr><p>まず、細かいのも含めると、実験コードには次の構成要素があります。</p><ul><li>DataProcess : 入力・出力データの加工する</li><li>Dataset : データをChainer用にまとめる</li><li>Network : 汎用のニューラルネットワーク</li><li>Loss : 損失の取り回し</li><li>Model : ニューラルネットワーク全体</li><li>Updater : モデルの更新（＋データの取り回し）</li><li>Trainer : 便利モジュールとの連携</li></ul><p>規模や実験内容に応じて<code>DataProcess</code>は<code>Dataset</code>に、<code>Network</code>と<code>Loss</code>は<code>Model</code>にまとめることもあります。このうち、学習済みモデルを用いたサービスを作る際に必要なのは、<code>DataProcess</code>と<code>Model</code>だけです。</p><p>それぞれに関して、なんなのか、なぜそれが必要か、どういうときに必要かを書きます。</p><hr><h2 id="DataProcess"><a href="#DataProcess" class="headerlink" title="DataProcess"></a>DataProcess</h2><p>入力データや出力データを加工する関数、もしくは呼び出し可能なオブジェクトです。画像を読み出す、クロップする、線画化する、などなど。これらのデータ処理は、<strong><code>DatasetMixin</code>オブジェクトの<code>get_example</code>メソッドに書くこともできますが、こうしてしまうとあとで流用する際にそのオブジェクトの構造を意識する必要が出てきます。</strong>例えば１枚の画像を加工したいだけでも、<code>DatasetMixin</code>オブジェクトを作成し、<code>get_example(0)</code>しなければいけません。最初からデータを加工する関数を切り出しておけば、後で簡単に流用できます。</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>データをChainer用にまとめるクラスです。<code>DatasetMixin</code>を継承して作るのが一般的です。<code>DataProcess</code>にも書いたとおり、<strong>ここに記述した処理は後で流用しづらいので、なるべく簡単なことしか書かないほうが良い</strong>と思います。僕は<code>DataProcess</code>を１つだけ受け取ってデータ加工する<code>Dataset</code>クラスをよく使っています。</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Dataset</span><span class="hljs-params">(chainer.dataset.DatasetMixin)</span>:</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, inputs, data_process)</span>:</span>    self._inputs = inputs    self._data_process = data_process  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>    <span class="hljs-keyword">return</span> len(self._inputs)  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_example</span><span class="hljs-params">(self, i)</span>:</span>    <span class="hljs-keyword">return</span> self._data_process(self._inputs[i])</code></pre><h2 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h2><p>汎用のネットワークを書きます。簡単なモデルの場合はなくても良いと思います。僕はよくBatchNormalizationとConvolution2Dをまとめたのを流用しています。</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BNConvolution2D</span><span class="hljs-params">(chainer.link.Chain)</span>:</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_channels, out_channels, ksize, stride=<span class="hljs-number">1</span>, pad=<span class="hljs-number">0</span>, **kwargs)</span>:</span>    super().__init__()    <span class="hljs-keyword">with</span> super().init_scope():      self.conv = chainer.links.Convolution2D(in_channels, out_channels, ksize, stride, pad, nobias=<span class="hljs-literal">True</span>, **kwargs)      self.bn = chainer.links.BatchNormalization(out_channels)  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(self, x)</span>:</span>    <span class="hljs-keyword">return</span> chainer.functions.relu(self.bn(self.conv(x)))</code></pre><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>損失関数を実装します。簡単なモデルの場合はなくても良いと思います。Chainerの<code>Trainer</code>とloss周りの扱いはややこしく、<code>chainer.report</code>を使ったりする必要があります。<strong><code>Loss</code>クラスの書き方は<a href="https://github.com/chainer/chainer/blob/4ce120d09b6543ae60a6d18830b4345992f1322d/chainer/links/model/classifier.py" target="_blank" rel="noopener">chainer.links.Classifier</a>がとても参考になります。</strong>コンストラクタで<code>Model</code>オブジェクトを受け取って<code>__call__</code>でフォワードし、得られた出力を元にlossを作ってreturnする設計です。<strong><code>Loss</code>クラスが必要になるのはモデルが２種類以上あるとき</strong>です。DCGANなどのタスクでは<code>Loss</code>クラスを作って、生成器と判別器用のlossを返すと綺麗にコードが書けます。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>ニューラルネットワークをまとめたクラスです。<strong><code>Optimizer</code>１つにつき<code>Model</code>１つ</strong>と考えると理解しやすいです。<code>chainer.link.Chain</code>や<code>chainer.link.ChainList</code>を継承して書くのが一般的です。</p><h2 id="Updater"><a href="#Updater" class="headerlink" title="Updater"></a>Updater</h2><p>こいつがむちゃくちゃしんどいです。<code>Model</code>が１つしかなければ<code>chainer.training.StandardUpdater</code>を使うと大体うまく行きます。<code>Model</code>が複数ある場合、<code>StandardUpdater</code>を継承した<code>Updater</code>クラスを自分で定義し、データの流れとモデルの更新を自分で書く必要があります。<a href="https://github.com/chainer/chainer/blob/7d0d6e70aab9763727802e2a8524744687e9086d/examples/dcgan/updater.py#L10" target="_blank" rel="noopener">DCGANのサンプル実装</a>でちょっと雰囲気がつかめると思います。<code>Loss</code>クラスをうまく切り出せてさえいればある程度綺麗に書けます。</p><h2 id="Trainer"><a href="#Trainer" class="headerlink" title="Trainer"></a>Trainer</h2><p>Chainerが用意した学習用のクラスです。<code>Updater</code>や<code>Model</code>を与えるとよしなに色々やってくれます。これに関してはいろんな記事があるので説明は割愛します。</p><hr><p>これらの方式で実験コードを書くと、ある程度煩雑になってきても大規模な改修は発生しづらくなります。また、<code>DataProcess</code>と<code>Model</code>をライブラリ化すれば、サービス応用も比較的簡単に行なえます。</p><p>Chainerは柔軟でいろんなクラス設計が可能です。試行錯誤を伴う実験をしていてもコードが散らばらないような設計があれば、ぜひ教えてください。開発方針を自分の中で持っておいて、どんどん研究開発していきたいものです。</p>]]></content>
    
    <summary type="html">
    
      この記事はChainer Advent Calendar 2017の23日目の記事です。僕は普段、Chainerを使って研究開発しています。このとき、クラスをどう分けるべきかよく悩みます。いろいろやってみてある程度固まってきたので、自分なりにまとめてみました。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>DeepLearningでも声質変換したい！</title>
    <link href="http://blog.hiroshiba.jp/voice-conversion-deep-leanring-and-other-delusions/"/>
    <id>http://blog.hiroshiba.jp/voice-conversion-deep-leanring-and-other-delusions/</id>
    <published>2017-12-09T18:39:08.000Z</published>
    <updated>2018-02-13T02:15:26.103Z</updated>
    
    <content type="html"><![CDATA[<p>これは<a href="https://qiita.com/advent-calendar/2017/dwango" target="_blank" rel="noopener">ドワンゴ Advent Calendar 2017</a>の9日目の記事です。</p><p>漫画やアニメを見ていると、可愛い女の子になって可愛い女の子と他愛もない会話をして過ごす日常に憧れます。そんな感じで、可愛い女の子になりたい人は多いと思います<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。しかし残念なことに、現在の技術で真の可愛い女の子になるのはとても難しいです。じゃあせめて仮想でいいから可愛い女の子になりたいですよね（バーチャルyoutuberキズナアイみたいな）。しかし、仮に姿を可愛い女の子にしても、声が可愛くなければ願いは叶いません。ということで、声を可愛くする声質変換を目指してみました。今回は僕の声をDeepLearningの力を借りて結月ゆかりにしました。</p><a id="more"></a><h2 id="お勉強"><a href="#お勉強" class="headerlink" title="お勉強"></a>お勉強</h2><p>まずは音声の勉強をします。これが一番時間かかりました。最近の音声合成手法は3種類あります。</p><ul><li>音響特徴量+vocoder</li><li>wavenet</li><li>STFT+位相推定</li></ul><p>今回使ったのは音響特徴量+vocoderですが、紹介がてらこれらを簡単に説明していきます。</p><h3 id="音響特徴量-vocoder"><a href="#音響特徴量-vocoder" class="headerlink" title="音響特徴量+vocoder"></a>音響特徴量+vocoder</h3><p>声と音響特徴量（基本周波数、スペクトラム、非周期信号）を相互変換して音声合成する手法です。基本周波数は声の高さ、スペクトラムは声質、非周期信号は子音とかの情報をうまくコードすることが期待されています。声と音響特徴量を相互変換する仕組みがvocoderです。話者Aの音声から、話者Bの対応する基本周波数・スペクトラムを推定することで声質変換ができます。非周期信号は推定無しで、そのまま転写すればいいとのこと。数年前まで、音響特徴量の推定にはGMMが用いられてきましたが、近年はDeepで換装した報告が多くあります。</p><h3 id="wavenet"><a href="#wavenet" class="headerlink" title="wavenet"></a>wavenet</h3><p>自己回帰モデルを使って生の音声を直接推定する手法です。とても綺麗な音声を合成できることが知られていますが、学習に時間がかかります。自己回帰モデルをなので当然生成に時間がかかります。が、後者の問題に関しては、先月高速に音声合成できるらしい手法が提案されました。お金とデータがいっぱいあるならこの手法が一番良さそうです。</p><h3 id="STFT-位相推定"><a href="#STFT-位相推定" class="headerlink" title="STFT+位相推定"></a>STFT+位相推定</h3><p>生の音声よりは推定しやすそうなSTFTを経由して音声合成する手法です。STFTを推定した後、さらに位相推定して音声を復元します。位相推定には古き良き手法がよく用いられますが、この手法は推定に時間がかかるので、リアルタイム音声合成には向きません。STFTは画像なので、画像生成分野の手法をそのままこれに適用するのが流行るかと思ってましたが、全然流行りませんでした。</p><h3 id="データセットの準備"><a href="#データセットの準備" class="headerlink" title="データセットの準備"></a>データセットの準備</h3><p><a href="http://www.ah-soft.com/voiceroid/yukari/" target="_blank" rel="noopener">voiceloid2の結月ゆかり</a>の音声と、自分の音声を用いました。まず結月ゆかりに５０３文読んでもらい、それを僕が読みます。僕は下手くそだったのと、録り直ししたのとで十数時間かかりました。もう二度とやりたくありません。波形をスペクトログラムにしたのがこちらで、見て分かる通り音声がズレています。</p><figure>  <figcaption>話者１のスペクトログラム</figcaption>  <img src="/voice-conversion-deep-leanring-and-other-delusions/raw-melspec-1-000.svg" class=""></figure><figure>  <figcaption>話者２のスペクトログラム</figcaption>  <img src="/voice-conversion-deep-leanring-and-other-delusions/raw-melspec-2-000.svg" class=""></figure><p>統計的声質変換の学習には、このズレが無い綺麗なデータが必要です。そのため音声アライメントする必要があります。今回は無音区間を省いてMFCCを求め、DTWを使ってアライメントしました（詳細：<a href="../sandbox-alignment-voice-actress-data/">声優統計コーパスをアライメントしてみる</a>）。あとついでに、特徴量がN(0,1)になるように正規化しました。</p><h2 id="モデル"><a href="#モデル" class="headerlink" title="モデル"></a>モデル</h2><p>時系列データを生成でき、かつ変換手法に適していそうなresidual構造のあるものを使います。ちょうどよかったので音声合成で良い性能を叩き出した<a href="https://arxiv.org/abs/1703.10135" target="_blank" rel="noopener">Tacotronモデル</a>の一部を使いました。</p><h2 id="学習条件"><a href="#学習条件" class="headerlink" title="学習条件"></a>学習条件</h2><p>損失関数は目標の音声と合成した音声の平均絶対誤差にしました。</p><p>音響特徴量は無声区間とそうじゃない区間で意味が変わってきます。例えば無声区間では基本周波数に意味がありません。僕はこの部分の損失が0になるようにマスクして学習させました。</p><p>他にもここに書いてないような細かい処理を色々しています。他の条件はまあ<a href="https://github.com/Hiroshiba/become-yukarin" target="_blank" rel="noopener">ソースコード</a>を参照ということで。。</p><h2 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h2><p>学習させてみました。全然ダメでした。</p><figure>  <figcaption>損失のグラフ</figcaption>  <img src="/voice-conversion-deep-leanring-and-other-delusions/loss-bad.png" class=""></figure><p>めっちゃ過学習します。モデルを弱くしてもやっぱり過学習します。時間がなかったので、入出力に大量にノイズ乗せて学習することにしました。学習したモデルを使って学習データを変換した結果はこうなりました。</p><figure>  <figcaption>声質変換結果</figcaption>  <audio src="output-hiho-pause-atr-A01.mp3" controls></audio></figure><p>変換された音声はまあ、四捨五入すると結月ゆかりでした。ちなみに入力音声と目標音声はこんな感じです。</p><figure>  <figcaption>入力音声</figcaption>  <audio src="input-hiho-pause-atr-A01.mp3" controls></audio></figure><figure>  <figcaption>目標音声</figcaption>  <audio src="target-yukari-pause-atr-A01.mp3" controls></audio></figure><p>学習済みモデルを使っていろいろ試してみました。</p><h3 id="テスト音声"><a href="#テスト音声" class="headerlink" title="テスト音声"></a>テスト音声</h3><p>先ほどの例は学習に用いた音声を変換しました。学習に用いていないテスト音声を変換してみます。</p><figure>  <figcaption>入力音声「僕の声をDeepLearningの力を借りて、結月ゆかりにするプロジェクト」</figcaption>  <audio src="input-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><figure>  <figcaption>変換結果「僕の声をDeepLearningの力を借りて、結月ゆかりにするプロジェクト」</figcaption>  <audio src="output-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><p>学習に用いた音声と同程度のクオリティでした。長い音声だと最後の方の声がふにゃふにゃになる傾向がありました。</p><h3 id="歌ってみた"><a href="#歌ってみた" class="headerlink" title="歌ってみた"></a>歌ってみた</h3><p>歌は母音の期間が長いので声質変換の結果がよく聞こえるらしいです。ということで歌ってみました。</p><figure>  <figcaption>入力音声「かえるのうたが」</figcaption>  <audio src="input-test-kaeru.mp3" controls></audio></figure><figure>  <figcaption>変換結果「かえるのうたが」</figcaption>  <audio src="output-test-kaeru.mp3" controls></audio></figure><p>音程がだいぶ揺れていました。歌は苦手なようです。</p><h3 id="ささやき声"><a href="#ささやき声" class="headerlink" title="ささやき声"></a>ささやき声</h3><p>可愛い女の子に耳元でささやかれたい人は多いと思います。ささやき声は母音が消えやすく、結構特殊な音声らしいです。ということで、ささやき声も変換してみました。</p><figure>  <figcaption>入力音声「お姉ちゃんのねぼすけ」（2014年春アニメ「ご注文はうさぎですか？」最終羽のチノがココアに言うセリフ）</figcaption>  <audio src="input-test-oneechan-no-nebosuke.mp3" controls></audio></figure><figure>  <figcaption>変換結果「お姉ちゃんのねぼすけ」</figcaption>  <audio src="output-test-oneechan-no-nebosuke.mp3" controls></audio></figure><p>だいぶぶっ壊れていました。結構難しいようです。</p><h3 id="他の人の音声"><a href="#他の人の音声" class="headerlink" title="他の人の音声"></a>他の人の音声</h3><p>これは「僕の声を」結月ゆかりにするプロジェクトです。この世に結月ゆかりは３人も必要ありません。他の人の声も変換できちゃうとダメです。僕の声だけを使って学習させたモデルを使って、他の人の音声も変換してみました。</p><figure>  <figcaption>入力音声（僕）「ありがとうございます」</figcaption>  <audio src="input-test-thankyou.mp3" controls></audio></figure><figure>  <figcaption>↑の変換結果</figcaption>  <audio src="output-test-thankyou.mp3" controls></audio></figure><figure>  <figcaption>入力音声（@lamazeP）「ありがとうございます」</figcaption>  <audio src="input-test-chino-thankyou.mp3" controls></audio></figure><figure>  <figcaption>↑の変換結果</figcaption>  <audio src="output-test-chino-thankyou.mp3" controls></audio></figure><figure>  <figcaption>入力音声（@AKIKAZEMOMIJInico）「ありがとうございます」</figcaption>  <audio src="input-test-momiji-thankyou.mp3" controls></audio></figure><figure>  <figcaption>↑の変換結果</figcaption>  <audio src="output-test-momiji-thankyou.mp3" controls></audio></figure><p>他の人の音声でもなんとなく変換できていましたが、僕の声が一番綺麗に変換できていました。</p><h2 id="その他試行錯誤"><a href="#その他試行錯誤" class="headerlink" title="その他試行錯誤"></a>その他試行錯誤</h2><h3 id="DRAGAN"><a href="#DRAGAN" class="headerlink" title="DRAGAN"></a>DRAGAN</h3><p><a href="https://arxiv.org/abs/1705.07215" target="_blank" rel="noopener">DRAGAN</a>は、画像生成分野でわりといい結果を出してる手法です。この手法を適用した判別器に、<a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="noopener">pix2pix</a>のように条件付けした音声を入力して学習してみました。まだパラメータ調整がうまく行っていないのかきれいな結果は得られていません。代わりに、実験者の正気度を下げるような結果が生成されました。</p><figure>  <figcaption>入力音声「僕の声をDeepLearningの力を借りて、結月ゆかりにするプロジェクト」</figcaption>  <audio src="input-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><figure>  <figcaption>変換結果１</figcaption>  <audio src="dragan10-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><figure>  <figcaption>変換結果２</figcaption>  <audio src="dragan30-test-deep-learning-yuduki-yukari.mp3" controls></audio></figure><p>これはこれで何かに使えそうですね。</p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>この程度のクオリティじゃ全然満足できないので、ちょくちょく改良していきたいと思います。あと、音声について色々学べたので、今後も色々やっていきたいです。成功したら、VRで結月ゆかり<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>になって、フル仮想してみたいです。</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;"><a href="https://qiita.com/toRisouP/items/14fe62f89808013f9f6e" target="_blank" rel="noopener">VRで「結月ゆかり」になって生放送する @toRisouP</a></span><a href="#fnref:1" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      漫画やアニメを見ていると、可愛い女の子になって可愛い女の子と他愛もない会話をして過ごす日常に憧れます。今回は僕の声をDeepLearningの力を借りて結月ゆかりにしました。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>マストドンアイドル チュートリアル</title>
    <link href="http://blog.hiroshiba.jp/mastodon-idol-tutorial/"/>
    <id>http://blog.hiroshiba.jp/mastodon-idol-tutorial/</id>
    <published>2017-11-30T14:52:19.000Z</published>
    <updated>2017-12-03T20:31:32.294Z</updated>
    
    <content type="html"><![CDATA[<h3 id="なに？"><a href="#なに？" class="headerlink" title="なに？"></a>なに？</h3><p>マストドンアイドルに厳格な定義はありません。その意味は、漠然と、<a href="https://github.com/tootsuite/mastodon" target="_blank" rel="noopener">マストドン</a>で人気のある人を指しています。狭義には、<a href="https://friends.nico/" target="_blank" rel="noopener">friends.nico</a>で生放送する人のことを指します。</p><h3 id="だれが？"><a href="#だれが？" class="headerlink" title="だれが？"></a>だれが？</h3><p>創作活動をしている、あるいはしたいと思っているが、モチベーションが湧かない人向けです。</p><h3 id="なぜ？"><a href="#なぜ？" class="headerlink" title="なぜ？"></a>なぜ？</h3><p>チャット感覚での交流が行われるマストドンは、創作活動のフィードバックを得られやすいためです。</p><a id="more"></a><p>何かしらの才能を持っているけど、それを日常でいまいち発揮できないクリエイターがたくさんいます。そういう人たちはたまに、ニコ動投稿者や、Youtuberを目指します。しかし、これらはファンの獲得が難しく、最初に人気を獲得するまでに挫折してしまうことが多いです。</p><p>それらのアイドルを目指す前に、SNSで一定のファンを得ておくと、初めたばかりでもフィードバックがもらえ、やる気に繋がります。そのSNSとして、チャット感覚でユーザー間交流ができるマストドンが適しています。</p><h3 id="どこで？"><a href="#どこで？" class="headerlink" title="どこで？"></a>どこで？</h3><p>ユーザー数が多い、クリエイターが多い、同趣味の人がいる可能性が高いマストドンインスタンスだと良いです。</p><p>創作活動の内容によって最適なインスタンスは変わります。専用のインスタンスでやっても良いですが、 <em>チャット文化が根付いていてユーザー数も多い<a href="https://friends.nico/" target="_blank" rel="noopener">friends.nico</a></em> で始めることをおすすめします。</p><h3 id="どうやって？"><a href="#どうやって？" class="headerlink" title="どうやって？"></a>どうやって？</h3><p>まずマストドンを初めてチャットベースで知り合いを増やします。続いて創作活動の経験があればその体験を、なければ抱負を語ります。興味を示してくれた知り合いが、あなたのファンです。</p><p>マストドンでのアイドル活動の流れを、例で紹介します。</p><h4 id="自信のある創作活動がある例（歌がうまい）"><a href="#自信のある創作活動がある例（歌がうまい）" class="headerlink" title="自信のある創作活動がある例（歌がうまい）"></a>自信のある創作活動がある例（歌がうまい）</h4><ol><li>マストドンを始める</li><li>ユーザーとチャットベースで交流し、知り合いを増やす</li><li>会話の流れで出た曲を歌った録音をアップロードするなど、歌がうまいことを伝える</li><li>知り合いが興味を持つ（＝ファン）</li><li>生放送など、よりコストの掛かる創作活動をする</li></ol><p>会話の流れで出た何かを使うことで、知り合いが興味を持ってくれやすくなります。会話はチャットベースで進みます。そのため脈絡のない発言は、ほとんどの場合で興味も持たれません。</p><p>脈絡があれば興味は持たれます。誰かのアイコンを描いたり、ショートストーリーを書いたり、誰かの放送の発言をMADにしたり、マストドン用の簡単なシステムを作ったり、手法は様々です。</p><h4 id="自信のある創作活動がないが、強い興味がある例（ゲーム作りたい）"><a href="#自信のある創作活動がないが、強い興味がある例（ゲーム作りたい）" class="headerlink" title="自信のある創作活動がないが、強い興味がある例（ゲーム作りたい）"></a>自信のある創作活動がないが、強い興味がある例（ゲーム作りたい）</h4><ol><li>マストドンを始める</li><li>ユーザーとチャットベースで交流し、知り合いを増やす</li><li>会話の流れで、ゲームを作りたいことを伝える</li><li>知り合いが興味を持つ（＝ファン）</li><li>ゲーム作成の本を買うなど、よりコストの掛かる創作活動をする</li></ol><p>これも同じく、会話の流れで発言することで興味を持ってくれやすくなります。突然言っても、スルーされたり反応が少なかったりします。</p><h3 id="いつ？"><a href="#いつ？" class="headerlink" title="いつ？"></a>いつ？</h3><h4 id="まだアカウントを持っていない場合"><a href="#まだアカウントを持っていない場合" class="headerlink" title="まだアカウントを持っていない場合"></a>まだアカウントを持っていない場合</h4><p>今すぐでも良いですが、新規参入者が多いタイミングで入るとより良いです。インスタンスには独自の文化が根づいています。新参が自分ひとりだと、昔からいる人たちの輪に入りづらいためです。friends.nicoであれば、ニコニコのバージョンアップの際が最も良いでしょう。他にも、大手ITサイトがマストドン関連の記事を出した際など、イベントがあったタイミングがベストです。</p><h4 id="既にアカウントがある場合"><a href="#既にアカウントがある場合" class="headerlink" title="既にアカウントがある場合"></a>既にアカウントがある場合</h4><p>ある程度仲の良い知り合いが増えた段階だと良いです。10人程度から挨拶を貰えるようになった段階が理想です。</p><h3 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h3><p>マストドンにいろんなクリエイターが集まって欲しいなー、という願いがあってこんな記事を書きました。</p><p>僕自身、friends.nicoでマストドンアイドル活動（創作活動＋生放送）やってます。内容は主にプログラミングやDeeeeeeepLearningです。良ければ<a href="http://com.nicovideo.jp/community/co3686550" target="_blank" rel="noopener">茶化しに来て</a>ください。</p><p>この記事は、<a href="https://adventar.org/calendars/2220" target="_blank" rel="noopener">friends.nicoアドベントカレンダー</a>初日の投稿記事です。12月1日の0時に間に合わせる予定でした。しかし、<a href="https://twitter.com/hiho_karuta/status/936153544800976896" target="_blank" rel="noopener">11月30日現在はタイに出張</a>しており、記事を書き始めた22時には日本時間で12月になってしまっていました。すみませんでした。でも、間に合わせるつもりでした。許してください。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;なに？&quot;&gt;&lt;a href=&quot;#なに？&quot; class=&quot;headerlink&quot; title=&quot;なに？&quot;&gt;&lt;/a&gt;なに？&lt;/h3&gt;&lt;p&gt;マストドンアイドルに厳格な定義はありません。
その意味は、漠然と、&lt;a href=&quot;https://github.com/tootsuite/mastodon&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;マストドン&lt;/a&gt;で人気のある人を指しています。
狭義には、&lt;a href=&quot;https://friends.nico/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;friends.nico&lt;/a&gt;で生放送する人のことを指します。&lt;/p&gt;
&lt;h3 id=&quot;だれが？&quot;&gt;&lt;a href=&quot;#だれが？&quot; class=&quot;headerlink&quot; title=&quot;だれが？&quot;&gt;&lt;/a&gt;だれが？&lt;/h3&gt;&lt;p&gt;創作活動をしている、あるいはしたいと思っているが、モチベーションが湧かない人向けです。&lt;/p&gt;
&lt;h3 id=&quot;なぜ？&quot;&gt;&lt;a href=&quot;#なぜ？&quot; class=&quot;headerlink&quot; title=&quot;なぜ？&quot;&gt;&lt;/a&gt;なぜ？&lt;/h3&gt;&lt;p&gt;チャット感覚での交流が行われるマストドンは、創作活動のフィードバックを得られやすいためです。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>声優統計コーパスをアライメントしてみる</title>
    <link href="http://blog.hiroshiba.jp/sandbox-alignment-voice-actress-data/"/>
    <id>http://blog.hiroshiba.jp/sandbox-alignment-voice-actress-data/</id>
    <published>2017-11-03T10:58:12.000Z</published>
    <updated>2018-02-12T03:44:59.903Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目次"><a href="#目次" class="headerlink" title="目次"></a>目次</h3><ul><li>（背景）声質変換用のデータを作るために音声アライメントを試してみたい</li><li>（手法）<a href="http://voice-statistics.github.io/" target="_blank" rel="noopener">声優統計コーパス</a>のデータを使用し、MFCCでアライメントした</li><li>（結果）アライメント後の音声はところどころ伸びていた。無音とする閾値を下げると伸びは抑制された。</li><li>（考察）もっと完璧に揃うと思っていた。</li></ul><figure>  <figcaption>話者１のメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-1-000.svg" class=""></figure><figure>  <figcaption>話者２のメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-2-000.svg" class=""></figure><a id="more"></a><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>DeeeeepLearingを使って声質変換してみたい。どうやら最初に、違う話者による同じ発話内容の音声を揃える必要があるらしい。声質変換用のデータを作るために音声アライメントを試してみた。</p><h3 id="手法"><a href="#手法" class="headerlink" title="手法"></a>手法</h3><p>データセットは<a href="http://voice-statistics.github.io/" target="_blank" rel="noopener">声優統計コーパス</a>の音声データを使用した。話者は<code>fujitou_normal</code>（話者１）と<code>tsuchiya_normal</code>（話者２）を選んだ。</p><p>アライメント用の特徴量はいろんな文献に従いMFCCとした。MFCC抽出には<a href="http://ml.cs.yamanashi.ac.jp/world/" target="_blank" rel="noopener">World</a>と、そのpythonラッパー<a href="https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder" target="_blank" rel="noopener">pyworld</a>を用いた。</p><p>音声のアライメントはMFCCにDTWを適用した結果を用いた。DTWは<a href="https://r9y9.github.io/nnmnkwii/latest/nnmnkwii_gallery/notebooks/vc/01-GMM%20voice%20conversion%20%28en%29.html" target="_blank" rel="noopener">音声変換の実装例</a>アライメントされた音声の生成は、DTW結果のindexに従ってSTFTを並び替え、STFTを逆変換して求めた。</p><p>比較用にメルスペクトログラムを求め、作図に用いた。これの生成は<a href="https://github.com/keithito/tacotron/blob/master/util/audio.py" target="_blank" rel="noopener">tacotronの実装例</a>を参考にした。</p><p>コードは<a href="https://gist.github.com/Hiroshiba/25fee12b3e51b2209b249fdfbb6ade88" target="_blank" rel="noopener">gist</a>に公開した。無音とする閾値は<code>librosa.effects.split</code>の<code>top_db</code>で変更できる。</p><h3 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h3><h4 id="アライメントされた音声"><a href="#アライメントされた音声" class="headerlink" title="アライメントされた音声"></a>アライメントされた音声</h4><p>話者１と話者２の音声をアライメント結果は以下のようになった。</p><figure>  <figcaption>話者１の元ボイス</figcaption>  <audio src="raw-voice-1-000.wav" controls></audio></figure><figure>  <figcaption>話者２の元ボイス</figcaption>  <audio src="raw-voice-2-000.wav" controls></audio></figure><figure>  <figcaption>アライメントされた話者１のボイス</figcaption>  <audio src="aligned-voice-1-000.wav" controls></audio></figure><figure>  <figcaption>アライメントされた話者２のボイス</figcaption>  <audio src="raw-voice-2-000.wav" controls></audio></figure><p>前半はきれいに揃っているが、後半の最後は伸びた感じになっていた。</p><h4 id="アライメントされたメルスペクトログラム"><a href="#アライメントされたメルスペクトログラム" class="headerlink" title="アライメントされたメルスペクトログラム"></a>アライメントされたメルスペクトログラム</h4><p>先程の音声のメルスペクトログラムは以下のようになった。</p><figure>  <figcaption>話者１の元ボイスのメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/raw-melspec-1-000.svg" class=""></figure><figure>  <figcaption>話者２の元ボイスのメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/raw-melspec-2-000.svg" class=""></figure><figure>  <figcaption>アライメントされた話者１の元ボイスのメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/aligned-melspec-1-000.svg" class=""></figure><figure>  <figcaption>アライメントされた話者２の元ボイスのメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/aligned-melspec-2-000.svg" class=""></figure><p>無音区間の部分アライメントに悪影響しているようだった。</p><h4 id="無音区間の閾値を下げてアライメントした結果"><a href="#無音区間の閾値を下げてアライメントした結果" class="headerlink" title="無音区間の閾値を下げてアライメントした結果"></a>無音区間の閾値を下げてアライメントした結果</h4><p>無音区間とする音圧の閾値を下げてアライメントした。<code>librosa.effects.split</code>の<code>top_db</code>をデフォルト値から<code>20</code>にした。</p><figure>  <figcaption>話者１のボイス</figcaption>  <audio src="muteup-aligned-voice-1-000.wav" controls></audio></figure><figure>  <figcaption>話者２のボイス</figcaption>  <audio src="muteup-aligned-voice-2-000.wav" controls></audio></figure><figure>  <figcaption>話者１のメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-1-000.svg" class=""></figure><figure>  <figcaption>話者２のメルスペクトログラム</figcaption>  <img src="/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-2-000.svg" class=""></figure><p>ところどころブザーのような音になった。よくなっているように思える。</p><h3 id="考察"><a href="#考察" class="headerlink" title="考察"></a>考察</h3><p>DTWの仕組みから考えて無音区間は点的なのだろう。積極的に排除した方がいい。（声質変換の論文の実験データ項目を見ても、MFCCを使った、程度のことしか書いてなかった。）</p><p>DTWで音声が完璧に揃えられると思っていたけど、いまいちだった。とりあえずこのまま機械学習の入出力にしたいと思う。</p><p>MFCCでDTWして別のデータを並び替えるために、DTWAlignerに別のデータを与える設計にしたが、これは良くない。<a href="https://librosa.github.io/librosa/generated/librosa.effects.trim.html#" target="_blank" rel="noopener">librosa.effects.trim</a>のように、インデックスを返す実装の方がいい。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;目次&quot;&gt;&lt;a href=&quot;#目次&quot; class=&quot;headerlink&quot; title=&quot;目次&quot;&gt;&lt;/a&gt;目次&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;（背景）声質変換用のデータを作るために音声アライメントを試してみたい&lt;/li&gt;
&lt;li&gt;（手法）&lt;a href=&quot;http://voice-statistics.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;声優統計コーパス&lt;/a&gt;のデータを使用し、MFCCでアライメントした&lt;/li&gt;
&lt;li&gt;（結果）アライメント後の音声はところどころ伸びていた。無音とする閾値を下げると伸びは抑制された。&lt;/li&gt;
&lt;li&gt;（考察）もっと完璧に揃うと思っていた。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
  &lt;figcaption&gt;話者１のメルスペクトログラム&lt;/figcaption&gt;
  &lt;img src=&quot;/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-1-000.svg&quot; class=&quot;&quot;&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;figcaption&gt;話者２のメルスペクトログラム&lt;/figcaption&gt;
  &lt;img src=&quot;/sandbox-alignment-voice-actress-data/muteup-aligned-melspec-2-000.svg&quot; class=&quot;&quot;&gt;
&lt;/figure&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ブログ作りました</title>
    <link href="http://blog.hiroshiba.jp/first-commit/"/>
    <id>http://blog.hiroshiba.jp/first-commit/</id>
    <published>2017-09-30T20:12:35.000Z</published>
    <updated>2017-11-02T12:21:42.028Z</updated>
    
    <content type="html"><![CDATA[<p>技術系の日記をつける予定です。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;技術系の日記をつける予定です。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
