---
title: ディープラーニングの力で結月ゆかりの声になってみた
date: 2018-02-13 11:08:25
tags:
categories:
permalink: became-yuduki-yukari-with-deep-learning-power
description: 自分の声を結月ゆかりにしたい。前回はあまりクオリティが良くなかったので、手法を変えて質を上げたい。声質変換を、低音質変換と高音質化の二段階に分けてそれぞれ学習させた。画像分野で有名なモデルを使った。性能が飛躍的に向上し、かなり聞き取れるものになった。精度はまだ改善の余地があり、多対多声質変換にすることで精度が向上すると考えられる。今回の結果を論文化したい。
---

### 目次

* （背景）自分の声を結月ゆかりにしたい。前回はあまりクオリティが良くなかったので、手法を変えて質を上げたい。
* （手法）声質変換を、低音質変換と高音質化の二段階に分けてそれぞれ学習させた。画像分野で有名なモデルを使った。
* （結果）性能が飛躍的に向上し、かなり聞き取れるものになった。
* （考察）精度はまだ改善の余地があり、多対多声質変換にすることで精度が向上すると考えられる。今回の結果を論文化したい。

<!-- more -->

### デモ動画

<script type="application/javascript" src="https://embed.nicovideo.jp/watch/sm32724409/script?w=640&h=360"></script><noscript><a href="http://www.nicovideo.jp/watch/sm32724409">ディープラーニングの力で結月ゆかりの声になってみた</a></noscript>

### 背景
多くの人が可愛い女の子になりたいと思っている。
ＣＧ技術やモーションキャプチャ技術の向上により、姿は女の子に仮想化できるようになってきた。
しかし、声に関してはまだまだ課題が多い。
声質変換は「遅延」「音質」「複数話者」などの難しい課題がある。
今回は、自分の声を結月ゆかりにするための、低遅延で実現可能な高音質声質変換を目指した。

### 手法
大きく分けて３つの工夫をした。

1. 画像ディープラーニング分野で性能の良かったモデルを使用した
2. 声質変換を「低音質声質変換」部分と「高音質化」部分に分けた
3. 音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った

#### 画像ディープラーニング分野で性能の良かったモデルを使用した

画像ディープラーニング分野は音声より数年早く進んでおり、
解きたいタスクに対して使用すれば良いモデルが知られている。
今回のように、対応関係のあるもの（音声分野で言うところのパラレルデータ）の変換を、
少ないデータ数で学習させるには、[pix2pixモデル](https://phillipi.github.io/pix2pix/)が適している。
今回はこのpix2pixモデルを使って声質変換タスクを解いた。

#### 声質変換を「低音質声質変換」部分と「高音質化」部分に分けた

声質変換には、複数話者によるペア音声を用意する必要があるが、
結月ゆかりは大量の音声データを簡単に得られるのに対し、自分の声を大量に用意するのは容易ではない。
そこで、少ないペア音声を使って低音質な声質変換するタスクと、
大量の音声を使って高音質化するタスクを分けた。
低音質声質変換では、WORLDを用いて自分の音響特徴量から結月ゆかりの音響特徴量を推定する学習を行った。
高音質化では、低解像度なスペクトログラムから高解像度なスペクトログラムを推定する学習を行った。

#### 音響特徴量の変換では１次元のpix2pixモデルを、スペクトログラムの変換では２次元のpix2pixモデルを使った
スペクトログラムは時間×周波数辺りの音声振幅であり、どちらの次元でも隣接情報に相関がある。
そのため、スペクトログラムを１チャンネルの２次元画像とみなし、その変換には画像用のpix2pixモデルをそのまま適用した。
一方、音響特徴量は時間辺りの特徴量であるため、画像用のpix2pixネットワークが適用できない。
そこで、新たに1次元用のpix2pixネットワーク構造を提案し、これを音響特徴量の変換に適用した。

### 結果

以前の結果と、高音質化やpix2pixモデルでの特徴量変換の結果を比較した。
この記事で用いたコードは[become-yukarin Githubリポジトリ](https://github.com/Hiroshiba/become-yukarin)で公開している。

#### 前回の結果
ベースとなる[前回](../voice-conversion-deep-leanring-and-other-delusions)の結果は以下のようになっていた。
（発話内容は「僕の声をディープラーニングの力を借りて結月ゆかりにするプロジェクト」）

<figure>
  <figcaption>入力音声</figcaption>
  <audio src="0-input.wav" controls></audio>
</figure>

<figure>
  <figcaption>ベース手法での変換結果</figcaption>
  <audio src="0-output.wav" controls></audio>
</figure>

<figure>
  <figcaption>ベース手法での変換結果スペクトログラム</figcaption>
  {% asset_img 0-output-spectrogram.svg %}
</figure>

変換結果は結月ゆかりに近いが、発話内容が明細ではなく、音質も良くない。


#### 高音質化の結果
ベース手法の結果に高音質化を適用すると以下のようになった。

<figure>
  <figcaption>高音質化手法での変換結果</figcaption>
  <audio src="1-output.wav" controls></audio>
</figure>

<figure>
  <figcaption>高音質化手法での変換結果スペクトログラム</figcaption>
  {% asset_img 1-output-spectrogram.svg %}
</figure>

音質は改善したが、発話内容がまだわかりにくい。

#### 音響特徴量変換pix2pixモデルの結果
音響特徴量変換をpix2pixモデルにすると以下のようになった。

<figure>
  <figcaption>pix2pixモデルでの変換結果</figcaption>
  <audio src="2-output.wav" controls></audio>
</figure>

<figure>
  <figcaption>pix2pixモデルでの変換結果スペクトログラム</figcaption>
  {% asset_img 2-output-spectrogram.svg %}
</figure>

発話内容が明確にわかるようになった。

#### おまけ
入力データには全く関係のない音声を入力した結果は以下のようになった。

<figure>
  <figcaption>入力音声（[チュルリラ・チュルリラ・ダッダッダ！](http://www.nicovideo.jp/watch/sm28276238)の一部）</figcaption>
  <audio src="3-input-spectrogram.wav" controls></audio>
</figure>

<figure>
  <figcaption>変換結果</figcaption>
  <audio src="3-output-spectrogram.wav" controls></audio>
</figure>

<figure>
  <figcaption>入力音声スペクトログラム</figcaption>
  {% asset_img 3-input-spectrogram.svg %}
</figure>

<figure>
  <figcaption>変換結果スペクトログラム</figcaption>
  {% asset_img 3-output-spectrogram.svg %}
</figure>

入力音声にかかわらず、変換結果が学習データに近くなるように学習されていることがわかった。

### 考察

pix2pixモデルを適用することで、高音質化ができ、変換結果の視聴性が上がった。

高音質化が上手く行ったのは、pix2pixモデルがかなり広い範囲（時間方向に5ミリ秒×128≒0.6秒、周波数方向に12000Hz÷1024×128≒1500Hz）の入力を元に推定できること、
音声スペクトルは周波数方向に周期性があってスペクトログラムと多層CNNの相性が良いこと、
そもそもタスク（結月ゆかり音声合成の高音質化）が簡単だったことが理由に考えられる。

低音質声質変換が上手く行ったのは、特徴量の次元を下げたことで過学習を防ぎつつGANが有効に働いたこと、
pix2pixモデルが時間方向で広い範囲の入力を元に推定できることが理由に考えられる。

まだ精度は改善の余地がある。
実使用時の基本周波数推定のエラーが課題の１つとして挙げられる。
スペクトログラムベースのend-to-endな声質変換モデルにしたいが、
ディープラーニングを使って位相推定できる上手な手法が思いつかない。

今後は解析を進めつつ、多対多声質変換を目指したい。また、可能そうであれば論文にしたい。

（別の記事に試行錯誤や雑多なメモを日記として書きます）

### 宣伝＆謝辞

この記事の内容は、ニコニコ生放送でライブコーディングしていました。
僕の[ニコニココミュ](http://com.nicovideo.jp/community/co3686550)で朝6:30くらいから生放送しているのでよかったらいらしてください。（特に音声信号処理に詳しい方！）

デモ動画の作成にあたって、上記放送に来て議論してくださった方々にはとても感謝しています。
特にサムネイル絵を頂いた@Kyowsukeさんに深く感謝します。ありがとうございます。
