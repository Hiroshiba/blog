---
title: 声優統計コーパスをアライメントしてみる
permalink: sandbox-alignment-voice-actress-data
date: 2017-11-03 19:58:12
tags:
categories:
---

### 目次

* （背景）声質変換用のデータを作るために音声アライメントを試してみたい
* （手法）[声優統計コーパス](http://voice-statistics.github.io/)のデータを使用し、MFCCでアライメントした
* （結果）アライメント後の音声はところどころ伸びていた。無音とする閾値を下げると伸びは抑制された。
* （考察）もっと完璧に揃うと思っていた。

<figure>
  {% asset_img muteup-aligned-melspec-1-000.svg %}
  <figcaption>話者１のメルスペクトログラム</figcaption>
</figure>

<figure>
  {% asset_img muteup-aligned-melspec-2-000.svg %}
  <figcaption>話者２のメルスペクトログラム</figcaption>
</figure>

<!-- more -->

### 背景
DeeeeepLearingを使って声質変換してみたい。
どうやら最初に、違う話者による同じ発話内容の音声を揃える必要があるらしい。
声質変換用のデータを作るために音声アライメントを試してみた。

### 手法
データセットは[声優統計コーパス](http://voice-statistics.github.io/)の音声データを使用した。
話者は`fujitou_normal`（話者１）と`tsuchiya_normal`（話者２）を選んだ。

アライメント用の特徴量はいろんな文献に従いMFCCとした。
MFCC抽出には[World](http://ml.cs.yamanashi.ac.jp/world/)と、そのpythonラッパー[pyworld](https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder)を用いた。

音声のアライメントはMFCCにDTWを適用した結果を用いた。
DTWは[音声変換の実装例](https://r9y9.github.io/nnmnkwii/latest/nnmnkwii_gallery/notebooks/vc/01-GMM%20voice%20conversion%20%28en%29.html)アライメントされた音声の生成は、DTW結果のindexに従ってSTFTを並び替え、STFTを逆変換して求めた。

比較用にメルスペクトログラムを求め、作図に用いた。
これの生成は[tacotronの実装例](https://github.com/keithito/tacotron/blob/master/util/audio.py)を参考にした。

コードは[gist](https://gist.github.com/Hiroshiba/25fee12b3e51b2209b249fdfbb6ade88)に公開した。
無音とする閾値は`librosa.effects.split`の`top_db`で変更できる。

### 結果

#### アライメントされた音声
話者１と話者２の音声をアライメント結果は以下のようになった。

<figure>
  <audio src="raw-voice-1-000.wav" controls></audio>
  <figcaption>話者１の元ボイス</figcaption>
</figure>

<figure>
  <audio src="raw-voice-2-000.wav" controls></audio>
  <figcaption>話者２の元ボイス</figcaption>
</figure>

<figure>
  <audio src="aligned-voice-1-000.wav" controls></audio>
  <figcaption>アライメントされた話者１のボイス</figcaption>
</figure>

<figure>
  <audio src="raw-voice-2-000.wav" controls></audio>
  <figcaption>アライメントされた話者２のボイス</figcaption>
</figure>

前半はきれいに揃っているが、後半の最後は伸びた感じになっていた。

#### アライメントされたメルスペクトログラム

先程の音声のメルスペクトログラムは以下のようになった。

<figure>
  {% asset_img raw-melspec-1-000.svg %}
  <figcaption>話者１の元ボイスのメルスペクトログラム</figcaption>
</figure>

<figure>
  {% asset_img raw-melspec-2-000.svg %}
  <figcaption>話者２の元ボイスのメルスペクトログラム</figcaption>
</figure>

<figure>
  {% asset_img aligned-melspec-1-000.svg %}
  <figcaption>アライメントされた話者１の元ボイスのメルスペクトログラム</figcaption>
</figure>

<figure>
  {% asset_img aligned-melspec-2-000.svg %}
  <figcaption>アライメントされた話者２の元ボイスのメルスペクトログラム</figcaption>
</figure>

無音区間の部分アライメントに悪影響しているようだった。

#### 無音区間の閾値を下げてアライメントした結果

無音区間とする音圧の閾値を下げてアライメントした。
`librosa.effects.split`の`top_db`をデフォルト値から`20`にした。

<figure>
  <audio src="muteup-aligned-voice-1-000.wav" controls></audio>
  <figcaption>話者１のボイス</figcaption>
</figure>

<figure>
  <audio src="muteup-aligned-voice-2-000.wav" controls></audio>
  <figcaption>話者２のボイス</figcaption>
</figure>

<figure>
  {% asset_img muteup-aligned-melspec-1-000.svg %}
  <figcaption>話者１のメルスペクトログラム</figcaption>
</figure>

<figure>
  {% asset_img muteup-aligned-melspec-2-000.svg %}
  <figcaption>話者２のメルスペクトログラム</figcaption>
</figure>

ところどころブザーのような音になった。よくなっているように思える。

### 考察
DTWの仕組みから考えて無音区間は点的なのだろう。積極的に排除した方がいい。
（声質変換の論文の実験データ項目を見ても、MFCCを使った、程度のことしか書いてなかった。）

DTWで音声が完璧に揃えられると思っていたけど、いまいちだった。
とりあえずこのまま機械学習の入出力にしたいと思う。

MFCCでDTWして別のデータを並び替えるために、DTWAlignerに別のデータを与える設計にしたが、これは良くない。
[librosa.effects.trim](https://librosa.github.io/librosa/generated/librosa.effects.trim.html#)のように、インデックスを返す実装の方がいい。
